{
  
    
        "post0": {
            "title": "혼공머 07-3",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 앙상블 from sklearn.ensemble import ExtraTreesClassifier # 엑스트라 트리 앙상블 from sklearn.ensemble import GradientBoostingClassifier # 그레이디언트 부스팅 앙상블 # 히스토그램 기반 그레이디언트 부스팅 from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.inspection import permutation_importance # 특성중요도 from xgboost import XGBClassifier # 알고리즘을 구현한 또다른 라이브러리1 from lightgbm import LGBMClassifier # 알고리즘을 구현한 또다른 라이브러리2 , 마이크로소프트에서 구현 # ____________________ from sklearn.cluster import KMeans # KMeans from sklearn.decomposition import PCA # 주성분 분석 # 7장 딥러닝 from tensorflow import keras # 케라스 import tensorflow as tf # 텐서플로 . . &#49552;&#49892; &#44257;&#49440; . (train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() train_scaled = train_input / 255.0 train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) . def model_fn(a_layer=None): model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28, 28))) model.add(keras.layers.Dense(100, activation=&#39;relu&#39;)) if a_layer: model.add(a_layer) model.add(keras.layers.Dense(10, activation=&#39;softmax&#39;)) return model . if 구문을 제외하면 이전 절과 같다. . if 구문의 역할 :model_fn() 함수에 케라스 층을 추가하면 은닉층 뒤에 또 하나의 층을 추가하는 것 . model = model_fn() model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense (Dense) (None, 100) 78500 dense_1 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ . model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) history = model.fit(train_scaled, train_target, epochs=5, verbose=0) . verbose = 0 :훈련과정 출력을 조절, 0으로 지정하여 훈련 과정을 나타내지 않음 . print(history.history.keys()) . dict_keys([&#39;loss&#39;, &#39;accuracy&#39;]) . (손실, 정확도) . plt.plot(history.history[&#39;loss&#39;]) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.show() . plt.plot(history.history[&#39;accuracy&#39;]) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.show() . model = model_fn() model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) history = model.fit(train_scaled, train_target, epochs=20, verbose=0) . plt.plot(history.history[&#39;loss&#39;]) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.show() . &#44160;&#51613; &#49552;&#49892; . - 에포크에 따른 과대/과소적합 알아보기 . - 훈련 세트 뿐만 아니라 검증 세트에 대한 점수도 필요. . - 에포크마다 검증 손실을 계산 하기 위해 케라스 모델의 fit() 메서드에 검증데이터를 전달 . model = model_fn() model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target)) . print(history.history.keys()) . dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;]) . plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;train&#39;, &#39;val&#39;]) plt.show() . 초기에 검증 손실이 감소하다가 다섯 번째 에포크만에 다시 상승 . 전형 적인 과대적합 모델 생성 . model = model_fn() model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target)) . 과대적합이 훨씬 줄어듦 . 검증손실 그래프에 여전히 요동이 남아 있지만 열 번째 에포크 까지 전반적인 감소 추세가 이어짐 $ to$ Adam 옵티마이저가 이 데이터셋에 잘 맞는다는 뜻 . &#46300;&#47213;&#50500;&#50883; dropout . 신경망에서 사용하는 대표적인 규제 방법 . 제프리 힌턴이 소개 . 훈련과정에서 층에 있는 일부 뉴런을 랜덤하게 꺼서 (뉴런의 출력을 0으로 만들어) 과대적합을 막는다. . 얼마나 많은 뉴런을 드롭할지는 하이퍼파라미터 . model = model_fn(keras.layers.Dropout(0.3)) model.summary() . Model: &#34;sequential_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_4 (Flatten) (None, 784) 0 dense_8 (Dense) (None, 100) 78500 dropout (Dropout) (None, 100) 0 dense_9 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ . 드롭아웃 층은 훈련되는 모델 파라미터가 없다. 또한 입력과 출력의 크기가 같다. . 뉴런의 추력을 0으로 바꾸기는 하지만 전체 출력배열의 크기를 바꾸지는 않는다. . 훈련이 끝난 뒤 평가나 예측을 수행할 때는 드랍아웃을 적용하지 말아야 한다. 훈련된 모든 뉴런을 사용해야 올바른 예측 수행 가능 . 텐서플로와 케라스는 모델을 평가와 예측에 사용할 때는 자동으로 드롭아웃을 적용하지 않음 . model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target)) plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;train&#39;, &#39;val&#39;]) plt.show() . 과대적합이 확실히 줄어듦 . &#47784;&#45944; &#51200;&#51109;&#44284; &#48373;&#50896; . model = model_fn(keras.layers.Dropout(0.3)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) history = model.fit(train_scaled, train_target, epochs=10, verbose=0, validation_data=(val_scaled, val_target)) . model.save_weights(&#39;model-weights.h5&#39;) . model.save(&#39;model-whole.h5&#39;) . !ls -al *.h5 . -rwxrwxrwx 1 cjfal cjfal 333448 Mar 9 21:12 model-weights.h5 -rwxrwxrwx 1 cjfal cjfal 982704 Mar 9 21:12 model-whole.h5 . &#44036;&#45800;&#54620; &#49892;&#54744; . 1 훈련을 하지 않은 새로운 모델을 만들고 model-weights.h5 파일에서 훈련된 모델 파라미터를 읽어서 사용. . model = model_fn(keras.layers.Dropout(0.3)) model.load_weights(&#39;model-weights.h5&#39;) . load_weights() 메서드를 사용하려면 save로 저장했던 모델과 정확히 같은 구조여야한다. . val_labels = np.argmax(model.predict(val_scaled), axis=-1) print(np.mean(val_labels == val_target)) . 0.882 . model = keras.models.load_model(&#39;model-whole.h5&#39;) model.evaluate(val_scaled, val_target) . 375/375 [==============================] - 0s 903us/step - loss: 0.3276 - accuracy: 0.8820 . [0.3275781571865082, 0.8820000290870667] . 같은 정확도를 가짐 . load_model() 함수는 모델 파라미터뿐만 아니라 모델 구조와 옵티마이저 상태까지 모두 복원하기 때문에 evaluate()메서드를 사용할 수 있다. 텐서플로 2.3 에서는 버그때문에 evaluate() 메서드를 사용하기 전에 compile() 메서드를 호출해야만 한다. . &#53084;&#48177; . 훈련과정 중간에 어떤 작업을 수행할 수 있게하는 객체 . model = model_fn(keras.layers.Dropout(0.3)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) checkpoint_cb = keras.callbacks.ModelCheckpoint(&#39;best-model.h5&#39;, save_best_only=True) model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb]) . &lt;keras.callbacks.History at 0x7fa3ccd28df0&gt; . model = keras.models.load_model(&#39;best-model.h5&#39;) model.evaluate(val_scaled, val_target) . 375/375 [==============================] - 0s 878us/step - loss: 0.3194 - accuracy: 0.8869 . [0.31941428780555725, 0.8869166374206543] . 과대적합이 시작되기 전에 훈련을 미리 중지하는 것 :조기종료(early stopping) 조기 종료는 훈련 에포크 횟수를 제한하는 역할이지만 모델이 과대적합되는 것을 막아 주기 때문에 규제 방법 중 하나로 생각할 수 있다. . model = model_fn(keras.layers.Dropout(0.3)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) checkpoint_cb = keras.callbacks.ModelCheckpoint(&#39;best-model.h5&#39;, save_best_only=True) early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True) history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) . print(early_stopping_cb.stopped_epoch) . 8 . plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;train&#39;, &#39;val&#39;]) plt.show() . 조기 종료 기법을 사용하면 안심하고 에포크 횟수를 크게 지정해도 괜찮다.(10번째 에포크에서 훈련 중지) . model.evaluate(val_scaled, val_target) . 375/375 [==============================] - 0s 752us/step - loss: 0.3310 - accuracy: 0.8782 . [0.3310009837150574, 0.878166675567627] . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 드롭아웃 : 은닉층에 있는 뉴런의 출력을 랜덤하게 꺼서 과대적합을 막는 기법입니다. 드롭아웃은 훈련 중에 적용되며 평가나 예측에서는 적용하지 않습니다. 텐서플로는 이를 자동으로 체리합니다. . - 콜백 : 케라스 모델을 훈련하는 도중에 어떤 작업을 수행할 수 있도록 도와주는 도구입니다. 대표적으로 최상의 모델을 자동으로 저장해 주거나 검증 점수가 더 이상 향상되지 않으면 일찍 종료할 수 있습니다. . - 조기 종료 : 검증 점수가 더 이상 감소하지 않고 상승하여 과대적합이 일어나면 훈련을 계속 진행하지 않고 멈추는 기법입니다. 이렇게 하면 계산 비용과 시간을 절약할 수 있습니다. . - 핵심 패키지 . - TensorFlow . 1 Dropout :드롭아웃 층입니다.&gt; &gt;첫 번째 매개변수로 드롭아웃 할 비율(r)을 지정합니다. 드롭아웃 하지 않는 뉴런의 출력은 1/(1 - r)만큼 증가시켜 출력의 총합이 같도록 만듭니다. . save_weights()는 모든 층의 가중치와 절편을 파일에 저장합니다. . 첫 번째 매개변수에 저장할 파일을 지정합니다. . save_format 매개변수에서 저장할 파일 포맷을 지정합니다. 기본적으로 텐서플로의 Checkpoint 포맷을 사용합니다. 이 매개변수를 h5&#39;로 지정하거나 파일의 확장자가 :h5&#39;이면 HDF5 포맷으로 저장됩니다.&gt; 2 load_weights() :모든 층의 가중치와 절편을 파일에 읽습니다.&gt; &gt;첫 번째 매개변수에 읽을 파일을 지정합니다. . 3 save() :모델 구조와 모든 가중치와 절편을 파일에 저장합니다.&gt; &gt;첫 번째 매개변수에 저장할 파일을 지정합니다. . save_format 매개변수에서 저장할 파일 포맷을 지정합니다. 기본적으로 텐서플로의 SavedModel 포맷을 사용합니다. 이 매개변수를 &#39;h5&#39;로 지정하거나 파일의 확장자가 :h5&#39;이면 HDF5 포맷으로 저장됩니다.&gt; 4 load_model() :model.save()로 저장된 모델을 로드합니다.&gt; &gt;첫 번째 매개변수에 읽을 파일을 지정합니다. . 5 ModelCheckpoint :케라스 모델과 가중치를 일정 간격으로 저장합니다.&gt; &gt;첫 번째 매개변수에 저장할 파일을 지정합니다. . monitor 매개변수는 모니터링할 지표를 지정합니다. 기본값은 &#39;val loss&#39;로 검증 손실을 관찰합니다. . save_weights_only 매개변수의 기본값은 False로 전체 모델을 저장합니다. True로 지정하면 모델의 가중치와 절편만 저장합니다. . save_best_only 매개변수를 True로 지정하면 가장 낮은 검증 점수를 만드는 모델을 저장합니다. . 6 EarlyStopping :관심 지표가 더이상 향상하지 않으면 훈련을 중지합니다.&gt; &gt;monitor 매개변수는 모니터링할 지표를 지정합니다. 기본값은 val loss&#39;로 검증 손실을 관찰합니다. . patience 매개변수에 모델이 더 이상 향상되지 않고 지속할 수 있는 최대 에포크 횟수를 지정합니다. . restore_best_weights 매개변수에 최상의 모델 가중치를 복원할지 지정합니다. 기본값은 False입니다. . - NumPy . 1 argmax :배열에서 축을 따라 최댓값의 인덱스를 반환합니다. . axis 매개변수에서 어떤 축을 따라 최댓값을 찾을지 지정합니다. 기본값은 None으로 전체 배열에서 최댓값을 찾습니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/08/%ED%98%BC%EA%B3%B5%EB%A8%B87%EC%B1%953%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/08/%ED%98%BC%EA%B3%B5%EB%A8%B87%EC%B1%953%EC%9E%A5.html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "혼공머 07-2",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 앙상블 from sklearn.ensemble import ExtraTreesClassifier # 엑스트라 트리 앙상블 from sklearn.ensemble import GradientBoostingClassifier # 그레이디언트 부스팅 앙상블 # 히스토그램 기반 그레이디언트 부스팅 from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.inspection import permutation_importance # 특성중요도 from xgboost import XGBClassifier # 알고리즘을 구현한 또다른 라이브러리1 from lightgbm import LGBMClassifier # 알고리즘을 구현한 또다른 라이브러리2 , 마이크로소프트에서 구현 # ____________________ from sklearn.cluster import KMeans # KMeans from sklearn.decomposition import PCA # 주성분 분석 # 7장 딥러닝 from tensorflow import keras # 케라스 import tensorflow as tf # 텐서플로 . . 2&#44060;&#51032; &#52789; . (train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() . # 2. 28x28 크기의 2차원 배열을 784 크기의 1차원 배열로 펼치기 # 3. 훈련세트와 검증세트 나누기 train_scaled = train_input / 255.0 train_scaled = train_scaled.reshape(-1, 28*28) train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) . &#47564;&#46304; &#51064;&#44277; &#49888;&#44221;&#47581; &#47784;&#54805;&#50640; 2&#44060;&#51032; &#52789; &#52628;&#44032; . 입력층과 출력층 사이에 밀집층이 추가된 것. $ to$ 은닉층 이라고 부름. . 은닉층에는 활성화 함수가 있음 (신경망 층의 선형 방정식의 계산 값에 적용하는 함수) . 소프트 맥스 함수도 활성화 함수 . 출력층에 적용하는 활성화 함수는 종류가 제한 (이진 분류 :시그모이드 함수, 다중 분류 : 소프트맥스 함수) 분류 문제는 클래스에 대한 확률을 출력하기 위해 활성화 함수를 사용, 회귀의 출력은 임의의 어떤 숫자라서 활성화 함수를 적용할 필요가 없음. 즉, 출력층의 선형 방정식의 계산을 그대로 출력 . 은닉층에서 선형적인 산술 계산만 수행한다면 수행 역할이 없는 셈이라서 활성화 함수 사용 . dense1 = keras.layers.Dense(100, activation=&#39;sigmoid&#39;, input_shape=(784,)) dense2 = keras.layers.Dense(10, activation=&#39;softmax&#39;) . dense1 이 은닉층이고 100개의 뉴런을 가진 밀집층, sigmoid로 활성화 함수 지정, 출력층의 뉴런보다는 많게 만들어야함. . dense2 :출력층 . &#49900;&#52789; &#49888;&#44221;&#47581; &#47564;&#46308;&#44592; (Deep Neural network) . model = keras.Sequential([dense1, dense2]) . 리스트로 전달 해야하며 , 출력층을 가장 마지막에 두어야 한다. . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 100) 78500 dense_1 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ . 모델의 이름이 나오며 , 그 모델에 들어 있는 층이 순서대로 나열 (은닉층에서 출력층의 순서) . 층마다 층 이름, 클래스 ,출력 크기 ,모델 파라미터 개수가 출력 . Output Shape :(샘플의 수 , 뉴런의 수)&gt; Total params :총 모델 파라미터 개수&gt; Trainable params :훈련되는 파라미터 개수 . &#52789;&#51012; &#52628;&#44032;&#54616;&#45716; &#45796;&#47480; &#48169;&#48277; . model = keras.Sequential([ keras.layers.Dense(100, activation=&#39;sigmoid&#39;, input_shape=(784,), name=&#39;hidden&#39;), keras.layers.Dense(10, activation=&#39;softmax&#39;, name=&#39;output&#39;) ], name=&#39;패션 MNIST 모델&#39;) . 추가되는 층을 한눈에 쉽게 알아볼 수 있는 장점이 있음. . 층의 이름은 반드시 영문이어야 함. . model.summary() . Model: &#34;패션 MNIST 모델&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= hidden (Dense) (None, 100) 78500 output (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ . 모델 이름과 층이름이 바뀜 . model = keras.Sequential() model.add(keras.layers.Dense(100, activation=&#39;sigmoid&#39;, input_shape=(784,))) model.add(keras.layers.Dense(10, activation=&#39;softmax&#39;)) . model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 100) 78500 dense_3 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ . model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) model.fit(train_scaled, train_target, epochs=5) . Epoch 1/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.5661 - accuracy: 0.8092 Epoch 2/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.4068 - accuracy: 0.8532 Epoch 3/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3728 - accuracy: 0.8646 Epoch 4/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3515 - accuracy: 0.8725 Epoch 5/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3335 - accuracy: 0.8782 . &lt;keras.callbacks.History at 0x7f1298643f10&gt; . 추가된 층이 성능을 향상시켰다는 것을 알 수 있다. . 몇개의 층이 추가되어도 compile() 과 fit() 메서드의 사용법은 동일하다. . &#47120;&#47336; &#54632;&#49688; (&#54876;&#49457;&#54868; &#54632;&#49688;) . 시그모이드 함수는 층이 많을수록 효과가 누적되어 학습을 더 어렵게 만듬 $ to$ 해결 :렐루 함수 $ to$ 입력이 양수일 경우 마치 활성화 함수가 없는 것처럼 그냥 입력을 통과 시키고 음수일 경우에는 0으로 만듬 . model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28, 28))) model.add(keras.layers.Dense(100, activation=&#39;relu&#39;)) model.add(keras.layers.Dense(10, activation=&#39;softmax&#39;)) . 첫 번째 Dense 층에 있던 input_shape 매개변수를 Flatten 층으로 옮김 . 첫 번째 Dense 층의 활성화 함수를 relu로 바꿈 . Flatten 클래스는 학습하는 층이 아님 . model.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense_4 (Dense) (None, 100) 78500 dense_5 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________ . flatten 클래스에 포함된 모델 파라미터는 0개 . 케라스dml flatten 층을 신경망 모형에 추가하면 입력값의 차원을 짐작할 수 있다. . 784개의 입력이 첫 번째 은닉층에 전달되었다. . 입력 데이터에 대한 전처리 과정을 가능한 모델에 포함 시킨다. :케라스 API . (train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() train_scaled = train_input / 255.0 train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) . model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) model.fit(train_scaled, train_target, epochs=5) . Epoch 1/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.5315 - accuracy: 0.8127 Epoch 2/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3930 - accuracy: 0.8584 Epoch 3/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3572 - accuracy: 0.8723 Epoch 4/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3345 - accuracy: 0.8813 Epoch 5/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3215 - accuracy: 0.8850 . &lt;keras.callbacks.History at 0x7f12983d73a0&gt; . 시그모이드 함수를 사용했을 때와 비교하면 성능이 조금 향상 . model.evaluate(val_scaled, val_target) . 375/375 [==============================] - 0s 897us/step - loss: 0.3641 - accuracy: 0.8789 . [0.36406102776527405, 0.8789166808128357] . 은닉층을 추가하지 않은 경우보다 몇 퍼센트 성능이 향상 . &#50741;&#54000;&#47560;&#51060;&#51200; . 은닉층의 뉴런 개수도 하이퍼 파라미터이다. . 층의 종류도 하이퍼 파라미터 이다. . 옵티마이저 :케라스에서 제공하는 다양한 경사 하강법 알고리즘 RMSprop의 학습률 또한 하이터 파라미터 . &#44032;&#51109; &#44592;&#48376;&#51201;&#51064; &#50741;&#54000;&#47560;&#51060;&#51200; : &#54869;&#47456;&#51201; &#44221;&#49324; &#54616;&#44053;&#48277; SGD . model.compile(optimizer=&#39;sgd&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) . sgd = keras.optimizers.SGD() model.compile(optimizer=sgd, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) . sgd = keras.optimizers.SGD(learning_rate=0.1) . &#47784;&#47704;&#53568; &#52572;&#51201;&#54868; . 그레이디언트를 가속도처럼 사용 . SGD 클래스의 nesterov 매개변수를 기본값 False에서 True로 바꾸면 네스테로프 모멘텀 최적화 를 사용 . sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True) . &#51201;&#51025;&#51201; &#54617;&#49845;&#47456; . adagrad = keras.optimizers.Adagrad() model.compile(optimizer=adagrad, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) . rmsprop = keras.optimizers.RMSprop() model.compile(optimizer=rmsprop, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) . model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28, 28))) model.add(keras.layers.Dense(100, activation=&#39;relu&#39;)) model.add(keras.layers.Dense(10, activation=&#39;softmax&#39;)) . model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) model.fit(train_scaled, train_target, epochs=5) . Epoch 1/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.5299 - accuracy: 0.8139 Epoch 2/5 1500/1500 [==============================] - 2s 1ms/step - loss: 0.3942 - accuracy: 0.8590 Epoch 3/5 1500/1500 [==============================] - 1s 974us/step - loss: 0.3546 - accuracy: 0.8698 Epoch 4/5 1500/1500 [==============================] - 1s 990us/step - loss: 0.3245 - accuracy: 0.8810 Epoch 5/5 1500/1500 [==============================] - 1s 984us/step - loss: 0.3063 - accuracy: 0.8852 . &lt;keras.callbacks.History at 0x7f129cb65e80&gt; . model.evaluate(val_scaled, val_target) . 375/375 [==============================] - 0s 729us/step - loss: 0.3352 - accuracy: 0.8794 . [0.3352320194244385, 0.8794166445732117] . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 심층 신경망 : 2개 이상의 층을 포함한 신경망입니다. 종종 다층 인공 신경망, 심층 신경망, 딥러닝을 같은 의미로 사용합니다. . - 렐루 함수 : 이미지 분류 모델의 은닉층에 많이 사용하는 활성화 함수입니다. 시그모이드 함수는 층이 많을수록 활성화 함수의 양쪽 끝에서 변화가 작기 때문에 학습이 어려워집니다. 렐루함수는 이런 문제가 없으며 계산도 간단합니다. . - 옵티마이저 : 신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법을 말합니다. 케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있습니다. 대표적으로 SGD, 네스테로프 모멘텀, RMSprop, Adam 등이 있습니다. . - 핵심 패키지 . - TensorFlow . 1 add() :케라스 모델에 층을 추가하는 메서드입니다.&gt; &gt;케라스 모델의 add() 메서드는 keras.layers 패키지 아래에 있는 층의 객체를 입력받아 신경망 모델에 추가합니다. add() 메서드를 호출하여 전달한 순서대로 층이 차례대로 늘어납니다. . 2 summary () :케라스 모델의 정보를 출력하는 메서드입니다.&gt; &gt;모델에 추가된 층의 종류와 순서, 모델 파라미터 개수를 출력합니다. 층을 만들 때 name 매개변수로 이름을 지정하면 summary() 메서드 출력에서 구분하기 쉽습니다. . percentiles 매개변수에서 백분위수를 지정합니다. 기본값은 [0.25, 0.5, 0.75]입니다. . 3 SGD :기본 경사 하강법 옵티마이저 클래스입니다.&gt; &gt;learning_rate 매개변수로 학습률을 지정하며 기본값은 0.01입니다 . momentum 매개변수에 이 이상의 값을 지정하면 모멘텀 최적화를 수행합니다. . nesteroy 매개변수를 True로 설정하면 네스테로프 모멘텀 최적화를 수행합니다. . 4 Adagrad :Adagrad 옵티마이저 클래스입니다.&gt; &gt;learning_rate 매개변수로 학습률을 지정하며 기본값은 0.001입니다. learning_rate 매개변수로 학습률을 지정하며 기본값은 0.001입니다. learning_rate 매개변수로 학습률을 지정하며 기본값은 0.001입니다. . Adagrad는 그레이디언트 제곱을 누적하여 학습률을 나눕니다. initial accumulator Value 매개변수에서 누적 초깃값을 지정할 수 있으며 기본값은 0.1입니다. . 5 RMSprop :RMSprop 옵티마이저 클래스입니다.&gt; &gt;Adagrad처럼 그레이디언트 제곱으로 학습률을 나누지만 최근의 그레이디언트를 사용하기 위해 지수 감소를 사용합니다. rho 매개변수에서 감소 비율을 지정하며 기본값은 0.9입니다. . 6 Adam :Adam 옵티마이저 클래스입니다. . 모멘텀 최적화에 있는 그레이디언트의 지수 감소 평균을 조절하기 위해 beta 1 매개변수가 있으며 기본값은 0.9 입니다. . RMSprop에 있는 그레이디언트 제곱의 지수 감소 평균을 조절하기 위해 beta 2 매개변수가 있으며 기본값은 0.999입니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/05/%ED%98%BC%EA%B3%B5%EB%A8%B87%EC%B1%952%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/05/%ED%98%BC%EA%B3%B5%EB%A8%B87%EC%B1%952%EC%9E%A5.html",
            "date": " • Mar 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "혼공머 07-1",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 앙상블 from sklearn.ensemble import ExtraTreesClassifier # 엑스트라 트리 앙상블 from sklearn.ensemble import GradientBoostingClassifier # 그레이디언트 부스팅 앙상블 # 히스토그램 기반 그레이디언트 부스팅 from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.inspection import permutation_importance # 특성중요도 from xgboost import XGBClassifier # 알고리즘을 구현한 또다른 라이브러리1 from lightgbm import LGBMClassifier # 알고리즘을 구현한 또다른 라이브러리2 , 마이크로소프트에서 구현 # ____________________ from sklearn.cluster import KMeans # KMeans from sklearn.decomposition import PCA # 주성분 분석 # 7장 딥러닝 from tensorflow import keras # 케라스 import tensorflow as tf # 텐서플로 . . &#54056;&#49496; MNIST . 많이 연습하는 데이터셋 , 손으로 쓴 0~9 까지의 숫자로 구성된 원래 데이터를 숫자대신 패션아이템으로 구성 . (train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 2s 0us/step 26435584/26421880 [==============================] - 2s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step . load_data() 로 훈련데이터와 테스트데이터를 자동으로 분류 . print(train_input.shape, train_target.shape) . (60000, 28, 28) (60000,) . 훈련데이터:60000개의 이미지와 각 이미지는 28 x 28 크기&gt; 타깃데이터:60000개의 원소가 있는 1차원 배열 . print(test_input.shape, test_target.shape) . (10000, 28, 28) (10000,) . import matplotlib.pyplot as plt fig, axs = plt.subplots(1, 10, figsize=(10,10)) for i in range(10): axs[i].imshow(train_input[i], cmap=&#39;gray_r&#39;) axs[i].axis(&#39;off&#39;) plt.show() . 28x28이라서 꽤 작고 흐릿 . print([train_target[i] for i in range(10)]) . [9, 0, 0, 3, 0, 2, 7, 2, 5, 5] . - 샘플들의 의미 . 레이블 0 1 2 3 4 5 6 7 8 9 . 패션아이템 | 티셔츠 | 바지 | 스웨터 | 드레스 | 코트 | 샌달 | 셔츠 | 스니커즈 | 가방 | 앵클 부츠 | . print(np.unique(train_target, return_counts=True)) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) . 6000개씩 포함 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47196; &#54056;&#49496; &#50500;&#51060;&#53596; &#48516;&#47448;&#54616;&#44592; . 샘플을 하나씩 꺼내서 모델을 훈련하는 방법이 더 효율적 :경사하강법 이용&gt; 0~255의 픽셀을 0~1 로 정규화 :전처리 . train_scaled = train_input / 255.0 train_scaled = train_scaled.reshape(-1, 28*28) . print(train_scaled.shape) . (60000, 784) . sc = SGDClassifier(loss=&#39;log&#39;, max_iter=5, random_state=42) scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1) print(np.mean(scores[&#39;test_score&#39;])) . 0.8196000000000001 . sc = SGDClassifier(loss=&#39;log&#39;, max_iter=9, random_state=42) scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1) print(np.mean(scores[&#39;test_score&#39;])) . 0.8303666666666667 . sc = SGDClassifier(loss=&#39;log&#39;, max_iter=20, random_state=42) scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1) print(np.mean(scores[&#39;test_score&#39;])) . 0.8436666666666666 . &#51064;&#44277; &#49888;&#44221;&#47581; . 확률적 경사 하강법을 이용한 로지스틱 회귀 :가장 기본적인 인공 신경망 출력층 , z값(뉴런 = 유닛) , 입력층 . &#52992;&#46972;&#49828; &#46972;&#51060;&#48652;&#47084;&#47532; . 직접 GPU 연산을 수행하지 않음 . 대신 GPU 연산을 수행하는 다른 라이브러리를 &#39;백엔드&#39;로 사용 . &#51064;&#44277;&#49888;&#44221;&#47581;&#51004;&#47196; &#47784;&#45944; &#47564;&#46308;&#44592; . train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42) . print(train_scaled.shape, train_target.shape) . (48000, 784) (48000,) . print(val_scaled.shape, val_target.shape) . (12000, 784) (12000,) . &#51064;&#44277;&#49888;&#44221;&#47581;&#51032; &#50724;&#47480;&#51901;&#50640; &#45459;&#51064; &#52789; &#47564;&#46308;&#44592; . 밀집층 (=완전 연결층) . dense = keras.layers.Dense(10, activation=&#39;softmax&#39;, input_shape=(784,)) . - dense = keras.layers.Dense(뉴런수, 뉴런의 출력에 적용할 함수, 입력의 크기) . - 소프트 맥스와 같이 뉴런의 선형 방정식 계산결과에 적용되는 함수 : 활성화 함수 . model = keras.Sequential(dense) . &#51064;&#44277;&#49888;&#44221;&#47581;&#51004;&#47196; &#54056;&#49496; &#50500;&#51060;&#53596; &#48516;&#47448;&#54616;&#44592; . model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, metrics=&#39;accuracy&#39;) . print(train_target[:10]) . [7 3 5 8 6 9 3 3 9 9] . model.fit(train_scaled, train_target, epochs=5) . Epoch 1/5 1500/1500 [==============================] - 2s 976us/step - loss: 0.6094 - accuracy: 0.7941 Epoch 2/5 1500/1500 [==============================] - 1s 841us/step - loss: 0.4787 - accuracy: 0.8394 Epoch 3/5 1500/1500 [==============================] - 1s 764us/step - loss: 0.4558 - accuracy: 0.8465 Epoch 4/5 1500/1500 [==============================] - 1s 762us/step - loss: 0.4438 - accuracy: 0.8534 Epoch 5/5 1500/1500 [==============================] - 1s 750us/step - loss: 0.4360 - accuracy: 0.8546 . &lt;keras.callbacks.History at 0x7f30645bf610&gt; . 5번 반복에 정확도가 85%가 됨 . model.evaluate(val_scaled, val_target) . 375/375 [==============================] - 0s 632us/step - loss: 0.4495 - accuracy: 0.8528 . [0.44948506355285645, 0.8528333306312561] . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 인공 신경망 : 생물학적 뉴런에서 영감을 받아 만든 머신러닝 알고리즘입니다. 이름이 신경망이지만 실제 우리 뇌를 모델링한 것은 아닙니다. 신경망은 기존의 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘하면서 크게 주목받고 있습니다. 인공 신경망 알고리즘을 종종 딥러닝이라고도 부릅니다. . - 텐서플로 : 구글이 만든 딥러닝 라이브러리로 매우 인기가 높습니다. CPU와 GPU를 사용해 인공 신경망 모델을 효율적으로 훈련하며 모델 구축과 서비스에 필요한 다양한 도구를 제공합니다. 텐서플로 2.0부터는 신경망 모델을 빠르게 구성할 수 있는 케라스를 핵심 API로 채택하였습니다. 케라스를 사용하면 간단한 모델에서 아주 복잡한 모델까지 손쉽게 만들 수 있습니다. . - 밀집층 : 가장 간단한 인공 신경망의 층입니다. 인공 신경망에는 여러 종류의 층이 있습니다. 밀집층에서는 뉴런들이 모두 연결되어 있기 때문에 완전 연결 층이라고도 부릅니다. 특별히 출력층에 밀집층을 사용할 때는 분류하려는 클래스와 동일한 개수의 뉴런을 사용합니다. . - 원-핫 인코딩 : 정숫값을 배열에서 해당 정수 위치의 원소만 1이고 나머지는 모두 0으로 변환합니다. 이런 변환이 필요한 이유는 다중 분류에서 출력층에서 만든 확률과 크로스 엔트로피손실을 계산하기 위해서 입니다. 텐서플로에서는 &#39;sparse_categorical_entropy&#39; 손실을 지정하면 이런 변환을 수행할 필요가 없습니다. . - 핵심 패키지 . - TensorFlow . 1 Dense :신경망에서 가장 기본 층인 밀집층을 만드는 클래스입니다.&gt; &gt;이 층에 첫 번째 매개변수에는 뉴런의 개수를 지정합니다. . activation 매개변수에는 사용할 활성화 함수를 지정합니다. 대표적으로 sigmoid&#39;, &#39;softmax 함수가 있습니다. 아무것도 지정하지 않으면 활성화 함수를 사용하지 않습니다. . 케라스의 Sequential 클래스에 맨 처음 추가되는 층에는 input shape 매개변수로 입력의 크기를 지정해야 합니다. . 2 Sequential :케라스에서 신경망 모델을 만드는 클래스입니다.&gt; &gt;이 클래스의 객체를 생성할 때 신경망 모델에 추가할 층을 지정할 수 있습니다. 추가할 층이 1개 이상일 경우 파이썬 리스트로 전달합니다. . 3 compile() :모델 객체를 만든 후 훈련하기 전에 사용할 손실 함수와 측정 지표 등을 지정하는 메서드입니다.&gt; &gt;loss 매개변수에 손실 함수를 지정합니다. 이진 분류일 경우 &#39;binary_crossentropy&#39;, 다중 분류일 경우 categorical_crossentropy&#39;를 지정합니다. 클래스 레이블이 정수일 경우 &#39;sparse_categorical_crossentropy&#39;로 지정합니다. 회귀 모델일 경우 mean_square_error&#39; 등으로 지정할 수 있습니다. . metrics 매개변수에 훈련 과정에서 측정하고 싶은 지표를 지정할 수 있습니다. 측정 지표가 1개 이상일 경우 리스트로 전달합니다. . 4 fit() :모델을 훈련하는 메서드입니다.&gt; &gt;첫 번째와 두 번째 매개변수에 입력과 타깃 데이터를 전달합니다. . epochs 매개변수에 전체 데이터에 대해 반복할 에포크 횟수를 지정합니다. . 5 evaluate() :모델 성능을 평가하는 메서드입니다. . 첫 번째와 두 번째 매개변수에 입력과 타깃 데이터를 전달합니다. . Compile() 메서드에서 loss 매개변수에 지정한 손실 함수의 값과 metrics 매개변수에서 지정한 측정 지표를 출력합니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/04/%ED%98%BC%EA%B3%B5%EB%A8%B87%EC%B1%951%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/04/%ED%98%BC%EA%B3%B5%EB%A8%B87%EC%B1%951%EC%9E%A5.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "혼공머 06-3",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 앙상블 from sklearn.ensemble import ExtraTreesClassifier # 엑스트라 트리 앙상블 from sklearn.ensemble import GradientBoostingClassifier # 그레이디언트 부스팅 앙상블 # 히스토그램 기반 그레이디언트 부스팅 from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.inspection import permutation_importance # 특성중요도 from xgboost import XGBClassifier # 알고리즘을 구현한 또다른 라이브러리1 from lightgbm import LGBMClassifier # 알고리즘을 구현한 또다른 라이브러리2 , 마이크로소프트에서 구현 # ____________________ from sklearn.cluster import KMeans # KMeans from sklearn.decomposition import PCA # 주성분 분석 . . &#52264;&#50896;&#44284; &#52264;&#50896; &#52629;&#49548; . 특성 :데이터가 가진 속성 = 차원&gt; 비지도 학습 작업 중 하나 :차원 축소&gt; 차원 축소 :데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터 크기를 줄이고 지도 학습 모델의 성능을 향상&gt; 주성분 분석 :대표적 차원 축소 알고리즘 = PCA . &#51452;&#49457;&#48516; &#48516;&#49437; . 데이터에 있는 분산이 큰 방향을 찾는 것으로 이해 . 분산 :데이터가 널리 퍼져있는 정도를 말함 주성분 벡터의 원소 수 = 원본 데이터셋에 있는 특성 수 . 주성분은 원본 차원과 같고 주성분으로 바꾼 데이터는 차원이 줄어든다. . PCA &#53364;&#47000;&#49828; . 다운로드 !wget https://bit.ly/fruits_300_data -O fruits_300.npy . fruits = np.load(&#39;fruits_300.npy&#39;) fruits_2d = fruits.reshape(-1, 100*100) . pca = PCA(n_components=50) pca.fit(fruits_2d) . PCA(n_components=50) . print(pca.components_.shape) . (50, 10000) . n_components=50 로 지정했기 때문에 첫 번째 차원이 50 (50개의 주성분) , 두 번째 차원은 데이터수 . 데이터수가 같으므로 이미지 출력 가능 . def draw_fruits(arr, ratio=1): n = len(arr) rows = int(np.ceil(n/10)) cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: axs[i, j].imshow(arr[i*10 + j], cmap=&#39;gray_r&#39;) axs[i, j].axis(&#39;off&#39;) plt.show() . draw_fruits(pca.components_.reshape(-1, 100, 100)) . 원본 데이터를 주성분에 투영하여 특성의 개수를 10000개에서 50개 로 줄일 수 있다. . print(fruits_2d.shape) . (300, 10000) . fruits_pca = pca.transform(fruits_2d) print(fruits_pca.shape) . (300, 50) . &#50896;&#48376; &#45936;&#51060;&#53552; &#51116;&#44396;&#49457; . fruits_inverse = pca.inverse_transform(fruits_pca) print(fruits_inverse.shape) . (300, 10000) . fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100) for start in [0, 100, 200]: draw_fruits(fruits_reconstruct[start:start+100]) print(&quot; n&quot;) . . . . 특성이 잘 복원 됨 . &#49444;&#47749;&#46108; &#48516;&#49328; explained variance . 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값 . print(np.sum(pca.explained_variance_ratio_)) . 0.9215569399184715 . plt.plot(pca.explained_variance_ratio_) plt.show() . &#45796;&#47480; &#50508;&#44256;&#47532;&#51608;&#44284; &#54632;&#44760; &#49324;&#50857;&#54616;&#44592; . lr = LogisticRegression() . target = np.array([0] * 100 + [1] * 100 + [2] * 100) . scores = cross_validate(lr, fruits_2d, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) . 0.9966666666666667 0.40227322578430175 . 축소 값과 비교 . scores = cross_validate(lr, fruits_pca, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) . 1.0 0.01966242790222168 . pca = PCA(n_components=0.5) pca.fit(fruits_2d) . PCA(n_components=0.5) . print(pca.n_components_) . 2 . fruits_pca = pca.transform(fruits_2d) print(fruits_pca.shape) . (300, 2) . scores = cross_validate(lr, fruits_pca, target) print(np.mean(scores[&#39;test_score&#39;])) print(np.mean(scores[&#39;fit_time&#39;])) . 0.9933333333333334 0.03802709579467774 . /home/cjfal/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression /home/cjfal/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . 로지스틱 회귀모델이 완전히 수렴하지 못해서 경고 . 특성 2개만 사용했는데 정확도가 99% . km = KMeans(n_clusters=3, random_state=42) km.fit(fruits_pca) print(np.unique(km.labels_, return_counts=True)) . (array([0, 1, 2], dtype=int32), array([110, 99, 91])) . for label in range(0, 3): draw_fruits(fruits[km.labels_ == label]) print(&quot; n&quot;) . . . . 파인애플은 사과와 조금 혼동됨 . 차원을 줄여 시각화에 이득이 됨 . for label in range(0, 3): data = fruits_pca[km.labels_ == label] plt.scatter(data[:,0], data[:,1]) plt.legend([&#39;apple&#39;, &#39;banana&#39;, &#39;pineapple&#39;]) plt.show() . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 차원 축소 : 원본 데이터의 특성을 적은 수의 새로운 특성으로 변환하는 비지도 학습의 한 종류입니다. 차원 축소는 저장 공간을 줄이고 시각화하기 쉽습니다. 또한 다른 알고리즘의 성능을 높일 수도 있습니다. . - 주성분 분석 : 차원 축소 알고리즘의 하나로 데이터에서 가장 분산이 큰 방향을 찾는 방법입니다. 이런 방향을 주성분이라고 부릅니다. 원본 데이터를 주성분에 투영하여 새로운 특성을 만들 수 있습니다. 일반적으로 주성분은 원보 데이터에 있는 특성 개수보다 작습니다. . - 설명된 분산 : 주성분 분석에서 주성분이 얼마나 원본 데이터의 분산을 잘 나타내는지 기록한 것입니다. 사이킷런의 PCA 클래스는 주성분 개수나 설명된 분산의 비율을 지정하여 주성분 분석을 수행할 수 있습니다. . * 핵심 패키지 . * scikit-learn . 1 PCA :주성분 분석을 수행하는 클래스입니다. . -n_components는 주성분의 개수를 지정합니다. 기본값은 None으로 샘플 개수와 특성 개수 중에 작은 것의 값을 사용합니다. . -random_state에는 넘파이 난수 시드 값을 지정할 수 있습니다. . -components_ 속성에는 훈련 세트에서 찾은 주성분이 저장됩니다. . -explainedvariance 속성에는 설명된 분산이 저장되고, explained_variance_ratio_에는 설명된 분산의 비율이 저장됩니다. . -inverse_transform() 메서드는 transform() 메서드로 차원을 축소시킨 데이터를 다시 원본 차원으로 복원합니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/03/%ED%98%BC%EA%B3%B5%EB%A8%B86%EC%B1%953%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/03/%ED%98%BC%EA%B3%B5%EB%A8%B86%EC%B1%953%EC%9E%A5.html",
            "date": " • Mar 3, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "혼공머 06-2",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 앙상블 from sklearn.ensemble import ExtraTreesClassifier # 엑스트라 트리 앙상블 from sklearn.ensemble import GradientBoostingClassifier # 그레이디언트 부스팅 앙상블 # 히스토그램 기반 그레이디언트 부스팅 from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.inspection import permutation_importance # 특성중요도 from xgboost import XGBClassifier # 알고리즘을 구현한 또다른 라이브러리1 from lightgbm import LGBMClassifier # 알고리즘을 구현한 또다른 라이브러리2 , 마이크로소프트에서 구현 # ____________________ from sklearn.cluster import KMeans # KMeans . . k-&#54217;&#44512; &#50508;&#44256;&#47532;&#51608; . 1 무작위로 k개의 클러스터 중심을 정함 . 2 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정 . 3 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경 . 4 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복 . KMeans &#53364;&#47000;&#49828; . # !wget https://bit.ly/fruits_300_data -O fruits_300.npy . --2022-03-02 20:18:01-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-03-02 20:18:02-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 52.78.231.108 Connecting to github.com (github.com)|52.78.231.108|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-03-02 20:18:02-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M 6.53MB/s in 0.4s 2022-03-02 20:18:03 (6.53 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] . fruits = np.load(&#39;fruits_300.npy&#39;) fruits_2d = fruits.reshape(-1, 100*100) . km = KMeans(n_clusters=3, random_state=42) km.fit(fruits_2d) . KMeans(n_clusters=3, random_state=42) . 비지도 학습이라 따로 fit 메서드에 타깃 데이터를 사용하지 않음 . print(km.labels_) . [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] . print(np.unique(km.labels_, return_counts=True)) . (array([0, 1, 2], dtype=int32), array([111, 98, 91])) . def draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수 # 한 줄에 10개씩 이미지를 그림 샘플 개수를 10으로 나누어 전체 행 개수를 계산 rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수. 그렇지 않으면 10개. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개 까지만 그림. axs[i, j].imshow(arr[i*10 + j], cmap=&#39;gray_r&#39;) axs[i, j].axis(&#39;off&#39;) plt.show() . draw_fruits(fruits[km.labels_==0]) . 모두 파인애플은 아님, 그래도 잘 구별해냄 . draw_fruits(fruits[km.labels_==1]) . 모두 바나나 . draw_fruits(fruits[km.labels_==2]) . 모두 사과 . &#53364;&#47084;&#49828;&#53552; &#51473;&#49900; . draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) . print(km.transform(fruits_2d[100:101])) . [[3393.8136117 8837.37750892 5267.70439881]] . print(km.predict(fruits_2d[100:101])) . [0] . 0으로 예측 (파인애플이 많았었음) . draw_fruits(fruits[100:101]) . print(km.n_iter_) . 4 . &#52572;&#51201;&#51032; k &#52286;&#44592; . &#50648;&#48372;&#50864; &#48169;&#48277; . 이너셔 :클러스터 중심과 클러스터에 속한 샘플 사이의 거리의 제곱의 합 (클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지를 나타내는 값으로 생각 가능) 일반적으로 클러스터 개수가 늘면 클러스터 각각의 크기는 줄기 때문에 이너셔의 크기도 준다. . 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는다. . inertia = [] for k in range(2, 7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_) plt.plot(range(2, 7), inertia) plt.xlabel(&#39;k&#39;) plt.ylabel(&#39;inertia&#39;) plt.show() . 꺾이는 곳이 최적의 클러스터 개수 , k=3 에서 희미하게 값이 변한것이 보인다. . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - k-평균 : 알고리즘은 처음에 랜덤하게 클러스터 중심을 정하고 클러스터를 만듭니다. 그다음 클러스터의 중심을 이동하고 다시 클러스터를 만드는 식으로 반복해서 최적의 클러스터를 구성하는 알고리즘입니다. . - 클러스터 중심 : k-평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값입니다. 센트로이드centroid 라고도 부릅니다. 가장 가까운 클러스터 중심을 샘플의 또 다른 특성으로 사용하거나 새로운 샘플에 대한 예측으로 활용할 수 있습니다. . - 엘보우 방법 : 최적의 클러스터 개수를 정하는 방법 중 하나입니다. 이너셔는 클러스터 중심과 샘플 사이 거리의 제곱 합입니다. 클러스터 개수에 따라 이넛 감소가 꺾이는 지점이 적절한 클러스터 개수 k가 될 수 있습니다. 이 그래프의 모양을 따서 엘보우 방법이라고 부릅니다. . * 핵심 패키지 . * scikit-learn . 1 KMeans :k-평균 알고리즘 클래스입니다. . -n_clusters에는 클러스터 개수를 지정합니다. 기본값은 8입니다. . -처음에 랜덤하게 센트로이드를 초기화하기 때문에 여러 번 반복하여 이너셔를 기준으로 가장 좋은 결과를 선택합니다. n_inint는 이 반복 횟수를 지정합니다. 기본값은 10입니다. . -max_iter k-평균 알고리즘의 한 번 실행에서 최적의 센트로이드를 찾기 위해 반복할 수 있는 최대 횟수입니다. 기본값은 200입니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/02/%ED%98%BC%EA%B3%B5%EB%A8%B86%EC%B1%952%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/02/%ED%98%BC%EA%B3%B5%EB%A8%B86%EC%B1%952%EC%9E%A5.html",
            "date": " • Mar 2, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "혼공머 06-1",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 앙상블 from sklearn.ensemble import ExtraTreesClassifier # 엑스트라 트리 앙상블 from sklearn.ensemble import GradientBoostingClassifier # 그레이디언트 부스팅 앙상블 # 히스토그램 기반 그레이디언트 부스팅 from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.inspection import permutation_importance # 특성중요도 from xgboost import XGBClassifier # 알고리즘을 구현한 또다른 라이브러리1 from lightgbm import LGBMClassifier # 알고리즘을 구현한 또다른 라이브러리2 , 마이크로소프트에서 구현 # ____________________ . . &#53440;&#44611;&#51012; &#47784;&#47476;&#45716; &#48708;&#51648;&#46020; &#54617;&#49845; . 비지도 학습 :타깃이 없을 때 사용하는 머신러닝. 픽셀값 평균 ? . &#44284;&#51068; &#49324;&#51652; &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . # npy 파일이 다운로드 된다. . fruits = np.load(&#39;fruits_300.npy&#39;) print(fruits.shape) . (300, 100, 100) . (샘플 수 , 이미지 높이 , 이미지 너비) . print(fruits[0, 0, :]) . [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 3 2 1 2 1 1 1 1 2 1 3 2 1 3 1 4 1 2 5 5 5 19 148 192 117 28 1 1 2 1 4 1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] . 이 넘파이 배열은 흑백 사진을 담고 있다. (0~255의 정숫값) . plt.imshow(fruits[0], cmap=&#39;gray&#39;) # 흑백이라 gray plt.show() . 0에 가까울수록 검게 나타나고 높은 값은 밝게 표시 . 사진을 넘파이 배열로 변환할 때 반전시킨 것 . 우리의 관심 대상은 바탕이 아니라 사과라서 바탕에 힘을 준것.( 픽셀값이 0이면 출력도 0이 되어 의미가 없어짐, 즉 배경이 의미가 없게 만드는 것) . plt.imshow(fruits[0], cmap=&#39;gray_r&#39;) plt.show() . fig, axs = plt.subplots(1, 2) axs[0].imshow(fruits[100], cmap=&#39;gray_r&#39;) axs[1].imshow(fruits[200], cmap=&#39;gray_r&#39;) plt.show() . 서브플롯으로 나란히 출력 . &#54589;&#49472;&#44050; &#48516;&#49437;&#54616;&#44592; . 100x100을 펼쳐서 1차원 10000 으로 만들기(배열 계산에 용이) . apple = fruits[0:100].reshape(-1, 100*100) pineapple = fruits[100:200].reshape(-1, 100*100) banana = fruits[200:300].reshape(-1, 100*100) . print(apple.shape) . (100, 10000) . # 평균을 계산할 축 지정 : 1 (두번째 축인 열을 따라 계산) # 평균을 계산할 축 지정 : 0 (첫번째 축인 행을 따라 계산) print(apple.mean(axis=1)) . [ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999 90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.5078 87.2019 88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495 94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744 97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492 90.711 102.3193 87.1629 89.8751 86.7327 86.3991 95.2865 89.1709 96.8163 91.6604 96.1065 99.6829 94.9718 87.4812 89.2596 89.5268 93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159 102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022 82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252 87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604 81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823 101.556 100.7027 91.6098 88.8976] . plt.hist(np.mean(apple, axis=1), alpha=0.8) plt.hist(np.mean(pineapple, axis=1), alpha=0.8) plt.hist(np.mean(banana, axis=1), alpha=0.8) plt.legend([&#39;apple&#39;, &#39;pineapple&#39;, &#39;banana&#39;]) plt.show() . legend 로 주석 생성 . fig, axs = plt.subplots(1, 3, figsize=(20, 5)) axs[0].bar(range(10000), np.mean(apple, axis=0)) axs[1].bar(range(10000), np.mean(pineapple, axis=0)) axs[2].bar(range(10000), np.mean(banana, axis=0)) plt.show() . apple_mean = np.mean(apple, axis=0).reshape(100, 100) pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100) banana_mean = np.mean(banana, axis=0).reshape(100, 100) fig, axs = plt.subplots(1, 3, figsize=(20, 5)) axs[0].imshow(apple_mean, cmap=&#39;gray_r&#39;) axs[1].imshow(pineapple_mean, cmap=&#39;gray_r&#39;) axs[2].imshow(banana_mean, cmap=&#39;gray_r&#39;) plt.show() . &#54217;&#44512;&#44050; &#44284; &#44032;&#44620;&#50868; &#49324;&#51652; &#44256;&#47476;&#44592; . 절댓값 오차 이용 . abs_diff = np.abs(fruits - apple_mean) abs_mean = np.mean(abs_diff, axis=(1,2)) print(abs_mean.shape) . (300,) . apple_index = np.argsort(abs_mean)[:100] fig, axs = plt.subplots(10, 10, figsize=(10,10)) for i in range(10): for j in range(10): axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap=&#39;gray_r&#39;) axs[i, j].axis(&#39;on&#39;) # on 으로 하면 좌표까지 나옴 plt.show() . apple_mean과 가장 가까운 사진 100개를 골랐더니 전부 사과 . apple_index = np.argsort(abs_mean)[:100] fig, axs = plt.subplots(10, 10, figsize=(10,10)) for i in range(10): for j in range(10): axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap=&#39;gray_r&#39;) axs[i, j].axis(&#39;off&#39;) plt.show() . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 비지도 학습 : 머신러닝의 한 종류로 훈련 데이터에 타깃이 없습니다. 타깃이 없기 때문에 외부의 도움없이 스스로 유용한 무언가를 학습해야 합니다. 대표적인 비지도 학습 작업은 군집, 차원 축소 등입니다. . - 히스토그램 : 구간별로 값이 발생한 빈도를 그래프로 표시한 것입니다. 보통 x축이 값의 구간 (계급)이고 y축은 발생 빈도(빈수)입니다. . - 군집 : 비슷한 샘플끼리 하나의 그룹으로 모으는 대표적인 비지도 학습 작업입니다. 군집 알고리즘으로 모은 샘플 그룹을 클러스터라고 부릅니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/01/%ED%98%BC%EA%B3%B5%EB%A8%B86%EC%B1%951%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/03/01/%ED%98%BC%EA%B3%B5%EB%A8%B86%EC%B1%951%EC%9E%A5.html",
            "date": " • Mar 1, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "혼공머 05-3",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 앙상블 from sklearn.ensemble import ExtraTreesClassifier # 엑스트라 트리 앙상블 from sklearn.ensemble import GradientBoostingClassifier # 그레이디언트 부스팅 앙상블 # 히스토그램 기반 그레이디언트 부스팅 from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.inspection import permutation_importance # 특성중요도 from xgboost import XGBClassifier # 알고리즘을 구현한 또다른 라이브러리1 from lightgbm import LGBMClassifier # 알고리즘을 구현한 또다른 라이브러리2 , 마이크로소프트에서 구현 # ____________________ . . &#51221;&#54805; &#45936;&#51060;&#53552;&#50752; &#48708;&#51221;&#54805; &#45936;&#51060;&#53552; . 정형 데이터 :어떤 구조(주로 숫자)로 되어 있다는 뜻. CSV ,DB , EXCEL 등에 저장하기 쉽다&gt; 비정형 데이터 :텍스트 데이터 , 디카 사진 등 . &#50521;&#49345;&#48660; &#54617;&#49845; (ensemble learning) . 정형 데이터를 다루는 데 가장 뛰어나 성과를 내는 알고리즘 . &#47004;&#45924; &#54252;&#47112;&#49828;&#53944; . 앙상블 학습의 대표주자 . 결정트리를 랜덤하게 만들어 결저 트리(나무)의 숲을 만듦 . 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데, 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다. ( 한 샘플이 중복되어 추출 될 수 있음. / 부스트랩 샘플이라고 부름) . 부스트랩 샘플은 훈련 세트와 크기가 같다. ( 부스트랩 샘플에 포함되지 않고 남는 샘플 :OOB, out of bag)&gt; 부스트랩 방식 :데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미. RandomForestClassifier 는 자체적으로 모델을 평가하는 정수를 얻을 수 있다. . wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) data = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() target = wine[&#39;class&#39;].to_numpy() train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42) . rf = RandomForestClassifier(n_jobs=-1, random_state=42) scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.9973541965122431 0.8905151032797809 . rf.fit(train_input, train_target) print(rf.feature_importances_) . [0.23167441 0.50039841 0.26792718] . 결정 트리에서 보다 더 골고루 중요도가 분포되었다. . 과대적합을 줄이고 일반화 성능을 높이는데 기여 . rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42) rf.fit(train_input, train_target) print(rf.oob_score_) . 0.8934000384837406 . OOB 점수는 교차검증을 대신 할 수 있다. . &#50641;&#49828;&#53944;&#46972; &#53944;&#47532; . 기본적으로 100개의 결정 트리를 훈련 . 붓스트랩 샘플을 사용하지 않는다. . 각 결정트리를 만들 떄 전체 훈련 세트를 사용 . 노드 분할시 무작위로 분할 . 빠른 계산 속도 . et = ExtraTreesClassifier(n_jobs=-1, random_state=42) scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.9974503966084433 0.8887848893166506 . 랜덤포레스트와 비슷한 결과 . et.fit(train_input, train_target) print(et.feature_importances_) . [0.20183568 0.52242907 0.27573525] . 랜덤포레스트와 비슷한 결과 . &#44536;&#47112;&#51060;&#46356;&#50616;&#53944; &#48512;&#49828;&#54021; , gradient boosting . 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 . GradientBoostingClassifier은 기본적으로 깊이가 3인 결정 트리를 100개 사용 . 과대적합에 강하고 일반적으로 높은 일반화 성능 기대 . 경사 하강법을 사용하여 트리를 앙상블에 추가 . 분류에서는 로지스틱 손실 함수를 사용 하고 회귀에서는 평균 제곱 오차 함수 사용 . gb = GradientBoostingClassifier(random_state=42) scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.8881086892152563 0.8720430147331015 . 과대적합이 거의 되지 않음 . 트리의 개수를 늘려도 과대적합에 매우 강력 . gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42) scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.9464595437171814 0.8780082549788999 . gb.fit(train_input, train_target) print(gb.feature_importances_) . [0.15872278 0.68010884 0.16116839] . &#55176;&#49828;&#53664;&#44536;&#47016; &#44592;&#48152; &#44536;&#47112;&#51060;&#46356;&#50616;&#53944; &#48512;&#49828;&#54021; . 입력 특성을 256개의 구간으로 나눔. $ to$ 노드를 분할할 때 최적의 분할을 매우 바르게 찾을 수 있음 . 입력에 누락된 특성이 있어도 이를 따로 전처리할 필요가 없다. . 아직 테스트 과정이라 추가 모듈 임포트 필요 . hgb = HistGradientBoostingClassifier(random_state=42) scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.9321723946453317 0.8801241948619236 . hgb.fit(train_input, train_target) result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1) print(result.importances_mean) . [0.08876275 0.23438522 0.08027708] . result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1) print(result.importances_mean) . [0.05969231 0.20238462 0.049 ] . hgb.score(test_input, test_target) . 0.8723076923076923 . &#50508;&#44256;&#47532;&#51608;&#51012; &#44396;&#54788;&#54620; &#46608;&#45796;&#47480; &#46972;&#51060;&#48652;&#47084;&#47532;1 . xgb = XGBClassifier(tree_method=&#39;hist&#39;, random_state=42) scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.9555033709953124 0.8799326275264677 . &#50508;&#44256;&#47532;&#51608;&#51012; &#44396;&#54788;&#54620; &#46608;&#45796;&#47480; &#46972;&#51060;&#48652;&#47084;&#47532;2 . lgb = LGBMClassifier(random_state=42) scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1) print(np.mean(scores[&#39;train_score&#39;]), np.mean(scores[&#39;test_score&#39;])) . 0.935828414851749 0.8801251203079884 . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 앙상블 학습 : 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘을 말합니다. . - 랜덤 포레스트 : 대표적인 결정 트리 기반의 앙상블 학습 방법입니다. 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징입니다. . - 엑스트라 트리 : 랜덤 포레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만들지만 부트스트랩 샘플을 사용하지 않습니다. 대신 랜덤하게 노드를 분할해 과대적합을 감소시킵니다. . - 그레이디언트 부스팅 : 랜덤 포레스트나 엑스트라 트리와 달리 결정 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법입니다. 이런 이유로 훈련 속도가 조금 느리지만 더 좋은 성능을 기대할 수 있습니다. 그레이디언트 부스팅의 속도를 개선한 것이 히스토그램 기반 그레이디언트 부스팅이며 안정적인 결과와 높은 성능으로 매우 인기가 높습니다. . * 핵심 패키지 . * scikit-learn . 1 RandomForestClassifier :랜덤 포레스트 분류 클래스입니다.&gt; &gt;-n_esrimators 매개변수는 앙상블을 구성할 트리의 개수를 지정합니다. 기본값은 100입니다. . -criterion 매개변수는 불순도를 지정하며 기본값은 지니 불순도를 의미하는 &#39;gini&#39;이고 &#39;entropy&#39;를 선택하여 엔트로피 불순도를 사용할 수 있습니다. . -max_depth는 트리가 성장할 최대 깊이를 지정합니다. 기본값은 None으로 지정하면 리프노드가 순수하거나 min_samples_split보다 샘플 개수가 적을 때까지 성장합니다. . -min_samples_split은 노드를 나누기 위한 최소 샘플 개수입니다. 기본값은 2입니다. . -max_features 매개변수는 최적의 분할을 위해 탐색할 특성의 개수를 지정합니다. 기본값은 auto로 특성 개수의 제곱근입니다. . -bootstrap 매개변수는 부트스트랩 샘플을 사용할지 지정합니다. 기본값은 True입니다. . -ooh_score는 OOB 샘플을 사용하여 훈련한 모델을 평가할지 지정합니다. 기본값은 False입니다. . -n_jobs 매개변수는 병렬 실행에 사용할 CPU 코어 수를 지정합니다. 기본값은 1로 하나의 코어를 사용합니다. -1로 지정하면 시스템에 있는 모든 코어를 사용합니다. . 2 ExtraTreesClassifier :엑스트라 트리 분류 클래스입니다.&gt; &gt;-n_estimators, criterion, max_depth, min_samples_split, max_features 매개변수는 랜덤 포레스트와 동일합니다. . -bootstrap 매개변수는 부트스트랩 샘플을 사용할지 지정합니다. 기본값은 False입니다. . -oob_score는 OOB 샘플을 사용하여 훈련한 모델을 평가할지 지정합니다. 기본값은 False입니다. . -n_jobs 매개변수는 병렬 실행에 사용할 CPU 코어 수를 지정합니다. 기본값은 1로 하나의 코어를 사용합니다. -1로 지정하면 시스템에 있는 모든 코어를 사용합니다. . 3 GradientBoostingClassifier :그레이디언트 부스팅 분류 클래스입니다.&gt; &gt;-loss 매개변수는 손실 함수를 지정합니다. 기본값은 로지스틱 손실 함수를 의미하는 deviance&#39; 입니다. . -learning_rate 매개변수는 트리가 앙상블에 기여하는 정도를 조절합니다. 기본값은 0.1입니다. . -n_estinators 매개변수는 부스팅 단계를 수행하는 트리의 개수입니다. 기본값은 100입니다. . -subsample 매개변수는 사용할 훈련 세트의 샘플 비율을 지정합니다. 기본값은 1.0입니다. . -max_depth 매개변수는 개별 회귀 트리의 최대 깊이입니다. 기본값은 3입니다. . 4 HistGradientBoostingClassifier :히스토그램 기반 그레이디언트 부스팅 분류 클래스입니다. . -learning_rate 매개변수는 학습률 또는 감쇠율이라고 합니다. 기본값은 0.1이며 1.0이면 감쇠가 전혀 없습니다. . -max_iter는 부스팅 단계를 수행하는 트리의 개수입니다. 기본값은 100입니다. . -max_bins는 입력 데이터를 나눌 구간의 개수입니다. 기본값은 255이며 이보다 크게 지정할 수 없습니다. 여기에 1개의 구간이 누락된 값을 위해 추가됩니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/28/%ED%98%BC%EA%B3%B5%EB%A8%B85%EC%B1%953%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/28/%ED%98%BC%EA%B3%B5%EB%A8%B85%EC%B1%953%EC%9E%A5.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "혼공머 05-2",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 #___________________________________ from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 from sklearn.model_selection import cross_validate # 교차 검증 from sklearn.model_selection import StratifiedKFold # Kfold 교차 검증 from sklearn.model_selection import GridSearchCV # 그리드 서치 (하이퍼 파라미터 튜닝) from scipy.stats import uniform, randint #랜덤 서치 from sklearn.model_selection import RandomizedSearchCV # 랜덤 서치 클래스 . . &#44160;&#51613; &#49464;&#53944; . 테스트 세트를 사용하지 않고 이를 측정하기위해 훈련 세트를 또 나누는 것 . 보통 20~30% 를 테스트 세트와 검증 세트로 떼어 놓음 . wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) . data = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39;]].to_numpy() target = wine[&#39;class&#39;].to_numpy() . train_input , test_input , train_target , test_target = train_test_split(data, target , test_size=0.2 , random_state=42) . sub_input , val_input , sub_target , val_target = train_test_split(train_input, train_target , test_size=0.2 , random_state=42) . print(sub_input.shape , val_input.shape) . (4157, 3) (1040, 3) . dt = DecisionTreeClassifier(random_state=42) dt.fit(sub_input , sub_target) print(dt.score(sub_input ,sub_target)) print(dt.score(val_input ,val_target)) . 0.9971133028626413 0.864423076923077 . 훈련 세트에 과대적합 . &#44368;&#52264; &#44160;&#51613; (&#51076;&#54252;&#53944; &#54596;&#50836;) . 검증 세트가 불안정한 것을 방지 . 검증 세트를 떼어내어 평가하는 과정을 여러 번 반복 . 교차 검증을 할 때 훈련 세트를 섞으려면 분할기를 지정해야한다. . scores = cross_validate(dt , train_input , train_target) print(scores) . {&#39;fit_time&#39;: array([0.00535083, 0.00490546, 0.00474167, 0.00510621, 0.00448918]), &#39;score_time&#39;: array([0.00066662, 0.00050211, 0.00046086, 0.00050902, 0.0004313 ]), &#39;test_score&#39;: array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} . print(np.mean(scores[&#39;test_score&#39;])) . 0.855300214703487 . scores = cross_validate(dt, train_input , train_target , cv= StratifiedKFold()) print(np.mean(scores[&#39;test_score&#39;])) . 0.855300214703487 . splitter = StratifiedKFold(n_splits=10 , shuffle=True , random_state=42) scores = cross_validate(dt, train_input , train_target , cv= splitter) print(np.mean(scores[&#39;test_score&#39;])) . 0.8574181117533719 . &#54616;&#51060;&#54140;&#54028;&#46972;&#48120;&#53552;&#53916;&#45789; (&#54200;&#54616;&#44172; &#51076;&#54252;&#53944; &#51060;&#50857;) . 사용자가 지정해야만하는 파라미터 . 검증 세트의 점수나 교차 검증을 통해 매개변수를 조금씩 바꿔봄 . 그리드 서치 :훈련이 끝나면 25개의 모델 중에서 검증 점수가 가장 높은 모델의 매개변수 조합으로 전체 훈련 세트에서 자동으로 다시 모델을 훈련. . params = {&#39;min_impurity_decrease&#39; : [0.0001, 0.0002, 0.0003, 0.0004 , 0.0005] } . gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params , n_jobs=-1) . 결정 트리 클래스의 객체를 생성하자마자 바로 전달 . gs.fit(train_input ,train_target) . GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1, param_grid={&#39;min_impurity_decrease&#39;: [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}) . dt = gs.best_estimator_ print(dt.score(train_input ,train_target)) . 0.9615162593804117 . 최적의 매개변수는 bestparams 속성에 저장 . print(gs.best_params_) . {&#39;min_impurity_decrease&#39;: 0.0001} . print(gs.cv_results_[&#39;mean_test_score&#39;]) . [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605] . best_index = np.argmax(gs.cv_results_[&#39;mean_test_score&#39;]) print(gs.cv_results_[&#39;params&#39;][best_index]) . {&#39;min_impurity_decrease&#39;: 0.0001} . &#44284;&#51221; &#51221;&#47532; . 먼저 탐색할 매개변수 지정 | 그다음 훈련 세트에서 그리드 서치를 수행하여 최상의 평균 검증 점수가 나오는 매개변수 조합을 찾음.(이 조합은 그리드 서치 객체에 저장됨) | 그리드 서치는 최상의 매개변수에서 (교차 검증에 사용한 훈련 세트가 아니라) 전체 훈련 세트를 사용해 최종 모델을 훈련 (이 모델도 그리드 서치 객체에 저장됨) | params = {&#39;min_impurity_decrease&#39; : np.arange(0.0001 , 0.001, 0.0001), &#39;max_depth&#39;: range(5,20,1), &#39;min_samples_split&#39; : range(2,100,10)} . &#54632;&#49688; &#49444;&#47749; . np.arange(0.0001 , 0.001, 0.0001) : 첫번째 매개변수 값에서 시작하여 두번째 매개변수에 도달할 때까지 세번째 매개변수를 계속 더한 배열을 만듬 . range(x,y,z) : 정수만 사용 가능하며 .x에서 y까지 z씩 증가 하면서 값을 생성 . gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) gs.fit(train_input , train_target) . GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1, param_grid={&#39;max_depth&#39;: range(5, 20), &#39;min_impurity_decrease&#39;: array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009]), &#39;min_samples_split&#39;: range(2, 100, 10)}) . print(gs.best_params_) . {&#39;max_depth&#39;: 14, &#39;min_impurity_decrease&#39;: 0.0004, &#39;min_samples_split&#39;: 12} . print(np.max(gs.cv_results_[&#39;mean_test_score&#39;])) . 0.8683865773302731 . &#47004;&#45924; &#49436;&#52824; (&#51076;&#54252;&#53944;) . 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달 . rgen = randint(0 ,10) rgen.rvs(10) . array([3, 7, 6, 4, 5, 1, 3, 5, 5, 3]) . np.unique(rgen.rvs(1000), return_counts=True) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([109, 86, 96, 96, 104, 115, 97, 103, 101, 93])) . ugen = uniform(0,1) ugen.rvs(10) . array([0.46214935, 0.47391698, 0.08113901, 0.0342477 , 0.411983 , 0.79979556, 0.32456457, 0.33591199, 0.89279569, 0.63834879]) . params = {&#39;min_impurity_decrease&#39; : uniform(0.0001 , 0.001), &#39;max_depth&#39;: randint(20,50), &#39;min_samples_split&#39; : randint(2,25), &#39;min_samples_leaf&#39; : randint(1,25)} . gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100 , n_jobs=-1 , random_state=42) gs.fit(train_input , train_target) . RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_iter=100, n_jobs=-1, param_distributions={&#39;max_depth&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa8507b820&gt;, &#39;min_impurity_decrease&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa85e15fd0&gt;, &#39;min_samples_leaf&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa85084100&gt;, &#39;min_samples_split&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7ffa85078fa0&gt;}, random_state=42) . print(gs.best_params_) . {&#39;max_depth&#39;: 39, &#39;min_impurity_decrease&#39;: 0.00034102546602601173, &#39;min_samples_leaf&#39;: 7, &#39;min_samples_split&#39;: 13} . print(np.max(gs.cv_results_[&#39;mean_test_score&#39;])) . 0.8695428296438884 . dt = gs.best_estimator_ print(dt.score(test_input ,test_target)) . 0.86 . 수동으로 하는거보다 그리드 서치나 랜덤 서치가 편하다. . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 검증 세트 : 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어 낸 데이터 세트입니다. . - 교차 검증 : 훈련 세트를 여러 폴드로 나눈 다음 한 폴드가 검증 세트의 역할을 하고 나머지 폴드에서는 모델을 훈련합니다. 교차 검증은 이런 식으로 모든 폴드에 대해 검증 점수를 얻어 평균하는 방법입니다. . - 그리드 서치 : 하이퍼파라미터 탐색을 자동화해 주는 도구입니다. 탐색할 매개변수를 나열하면 교차 검증을 수행하여 가장 좋은 검증 점수의 매개변수 조합을 선택합니다. 마지막으로 이매개변수 조합으로 최종 모델을 훈련합니다. . - 랜덤 서치 : 연속된 매개변수 값을 탐색할 때 유용합니다. 탐색할 값을 직접 나열하는 것이 아니고 탐색 값을 샘플링할 수 있는 확률 분포 객체를 전달합니다. 지정된 횟수만큼 샘플링하여교차 검증을 수행하기 때문에 시스템 자원이 허락하는 만큼 탐색량을 조절할 수 있습니다. . - 핵심 패키지 . - scikit-learn . 1 cross_validate() :교차 검증을 수행하는 함수입니다.&gt; &gt;첫 번째 매개변수에 교차 검증을 수행할 모델 객체를 전달합니다. 두 번째와 세 번째 매개변수에 특성과 타깃 데이터를 전달합니다. . scoring 매개변수에 검증에 사용할 평가 지표를 지정할 수 있습니다. 기본적으로 분류 모델은정확도를 의미하는 accuracy&#39;, 회귀 모델은 결정계수를 의미하는 &#39;r2&#39;가 됩니다. . CV 매개변수에 교차 검증 폴드 수나 스플리터 객체를 지정할 수 있습니다. 기본값은 5입니다. 회귀일 때는 KFold 클래스를 사용하고 분류일 때는 StratifiedKFold 클래스를 사용하여 5-폴드 교차 검증을 수행합니다. . n_jobs 매개변수는 교차 검증을 수행할 때 사용할 CPU 코어 수를 지정합니다. 기본값은 1로 하나의 코어를 사용합니다. -1로 지정하면 시스템에 있는 모든 코어를 사용합니다. . return_train score 매개변수를 True로 지정하면 훈련 세트의 점수도 반환합니다. 기본값은 False입니다. . 2 GridSearchCV :교차 검증으로 하이퍼파라미터 탐색을 수행합니다. 최상의 모델을 찾은 후 훈련 세트 전체를 사용해 최종 모델을 훈련합니다.&gt; &gt;첫 번째 매개변수로 그리드 서치를 수행할 모델 객체를 전달합니다. 두 번째 매개변수에는 탐색할 모델의 매개변수와 값을 전달합니다. . Scoring, c, n jobs, return_train score 매개변수는 cross validate() 함수와 동일합니다. . 3 RandomizedSearchCV :교차 검증으로 랜덤한 하이퍼파라미터 탐색을 수행합니다. 최상의 모델을 찾은 후 훈련 세트 전체를 사용해 최종 모델을 훈련합니다. . 첫 번째 매개변수로 그리드 서치를 수행할 모델 객체를 전달합니다.두 번째 매개변수에는 탐색할 모델의 매개변수와 확률 분포 객체를 달합니다. . Scoring, cv, n_jobs, return train score 매개변수는 cross_validate() 함수와 동일합니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/27/%ED%98%BC%EA%B3%B5%EB%A8%B85%EC%B1%952%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/27/%ED%98%BC%EA%B3%B5%EB%A8%B85%EC%B1%952%EC%9E%A5.html",
            "date": " • Feb 27, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "혼공머 05-1",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 from sklearn.tree import DecisionTreeClassifier # 트리 from sklearn.tree import plot_tree # 트리 모형 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47196; &#50752;&#51064; &#48516;&#47448;&#54616;&#44592; . wine = pd.read_csv(&#39;https://bit.ly/wine_csv_data&#39;) . wine.head() . alcohol sugar pH class . 0 9.4 | 1.9 | 3.51 | 0.0 | . 1 9.8 | 2.6 | 3.20 | 0.0 | . 2 9.8 | 2.3 | 3.26 | 0.0 | . 3 9.8 | 1.9 | 3.16 | 0.0 | . 4 9.4 | 1.9 | 3.51 | 0.0 | . class 열 에서 0 :레드 와인 (음성 클래스) , 1 : 화이트 와인 (양성 클래스) . wine.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB . 총 6497개의 샘플이 있고 , 4개의 열은 모두 실수값 누락된 값은 없음 . wine.describe() . alcohol sugar pH class . count 6497.000000 | 6497.000000 | 6497.000000 | 6497.000000 | . mean 10.491801 | 5.443235 | 3.218501 | 0.753886 | . std 1.192712 | 4.757804 | 0.160787 | 0.430779 | . min 8.000000 | 0.600000 | 2.720000 | 0.000000 | . 25% 9.500000 | 1.800000 | 3.110000 | 1.000000 | . 50% 10.300000 | 3.000000 | 3.210000 | 1.000000 | . 75% 11.300000 | 8.100000 | 3.320000 | 1.000000 | . max 14.900000 | 65.800000 | 4.010000 | 1.000000 | . 평균 표준편차 최소 1분위 중간값 3분위 최대 . 알코올 도수와 당도, pH 값의 스케일이 다르다. . 표준화 필요 . data = wine[[&#39;alcohol&#39;, &#39;sugar&#39;, &#39;pH&#39; ]].to_numpy() target = wine[&#39;class&#39;].to_numpy() . train_input , test_input , train_target , test_target = train_test_split(data, target , test_size=0.2 , random_state=42) . 샘플 개수가 충분히 많아서 20% 정도만 테스트 세트로 나눔 . print(train_input.shape, test_input.shape) . (5197, 3) (1300, 3) . ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . lr = LogisticRegression() lr.fit(train_scaled , train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled , test_target)) . 0.7808350971714451 0.7776923076923077 . 낮은 점수 , 모델이 다소 과소 적합 $ to$ 규제 매개 변수 C의 값을 변경 or solver 매개변수에서 다른 알고리즘을 선택 . &#44208;&#51221; &#53944;&#47532; . 설명하기 쉬운 모델 , 스무고개 같음 . 표준화 전처리과정이 필요없다. . dt = DecisionTreeClassifier(random_state=42) dt.fit(train_scaled , train_target) print(dt.score(train_scaled , train_target)) print(dt.score(test_scaled , test_target)) . 0.996921300750433 0.8592307692307692 . 훈련 세트에 대한 점수가 엄청 높다. (과대적합) . plt.figure(figsize=(10,7)) #plot_tree(dt) #plt.show() . &lt;Figure size 720x504 with 0 Axes&gt; . &lt;Figure size 720x504 with 0 Axes&gt; . 너무 복잡해 렉이걸려 제한을 걸어줌 . plt.figure(figsize=(10,7)) plot_tree(dt, max_depth=1, filled=True, feature_names=[&#39;alcohol&#39;, &#39;sugar&#39;,&#39;pH&#39;]) plt.show() . &#51069;&#45716;&#48277; . 위에서 부터 테스트 조건 / 불순도 / 총 샘플 수 / 클래스별 샘플 수 . filled=True 를 이용하여 더 샘플이 많은 쪽이 진한 파란색으로 표시되었다. . &#48520;&#49692;&#46020; . gini는 지니 불순도를 의미 DecisionTreeClassifier 의 critreion 매개변수 의 기본값이 gini . $ 지니 불순도 = 1 - (음성 클래스 비율^2 + 양성 클래스 비율^2) $ . 부모노드와 자식노드의 불순도 차이(정보 이득)가 가능한 크도록 트리를 성장 . 엔트로피 불순도 :제곱이 아니라 밑이 2인 로그를 사용하여 곱 . &#44032;&#51648;&#52824;&#44592; . dt = DecisionTreeClassifier(max_depth=3, random_state=42) dt.fit(train_scaled , train_target) print(dt.score(train_scaled , train_target)) print(dt.score(test_scaled , test_target)) . 0.8454877814123533 0.8415384615384616 . 훈련세트의 성능은 낮아졌지만 테스트 세트의 성능은 거의 그대로 . plt.figure(figsize=(20,15)) plot_tree(dt , filled=True , feature_names=[&#39;alcohol&#39;, &#39;sugar&#39; , &#39;pH&#39;]) plt.show() . 깊이 3 . dt = DecisionTreeClassifier(max_depth=3 , random_state=42) dt.fit(train_input , train_target) print(dt.score(train_input , train_target)) print(dt.score(test_input , test_target)) . 0.8454877814123533 0.8415384615384616 . plt.figure(figsize=(20,15)) plot_tree(dt , filled=True , feature_names=[&#39;alcohol&#39;, &#39;sugar&#39; , &#39;pH&#39;]) plt.show() . &#44208;&#51221; &#51473;&#50836;&#46020; . 결정 트리 모델의 featureimportances 속성에 저장 . print(dt.feature_importances_) . [0.12345626 0.86862934 0.0079144 ] . sugar가 제일 중요한 결정 요소 . 다 더하면 1 . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 결정 트리 : 예 / 아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘입니다.비교적 예측 과정을 이해하기 쉽고 성능도 뛰어납니다. 결정 트리는 제한 없이 성장하면 훈련 세트에 과대적합되기 쉽습니다. . - 불순도 : 결정 트리가 최적의 질문을 찾기 위한 기준입니다. 사이킷런은 지니 불순도와 엔트로피 불순도를 제공합니다. . - 정보 이득 : 부모 노드와 자식 노드의 불순도 차이입니다. 결정 트리 알고리즘은 정보 이득이최대화되도록 학습합니다. . - 가지치기 : 결정 트리의성장을 제한하는 방법입니다. 사이킷런의 결정 트리 알고리즘은 여러 가지 가치지기 매개변수를 제공합니다. . - 특성 중요도 : 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값입니다. 특성 중요도를 계산할 수 있는 것이 결정 트리의 또다른 큰 장점입니다. . - 핵심 패키지 . - pandas . 1 info() :데이터프레임의 요약된 정보를 출력합니다.&gt; &gt;인덱스와 컬럼 타입을 출력하고 닐(null)이 아닌 값의 개수, 메모리 사용량을 제공합니다. . verbose 매개변수의 기본값 True를False로 바꾸면 각 열에 대한 정보를 출력하지 않습니다. . 2 describe() :데이터프레임 열의 통계 값을 제공합니다. 수치형일 경우 최소, 최대, 평균, 표준편차와 사분위값 등이 출력됩니다.&gt; &gt;문자열 같은 객체 타입의 열은 가장 자주 등장하는 값과 횟수 등이 출력됩니다. . percentiles 매개변수에서 백분위수를 지정합니다. 기본값은 [0.25, 0.5, 0.75]입니다. . - scikit-learn . 1 DecisionTreeClassifier :결정 트리 분류 클래스입니다.&gt; &gt;Criterion 매개변수는 불순도를 지정하며 기본값은 지니 불순도를 의미하는 &#39;gini&#39;이고 entropy&#39;를 선택하여 엔트로피 불순도를 사용할 수 있습니다. . Splitter 매개변수는 노드를 분할하는 전략을 선택합니다. 기본값은 &#39;Dest&#39;로 정보 이득이 최대가 되도록 분할합니다. random&#39;이면 임의로 노드를 분할합니다. . imax depth는 트리가 성장할 최대 깊이를 지정합니다. 기본값은 None으로 리프 노드가 순수하거나 min samples_split보다 샘플 개수가 적을 때까지 성장합니다. . min_samples_split은 노드를 나누기 위한 최소 샘플 개수입니다. 기본값은 2입니다. . max_features 매개변수는 최적의 분할을 위해 탐색할 특성의 개수를 지정합니다. 기본값은 None으로 모든 특성을 사용합니다. . 2 plot_tree() :결정 트리 모델을 시각화합니다. 첫 번째 매개변수로 결정 트리 모델 객체를 전달합니다. . max_depth 매개변수로 나타낼 트리의 깊이를 지정합니다. 기본값은 None으로 모든 노드를 출력합니다. . feature_names 매개변수로 특성의 이름을 지정할 수 있습니다. . filled 매개변수를 True로 지정하면 타깃값에 따라 노드 안에 색을 채웁니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/26/%ED%98%BC%EA%B3%B5%EB%A8%B85%EC%B1%951%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/26/%ED%98%BC%EA%B3%B5%EB%A8%B85%EC%B1%951%EC%9E%A5.html",
            "date": " • Feb 26, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "혼공머 04-2",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 from sklearn.linear_model import SGDClassifier # 확률적 경사 하강법 . &#51216;&#51652;&#51201;&#51064; &#54617;&#49845; . 기존 훈련 데이터에 새로운 데이터를 추가하여 모델을 매일매일 다시 훈련? . 서버 문제가 큼 $ to$ 새로운 데이터를 추가할 때 이전 데이터를 버려서 훈련 데이터 크기를 일정하게 유지(점진적 학습) . 대표적인 점진적 학습 알고리즘이 확률적 경사 하강법 . &#54869;&#47456;&#51201; &#44221;&#49324; &#54616;&#44053;&#48277; . 한개씩 이용 :확률적 경사 하강법&gt; 가장 가파른 길을 따라 원하는 지점에 도달하는 것이 목표 (조금씩 빠르게) . 랜덤하게 (확률적으로) . 훈련 세트를 한 번 모두 사용하는 과정 :에포크&gt; 여러개의 샘플을 사용해 경사 하강법 수행 :미니배치 경사 하강법&gt; 극단적으로 한 번 경사로를 따라 이동하기 위해 전체 샘플 이용 :배치경사하강법 . &#49552;&#49892;&#54632;&#49688; . 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준 . 최솟값은 모른다. . 가능한 많이 찾아봄 . 샘플 하나에 대한 손실을 정의 (비용함수 :훈련세트에 있는 모든 샘플에 대한 손실 함수의 합) &gt; 주로 사용하는 손실 함수 :MSE(평균 제곱 오차 , 작을수록 좋음) . &#47196;&#51648;&#49828;&#54001; &#49552;&#49892; &#54632;&#49688; (&#51060;&#51652; &#53356;&#47196;&#49828;&#50644;&#53944;&#47196;&#54588; &#49552;&#49892; &#54632;&#49688;) . 양성 클래스(타깃=1) 일 때 손실은 -log(예측 확률)로 계산. . 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 됨. . 음성 클래스 (타깃=0) 일 때 손실은 -log(1-예측 확률)로 계산. . 이 예측 확률이 0에서 멀어질수록 손실은 아주 큰 양수가 됨. . &#53356;&#47196;&#49828;&#50644;&#53944;&#47196;&#54588; &#49552;&#49892; &#54632;&#49688; :&#45796;&#51473; &#48516;&#47448;&#50640;&#49436; &#49324;&#50857;&#54616;&#45716; &#49552;&#49892; &#54632;&#49688; . SGDClassifier . fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) . fish_input = fish[[&#39;Weight&#39;, &#39;Length&#39;, &#39;Diagonal&#39; , &#39;Height&#39;, &#39;Width&#39;]].to_numpy() fish_target = fish[&#39;Species&#39;].to_numpy() . train_input , test_input , train_target , test_target = train_test_split(fish_input , fish_target , random_state=42) . ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . # loss는 손실 함수의 종류를 지정 # max_iter : 수행할 에포크 횟수 지정 sc = SGDClassifier(loss=&#39;log&#39; , max_iter=10 , random_state=42) sc.fit(train_scaled , train_target) print(sc.score(train_scaled ,train_target)) print(sc.score(test_scaled , test_target)) . 0.773109243697479 0.775 . /home/cjfal/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. . 지정한 반복 횟수 10번이 부족한 것으로 보임 (매개변수의 값을 늘려야함 오류가 아닌 &#39;경고&#39;) . sc.partial_fit(train_scaled , train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled , test_target)) . 0.8151260504201681 0.85 . 점수가 낮지만 정확도가 향상 . &#50640;&#54252;&#53356;&#50752; &#44284;&#45824;/&#44284;&#49548;&#51201;&#54633; . 확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다. . 에포크는 횟수가 적으면 모델이 훈련 세트를 덜 학습 . sc = SGDClassifier(loss = &#39;log&#39; , random_state=42) train_score = [] test_score = [] classes = np.unique(train_target) for _ in range(0 ,300): sc.partial_fit(train_scaled, train_target , classes=classes) train_score.append(sc.score(train_scaled , train_target)) test_score.append(sc.score(test_scaled , test_target)) . _ 는 특별한 변수, 나중에 사용하지 않고 그냥 버리는 값을 넣어두는 용도로 사용 . plt.plot(train_score) plt.plot(test_score) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.show() . 주황 :테스트 , 파랑 : 훈련 . 100 전에는 과소, 100이후에는 거리가 더 벌어짐 $ to$ 100째 에포크가 적절해보임 . sc = SGDClassifier(loss = &#39;log&#39; , max_iter=100 , tol=None ,random_state=42) sc.fit(train_scaled , train_target) print(sc.score(train_scaled , train_target)) print(sc.score(test_scaled , test_target)) . 0.957983193277311 0.925 . 정확도 상승! . loss &#47588;&#44060;&#48320;&#49688; . loss 매개변수의 기본값 :hinge&gt; 힌지 손실 :서포트 벡터 머신 이라 불리는 또다른 머신러닝 알고리즘을 위한 손실 함수 . sc = SGDClassifier(loss = &#39;hinge&#39; , max_iter=100 , tol=None ,random_state=42) sc.fit(train_scaled , train_target) print(sc.score(train_scaled , train_target)) print(sc.score(test_scaled , test_target)) . 0.9495798319327731 0.925 . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 확률적 경사 하강법 : 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을찾는 알고리즘입니다. 샘플을 하나씩 사용하지 않고 여러 개를 사용하면 미니배치 경사 하강법이 됩니다. 한 번에 전체 샘플을 사용하면 배치 경사 하강법이 됩니다. . - 손실 함수 : 확률적 경사 하강법이 최적화할 대상입니다. 대부분의 문제에 잘 맞는 손실 함수가 이미 정의되어 있습니다. 이진 분류에는 로지스틱 회귀(또는 이진 크로스엔트로피) 손실함수를 사용합니다. 다중 분류에는 크로스엔트로피 손실 함수를 사용합니다. 회귀 문제에는 평균 제곱 오차 손실 함수를 사용합니다. . - 에포크 : 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미합니다. 일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 반복합니다. . - 라쏘 : 또 다른 규제가 있는 선형 회귀 모델입니다. 릿지와 달리 계수 값을 아예 0으로 만들 수도 있습니다. . - 하이퍼파라미터 : 머신러닝 알고리즘이 학습하지 않는 파라미터입니다. 이런 파라미터는 사람이 사전에 지정해야 합니다. 대표적으로 릿지와 라쏘의 규제 강도 alpha 파라미터입니다. . - 핵심 패키지 . - scikit-learn . 1 SGDClassifier :확률적 경사 하강법을 사용한 분류 모델을 만듭니다. max_iter 매개변수는 에포크 횟수를 지정합니다. 기본값은 1000입니다. loss 매개변수는 확률적 경사 하강법으로 최적화할 손실 함수를 지정합니다. 기본값은 서포트벡터 머신을 위한 ge&#39; 손실 함수입니다. 로지스틱 회귀를 위해서는 &#39;log&#39;로 지정합니다. penalty 매개변수에서 규제의 종류를 지정할 수 있습니다. 기본값은 L2 규제를 위한 12&#39; 입니다. L1 규제를 적용하려면 11&#39;로 지정합니다. 규제 강도는 alpha 매개변수에서 지정합니다.기본값은 0,0001입니다. tol 매개변수는 반복을 멈출 조건입니다. tol 매개변수의 기본값은 0.001입니다. n_iter_no_change 매개변수에서 지정한 에포크 동안 손실이 tol 만큼 줄어들지 않으면 알고리즘이 중단됩니다. n_iter_no_change 매개변수의 기본값은 5입니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/25/%ED%98%BC%EA%B3%B5%EB%A8%B84%EC%B1%952%EC%9E%A5.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/25/%ED%98%BC%EA%B3%B5%EB%A8%B84%EC%B1%952%EC%9E%A5.html",
            "date": " • Feb 25, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "혼공머 04-1",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 from sklearn.linear_model import LogisticRegression # 로지스틱회귀 from scipy.special import expit #시그모이드함수 from scipy.special import softmax #소프트맥스함수 . &#47085;&#53412;&#48177; &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . 판다스를 이용해여 df로 저장하기 . https://bit.ly/fish_csv_data . fish = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fish.head . &lt;bound method NDFrame.head of Species Weight Length Diagonal Height Width 0 Bream 242.0 25.4 30.0 11.5200 4.0200 1 Bream 290.0 26.3 31.2 12.4800 4.3056 2 Bream 340.0 26.5 31.1 12.3778 4.6961 3 Bream 363.0 29.0 33.5 12.7300 4.4555 4 Bream 430.0 29.0 34.0 12.4440 5.1340 .. ... ... ... ... ... ... 154 Smelt 12.2 12.2 13.4 2.0904 1.3936 155 Smelt 13.4 12.4 13.5 2.4300 1.2690 156 Smelt 12.2 13.0 13.8 2.2770 1.2558 157 Smelt 19.7 14.3 15.2 2.8728 2.0672 158 Smelt 19.9 15.0 16.2 2.9322 1.8792 [159 rows x 6 columns]&gt; . Species &#50676;&#50640;&#49436; &#44256;&#50976;&#44050; &#52628;&#52636; . pd.unique() . print(pd.unique(fish[&#39;Species&#39;])) . [&#39;Bream&#39; &#39;Roach&#39; &#39;Whitefish&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Smelt&#39;] . Species &#50676;&#51012; &#53440;&#44611;&#51004;&#47196; &#47564;&#46308;&#44256; &#45208;&#47672;&#51648; 5&#44060; &#50676;&#51008; &#51077;&#47141;&#45936;&#51060;&#53552;&#47196; &#49324;&#50857; . fish_input = fish[[&#39;Weight&#39; , &#39;Length&#39;, &#39;Diagonal&#39; , &#39;Height&#39; , &#39;Width&#39;]].to_numpy() . print(fish_input[:5]) . [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961] [363. 29. 33.5 12.73 4.4555] [430. 29. 34. 12.444 5.134 ]] . 위 데이터 프레임의 헤드값과 같다. . &#53440;&#44611;&#45936;&#51060;&#53552; &#49373;&#49457; . fish_target = fish[&#39;Species&#39;].to_numpy() . &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944;&#49464;&#53944; &#49373;&#49457; . train_input , test_input , train_target , test_target = train_test_split(fish_input , fish_target , random_state=42) . &#54364;&#51456;&#54868; &#52376;&#47532; . ss = StandardScaler() ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . . k-&#52572;&#44540;&#51217; &#51060;&#50883; &#48516;&#47448;&#44592;&#51032; &#54869;&#47456; &#50696;&#52769; ( &#45796;&#51473; &#48516;&#47448; ) . kn = KNeighborsClassifier(n_neighbors=3) kn.fit(train_scaled , train_target) print(kn.score(train_scaled , train_target)) print(kn.score(test_scaled , test_target)) . 0.8907563025210085 0.85 . 점수가 낮지만 무시 (클래스 확률 중점) . 7&#51333;&#47448;&#51032; &#49373;&#49440;&#51032; &#51032;&#48120; . 타깃데이터에 7종류의 생선이 포함 $ to$ 2개이상의 클래스가 포함된 문제 :다중 분류 . print(kn.classes_) . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . print(kn.predict(test_scaled[:5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] . proba = kn.predict_proba(test_scaled[:5]) print(np.round(proba, decimals=4)) . [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] . distances, indexes = kn.kneighbors(test_scaled[3:4]) #4번째 데이터만 확인 print(train_target[indexes]) . [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . Perch(3번째)가 나올 확률 2/3 , Roach(5번째)가 나올 확률 1/3 $ to$ 맞음 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 이름은 회귀이지만 분류 모델 . 선형회귀와 동일하게 선형 방정식을 학습 . 0~1로 확률을 표시 가능 . &#49884;&#44536;&#47784;&#51060;&#46300; &#54632;&#49688; : $ phi = frac{1}{1+e^{-z}}$ = &#47196;&#51648;&#49828;&#54001;&#54632;&#49688; . z = np.arange(-5 , 5 , 0.1) phi = 1 / (1 + np.exp(-z)) plt.plot(z , phi) plt.xlabel(&#39;z&#39;) plt.ylabel(&#39;phi&#39;) plt.show() . 0~1 의 값 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47196; &#51060;&#51652; &#48516;&#47448; &#54644;&#48372;&#44592; . 0.5 이상이면 양성 클래스 이하면 음성 클래스 . &#48520;&#47532;&#50616; &#51064;&#45937;&#49905; &#50696;&#51228; . char_arr = np.array([&#39;A&#39;, &#39;B&#39; , &#39;C&#39; , &#39;D&#39; , &#39;E&#39;]) print(char_arr[[True, False, True, False, False]]) . [&#39;A&#39; &#39;C&#39;] . &#49373;&#49440;&#50640; &#51201;&#50857; (&#48729;&#50612;&#47564; &#44264;&#46972;&#45236;&#44592;) . bream_smelt_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) train_bream_smelt = train_scaled[bream_smelt_indexes] target_bream_smelt = train_target[bream_smelt_indexes] . lr = LogisticRegression() lr.fit(train_bream_smelt , target_bream_smelt) . LogisticRegression() . print(lr.predict(train_bream_smelt[:5])) . [&#39;Bream&#39; &#39;Smelt&#39; &#39;Bream&#39; &#39;Bream&#39; &#39;Bream&#39;] . 두번째 샘플 제외 모두 도미(Bream)로 예측 . print(lr.predict_proba(train_bream_smelt[:5])) . [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] . print(lr.classes_) . [&#39;Bream&#39; &#39;Smelt&#39;] . 사이킷런에서는 Bream이 양성 클래스 . print(lr.coef_, lr.intercept_) . [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132] . $ z = -0.404 times (Weight) - 0.576 times (Length) - 0.663 times (Diagonal) - 1.013 times (Height) - 0.732 times (Width) -2.161 $ . z&#44050; &#52636;&#47141; . decisions = lr.decision_function(train_bream_smelt[:5]) print(decisions) . [-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] . 이 z값을 시그모이드 함수에 통과시키면 확률을 얻을 수 있다. expit() :시그모이드 함수 임포트 필요 . print(expit(decisions)) . [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47196; &#45796;&#51473; &#48516;&#47448; &#49688;&#54665; . 반복적인 알고리즘 사용, max_iter 매개변수에서 반복 횟수를 지정하고 기본값은 100 . 계수의 제곱을 규제 (L2규제) , 매개변수 C 로 규제 제어 ( 작을수록 규제가 커짐 , 기본값 1) . lr = LogisticRegression(C=20 , max_iter=1000) lr.fit(train_scaled , train_target) print(lr.score(train_scaled , train_target)) print(lr.score(test_scaled , test_target)) . 0.9327731092436975 0.925 . 적당한거같다. . print(lr.predict(test_scaled[:5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;] . proba = lr.predict_proba(test_scaled[:5]) print(np.round(proba , decimals=3)) . [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . 5개 샘플에 대한 예측이라 5개 행 출력 . print(lr.classes_) . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . 첫번째 샘플은 Perch(농어) 를 가장 높은 확률로 예측 . print(lr.coef_.shape , lr.intercept_.shape) . (7, 5) (7,) . 다중 분류는 클래스마다 z 값을 하나씩 계산 . 소프트맥스 함수를 사용하여 7개의 z 값을 확률로 변환 . &#49548;&#54532;&#53944; &#47589;&#49828; &#54632;&#49688; (&#51076;&#54252;&#53944; &#54596;&#50836;) . $e_{sum}$ = $ e^{z1} + e^{z2} + e^{z3}+ e^{z4}+ e^{z5}+ e^{z6}+ e^{z7} $ . $ s1 = frac{e^{z1}}{e_{sum}}$ , $ s2 = frac{e^{z2}}{e_{sum}}$ , $ dots$ , $ s7 = frac{e^{z7}}{e_{sum}}$ . decision = lr.decision_function(test_scaled[:5]) print(np.round(decision, decimals=2)) . [[ -6.5 1.03 5.16 -2.73 3.34 0.33 -0.63] [-10.86 1.93 4.77 -2.4 2.98 7.84 -4.26] [ -4.34 -6.23 3.17 6.49 2.36 2.42 -3.87] [ -0.68 0.45 2.65 -1.19 3.26 -5.75 1.26] [ -6.4 -1.99 5.82 -0.11 3.5 -0.11 -0.71]] . proba = softmax(decision , axis=1) print(np.round(proba, decimals=3)) . [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . 로지스틱회귀랑 값이 같다. . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 로지스틱 회귀 : 선형 방정식을 사용한 분류 알고리즘입니다. 선형 회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여 클래스 확률을 출력할 수 있습니다. . - 다중 분류 : 타깃 클래스가 2개 이상인 분류 문제입니다. 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용하여 클래스를 예측합니다. . - 시그모이드 함수 : 선형 방정식의 출력을 0과 1 사이의 값으로 압축하며 이진 분류를 위해 사용합니다. . - 소프트맥스 함수 : 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만듭니다. . - 핵심 패키지 . - scikit-learn . 1 LogisticRegression :선형 분류 알고리즘인 로지스틱 회귀를 위한 클래스입니다. solver 매개변수에서 사용할 알고리즘을 선택할 수 있습니다. 기본값은 &#39;1bfgs&#39;입니다. 사이킷런 0.17 버전에 추가된 &#39;sag&#39;는 확률적 평균 경사 하강법 알고리즘으로 특성과 샘플 수가 많을 때 성능은 빠르고 좋습니다. 사이킷런 0.19 버전에는 &#39;sag&#39;의 개선 버전인 &#39;saga&#39;가 추가되었습니다. penalty 매개변수에서 L2 규제 (릿지 방식)와 L1 규제 (라쏘 방식)를 선택할 수 있습니다. 기본값은 L2 규제를 의미하는 12 입니다. C 매개변수에서 규제의 강도를 제어합니다. 기본값은 1.0이며 값이 작을수록 규제가 강해집니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/24/%ED%98%BC%EA%B3%B5%EB%A8%B84%EC%B1%951%EC%9E%A5%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/24/%ED%98%BC%EA%B3%B5%EB%A8%B84%EC%B1%951%EC%9E%A5%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "혼공머 03-3",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 from sklearn.preprocessing import PolynomialFeatures #다중회귀로의 변환기 from sklearn.preprocessing import StandardScaler #규제 from sklearn.linear_model import Ridge #릿지 from sklearn.linear_model import Lasso #라쏘 . &#45796;&#51473;&#54924;&#44480; . 여러 개의 특성을 사용한 선형 회귀 . 3차원 이상의 공간을 그리거나 상상할 수는 없음 $ to$ 특성공학 feature engineering 으로 기존의 특성을 사용해 새로운 특성을 뽑아내어 회귀 . &#45936;&#51060;&#53552; &#51456;&#48708; . https://bit.ly/perch_csv_data : 홈페이지에서 바로 csv 데이터를 사용한다.&gt; https://bit.ly/perch_data : 타깃 데이터 . &#54032;&#45796;&#49828; pandas &#51060;&#50857; . 데이터프레임으로 다차원 배열 다루기 . csv데이터를 불러오고 데이터프레임으로 저장한 뒤 넘파이 배열로 다시 바꾼다. . df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) #넘파이 배열로 바꾸기 perch_full = df.to_numpy() . perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . train_input , test_input , train_target, test_target = train_test_split(perch_full , perch_weight , random_state=42) . &#49324;&#51060;&#53431;&#47088;&#51032; &#48320;&#54872;&#44592; . 새로운 특성을 만든다. PolynomialFeatures 클래스 이용 . &#54632;&#49688; &#45796;&#47364;&#48372;&#44592; . poly = PolynomialFeatures() poly.fit([[2,3]]) print(poly.transform([[2,3]])) . [[1. 2. 3. 4. 6. 9.]] . 샘플 2,3 이 6개의 특성을 가진 샘플 1,2,3,4,5,9 로 바뀜 . 기존 2,3 에 각각을 제곱한 4,9 , 두 수를 곱한 6 , 선형 방정식의 절편에 곱해지는 1 (약속) . 이때 1은 굳이 필요없으므로 제거 $ to$ include_bias = False . poly = PolynomialFeatures(include_bias = False) poly.fit([[2,3]]) print(poly.transform([[2,3]])) . [[2. 3. 4. 6. 9.]] . &#49892;&#51228; &#45936;&#51060;&#53552;&#50640; &#51201;&#50857; . poly = PolynomialFeatures(include_bias = False) poly.fit(train_input) train_poly = poly.transform(train_input) print(train_poly.shape) # 배열의 크기 . (42, 9) . poly.get_feature_names() . [&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x0^2&#39;, &#39;x0 x1&#39;, &#39;x0 x2&#39;, &#39;x1^2&#39;, &#39;x1 x2&#39;, &#39;x2^2&#39;] . test_poly = poly.transform(test_input) . &#45796;&#51473; &#54924;&#44480; &#47784;&#45944; &#54984;&#47144;&#54616;&#44592; . lr = LinearRegression() lr.fit(train_poly, train_target) print(lr.score(train_poly ,train_target)) . 0.9903183436982125 . print(lr.score(test_poly , test_target)) . 0.9714559911594155 . &#53945;&#49457;&#51012; &#45908; &#52628;&#44032;&#54644;&#48372;&#44592; . 3제곱 4제곱항 등 더 추가 (degree로) . poly = PolynomialFeatures(degree = 5 , include_bias=False) poly.fit(train_input) train_poly = poly.transform(train_input) test_poly = poly.transform(test_input) print(train_poly.shape) . (42, 55) . lr.fit(train_poly , train_target) print(lr.score(train_poly, train_target)) . 0.9999999999938143 . print(lr.score(test_poly , test_target)) . -144.40744533753661 . 훈련세트에 너무 과대적합 되어서 테스트 세트에서는 형편없는 점수 . &#44508;&#51228; . 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하게 막아준다. (과대적합 막기) . 선형회귀모델에 규제를 추가한 모델 :ridge , lasso&gt; ridge :계수를 제곱한 값을 기준으로 규제 적용 , lasso : 계수의 절댓값을 기준으로 규제 적용 . ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) . &#47551;&#51648; &#54924;&#44480; . 규제의 양을 임의로 조절 가능 (alpha조절 $ to$ 사람이 알려줘야하는 파라미터 :하이퍼파라미터) . alpha 값이 크면 규제 강도가 세지므로 계수 값을 더 줄여 조금 더 과소적합되지 않도록 유도 . alpha 값이 작으면 선형회귀 모형과 비슷해져 과대적합이 될 가능성이 커짐 . ridge = Ridge() ridge.fit(train_scaled , train_target) print(ridge.score(train_scaled , train_target)) . 0.9896101671037343 . print(ridge.score(test_scaled , test_target)) . 0.979069397761539 . 테스트 세트 점수가 정상으로 돌아옴, 너무 과대적합되지 않아 테스트 세트에서도 좋은 성능 . &#51201;&#51208;&#54620; alpha &#44050; &#52286;&#44592; . alpha 값에 대한 $R^2$ 값의 그래프를 그려보기 . train_score = [] test_score = [] . alpha_list = [0.001 , 0.01 , 0.1 , 1, 10, 100] for alpha in alpha_list: ridge = Ridge(alpha=alpha) #릿지모델생성 ridge.fit(train_scaled, train_target) #릿지모델훈련 train_score.append(ridge.score(train_scaled, train_target)) test_score.append(ridge.score(test_scaled, test_target)) . plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() . 파란색 :훈련 세트 , 주황색 : 테스트 세트 두 그래프가 가장 가깝고 테스트 세트의 점수가 가장 높은 -1 일때 , 즉 $10^{-1}=0.1$ 이 최적의 alpha값 ! . ridge = Ridge(alpha = 0.1) ridge.fit(train_scaled , train_target) print(ridge.score(train_scaled , train_target)) print(ridge.score(test_scaled , test_target)) . 0.9903815817570368 0.9827976465386954 . 훌륭한 결과 . &#46972;&#50136; &#54924;&#44480; . lasso = Lasso() lasso.fit(train_scaled , train_target) print(lasso.score(train_scaled , train_target)) print(lasso.score(test_scaled , test_target)) . 0.989789897208096 0.9800593698421886 . train_score = [] test_score = [] alpha_list = [0.001, 0.01 , 0.1 ,1 ,10 ,100] for alpha in alpha_list: lasso = Lasso(alpha=alpha , max_iter=10000) #릿지모델생성 lasso.fit(train_scaled, train_target) #릿지모델훈련 train_score.append(lasso.score(train_scaled, train_target)) test_score.append(lasso.score(test_scaled, test_target)) . /home/cjfal/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18778.697957794062, tolerance: 518.2793833333334 /home/cjfal/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12972.821345401813, tolerance: 518.2793833333334 . 오류메시지!? :사이킷런의 라쏘 모델은 최적의 계수를 찾기 위해 반복적인 계산을 수행하는데, 지정한 반복 횟수가 부족할 때 이런 경고가 발생. . plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() . 파란색 :훈련 세트 , 주황색 : 테스트 세트 두 그래프가 가장 가깝고 테스트 세트의 점수가 가장 높은 1 일때 , 즉 $10^{1}=10$ 이 최적의 alpha값 ! . lasso = Lasso(alpha = 10) lasso.fit(train_scaled , train_target) print(lasso.score(train_scaled , train_target)) print(lasso.score(test_scaled , test_target)) . 0.9888067471131866 0.9824470598706695 . &#46972;&#50136;&#45716; &#44228;&#49688;&#44032; 0&#51068; &#49688;&#46020; &#51080;&#45796;. $ to$ coef_ &#50640; &#51200;&#51109; . print(np.sum(lasso.coef_ == 0)) . 40 . 55개 특성을 주입했지만 사용한 특성은 15개라는 뜻 . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 다중 회귀 : 여러 개의 특성을 사용하는 회귀 모델입니다. 특성이 많으면 선형 모델은 강력한 성능을 발휘합니다. . - 특성 공학 : 주어진 특성을 조합하여 새로운 특성을 만드는 일련의 작업 과정입니다. . - 릿지 : 규제가 있는 선형 회귀 모델 중 하나이며 선형 모델의 계수를 작게 만들어 과대적합을 완화시킵니다. 릿지는 비교적 효과가 좋아 널리 사용하는 규제 방법입니다. . - 라쏘 : 또 다른 규제가 있는 선형 회귀 모델입니다. 릿지와 달리 계수 값을 아예 0으로 만들 수도 있습니다. . - 하이퍼파라미터 : 머신러닝 알고리즘이 학습하지 않는 파라미터입니다. 이런 파라미터는 사람이 사전에 지정해야 합니다. 대표적으로 릿지와 라쏘의 규제 강도 alpha 파라미터입니다. . - 핵심 패키지 . - pandas .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/23/%ED%98%BC%EA%B3%B5%EB%A8%B83%EC%B1%953%EC%9E%A5-%ED%8A%B9%EC%84%B1%EA%B3%B5%ED%95%99.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/23/%ED%98%BC%EA%B3%B5%EB%A8%B83%EC%B1%953%EC%9E%A5-%ED%8A%B9%EC%84%B1%EA%B3%B5%ED%95%99.html",
            "date": " • Feb 23, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "혼공머 03-2",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 from sklearn.linear_model import LinearRegression # 선형 회귀 . k-&#52572;&#44540;&#51217; &#51060;&#50883;&#51032; &#54620;&#44228; . &#45936;&#51060;&#53552; &#51456;&#48708; . https://bit.ly/perch_data . perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . &#54984;&#47144;&#49464;&#53944;&#50752; &#45936;&#51060;&#53552;&#49464;&#53944;&#47196; &#45208;&#45572;&#44592; . train_input , test_input , train_target , test_target = train_test_split(perch_length , perch_weight , random_state=42) . 2&#52264;&#50896; &#48176;&#50676;&#47196; &#48148;&#44984;&#44592; . train_input = train_input.reshape(-1,1) test_input = test_input.reshape(-1,1) . &#52572;&#44540;&#51217; &#51060;&#50883; &#44060;&#49688; 3&#51004;&#47196; &#54616;&#45716; &#47784;&#45944; &#54984;&#47144; . knr = KNeighborsRegressor(n_neighbors=3) knr.fit(train_input , train_target) . KNeighborsRegressor(n_neighbors=3) . print(knr.predict([[50]])) . [1033.33333333] . 50cm 인 농어의 무게를 예측 했는데 1033g 이 나왔다. . &#49328;&#51216;&#46020;&#50640; &#54364;&#44592;&#54616;&#50668; &#54869;&#51064; . distances, indexes = knr.kneighbors([[50]]) # 50cm 농어의 이웃을 구합니다. plt.scatter(train_input , train_target) # 훈련 세트의 산점도를 그린다. # 훈련 세트 중에서 이웃 샘플만 다시 그린다. plt.scatter(train_input[indexes], train_target[indexes], marker=&#39;D&#39;) # 50cm 농어의 데이터 plt.scatter(50, 1033, marker= &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 이 산점도를 보면 길이가 커질수록 농어의 무게가 증가하는 경향이 있다. . print(np.mean(train_target[indexes])) # 이웃 샘플의 타깃의 평균 . 1033.3333333333333 . k-최근접 이웃 회귀는 훈련세트의 범위를 넘어가면 엉뚱한 값 예측 . &#54620;&#48264;&#45908; &#49328;&#51216;&#46020;&#47196; &#54869;&#51064; . distances , indexes = knr.kneighbors([[100]]) # 훈련 세트의 산점도 그리기 plt.scatter(train_input, train_target) # 훈련세트 중에서 이웃 샘플만 다시 그리기 plt.scatter(train_input[indexes],train_target[indexes], marker= &#39;D&#39;) # 100cm 농어 데이터 plt.scatter(100 , 1033 , marker=&#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 농어가 아무리 커도 무게가 더 늘어나지 않는다! . k-최근접 이웃 회귀를 이용하려면 가장 큰 농어가 포함되도록 훈련 세트를 다시 만들어야 한다. . &#49440;&#54805; &#54924;&#44480; . 대표적인 회귀 알고리즘 . lr = LinearRegression() # 선형 회귀 모델 훈련 lr.fit(train_input , train_target) # 50cm 농어에 대해 예측 print(lr.predict([[50]])) . [1241.83860323] . 아주 높게 예측 . print(lr.coef_ , lr.intercept_) . [39.01714496] -709.0186449535477 . 모델 파라미터, 최적의 모델 파라미터 찾기 :모델 기반 학습 . plt.scatter(train_input, train_target) # 15에서 50까지 1차 방정식 그래프 그리기 plt.plot([15,50], [15*lr.coef_ + lr.intercept_, 50*lr.coef_ + lr.intercept_]) # 50cm 농어 데이터 plt.scatter(50, 1241.8 , marker=&#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . print(lr.score(train_input, train_target)) # 테스트 세트의 결정계수 print(lr.score(test_input, test_target)) . 0.939846333997604 0.8247503123313558 . 전체적으로 과소 적합 되었다고 볼 수 있다. . 그런데 그래프 왼쪽 아래부분이 굉장히 이질적이다. . 2차방정식이 더 어울릴 것 같다. . &#45796;&#54637; &#54924;&#44480; . train_poly = np.column_stack((train_input ** 2 , train_input)) test_poly = np.column_stack((test_input ** 2 , test_input)) . print(train_poly.shape , test_poly.shape) . (42, 2) (14, 2) . 열이 2개로 늘어남 . # 타깃값은 그대로 lr = LinearRegression() lr.fit(train_poly , train_target) print( lr.predict([[50**2, 50]])) . [1573.98423528] . 1절에서 훈련한 것 보다 더 높은 값 예측 . print(lr.coef_, lr.intercept_) . [ 1.01433211 -21.55792498] 116.05021078278259 . 무게 = $ 1.01 times 길이^2 - 21.6 times 길이 + 116.05 $ . &#51687;&#51008; &#51649;&#49440;&#51012; &#51060;&#50612;&#49436; &#44536;&#47532;&#47732; &#47560;&#52824; &#44257;&#49440;&#52376;&#47100; &#54364;&#54788;&#54624; &#49688; &#51080;&#45796;. . point = np.arange(15, 50) # 훈련 세트의 산점도 그리기 plt.scatter(train_input , train_target) # 15에서 49까지 2차 방정식 그래프 그리기 plt.plot(point, (1.01)*point**2 - 21.6 * point + 116.05) # 50cm 농어 데이터 plt.scatter(50, 1574, marker=&#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . print(lr.score(train_poly , train_target)) # 테스트 세트 결정 계수 print(lr.score(test_poly , test_target)) . 0.9706807451768623 0.9775935108325121 . 그래프 모양도 흡족하고, 결정계수도 두 세트 모두 좋은데 과소적합이 아직 조금 남아있는듯. . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 선형 회귀 : 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾습니다. 특성이 하나면 직선 방정식이 됩니다. . 선형 회귀가 찾은 특성과 타깃 사이의 관계는 선형 방정식의 계수 또는 가중치에 저장됩니다. 머신러닝에서 종종 가중치는 방정식의 기울기와 절편을 모두 의미하는 경우가 많습니다. . - 모델 파라미터 : 선형 회귀가 찾은 가중치처럼 머신러닝 모델이 특성에서 학습한 파라미터를 말합니다. . - 다항 회귀 : 다항식을 사용하여 특성과 타깃 사이의 관계를 나타냅니다. 이 함수는 비선형일수 있지만 여전히 선형 회귀로 표현할 수 있습니다. . - 핵심 패키지 . - scikit-learn . 1 LinearRegression :사이킷런의 선형 회귀 클래스입니다. fit_intercept 매개변수를 False로 지정하면 절편을 학습하지 않습니다. 이 매개변수의 기본값은 True입니다. 학습된 모델의 coef 속성은 특성에 대한 계수를 포함한 배열입니다. 즉 이 배열의 크기는 특성의 개수와 같습니다. intercept 속성에는 절편이 저장되어 있습니다. .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/22/%ED%98%BC%EA%B3%B5%EB%A8%B83%EC%B1%952%EC%9E%A5%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/22/%ED%98%BC%EA%B3%B5%EB%A8%B83%EC%B1%952%EC%9E%A5%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "혼공머 03-1",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier # k 최근접이웃 from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor # 결정계수 from sklearn.metrics import mean_absolute_error # 타깃과 예측의 절댓값 오차 평균을 반환 . k-&#52572;&#44540;&#51217; &#51060;&#50883; &#54924;&#44480; . &#54924;&#44480; . 정해진 클래스가 없고 임의의 수치를 출력 . 두 변수 사이의 상관관계를 분석하는 방법 . &#45936;&#51060;&#53552; &#51456;&#48708; . https://bit.ly/perch_data . perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . &#49328;&#51216;&#46020; &#44536;&#47532;&#44592; . plt.scatter(perch_length,perch_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . &#54984;&#47144;&#49464;&#53944; &#53580;&#49828;&#53944;&#49464;&#53944; &#45208;&#45572;&#44592; . train_input , test_input , train_target , test_target = train_test_split(perch_length,perch_weight, random_state=42) . 1&#52264;&#50896; &#48176;&#50676;&#51012; 2&#52264;&#50896; &#48176;&#50676;&#47196; &#47564;&#46308;&#44592; . test_array = np.array([1,2,3,4]) print(test_array.shape) . (4,) . test_array = test_array.reshape(2, 2) # 바꾸려는 배열의 크기를 지정가능 print(test_array.shape) . (2, 2) . train_input = train_input.reshape(-1,1) test_input = test_input.reshape(-1,1) # reshape(-1,1) 은 배열의 전체 원소 개수를 매번 외우지 않아도 된다. print(train_input.shape , test_input.shape) . (42, 1) (14, 1) . &#44208;&#51221;&#44228;&#49688; $R^2$ . knr = KNeighborsRegressor() #k최근접 이웃 회귀 모델을 훈련 knr.fit(train_input, train_target) . KNeighborsRegressor() . print(knr.score(test_input,test_target)) . 0.992809406101064 . 결정계수 . &#44208;&#51221;&#44228;&#49688; $ R^2 = 1- frac{ sum(&#53440;&#44611; - &#50696;&#52769;)^2}{ sum(&#53440;&#44611; - &#54217;&#44512;)^2} $ . 예측이 타깃에 가까워질수록 1에 가까워진다. . test_prediction = knr.predict(test_input) mae = mean_absolute_error(test_target , test_prediction) print(mae) . 19.157142857142862 . 결과 예측이 평균적으로 19g정도 다르다는 것을 알 수 있음 . &#44284;&#45824; &#51201;&#54633; vs &#44284;&#49548; &#51201;&#54633; . print(knr.score(train_input,train_target)) . 0.9698823289099254 . 앞서 진행한 테스트 세트와 값이 다르고 더 작아졌다. . &#44284;&#45824; &#51201;&#54633; : &#54984;&#47144;&#49464;&#53944;&#50640;&#49436; &#51216;&#49688;&#44032; &#44361;&#51109;&#55176; &#51339;&#50520;&#45716;&#45936; &#53580;&#49828;&#53944; &#49464;&#53944;&#50640;&#49436;&#45716; &#51216;&#49688;&#44032; &#44361;&#51109;&#55176; &#45208;&#49248; &#46412; . 모델을 조금 덜 복잡하게 만들어 해결 . &#44284;&#49548; &#51201;&#54633; : &#54984;&#47144;&#49464;&#53944; &#48372;&#45796; &#53580;&#49828;&#53944; &#49464;&#53944;&#51032; &#51216;&#49688;&#44032; &#45458;&#44144;&#45208; &#46160; &#51216;&#49688;&#44032; &#47784;&#46160; &#45320;&#47924; &#45230;&#51008; &#44221;&#50864; . 모델을 조금 더 복잡하게 만들어 해결 . k&#52572;&#44540;&#51217;&#51060;&#50883; &#50508;&#44256;&#47532;&#51608;&#51032; &#44592;&#48376;&#44050; k&#47484; 5&#50640;&#49436; 3&#51004;&#47196; &#45230;&#52628;&#44592; . knr.n_neighbors = 3 knr.fit(train_input , train_target) print(knr.score(train_input,train_target)) . 0.9804899950518966 . k 값을 줄였더니 훈련세트의 $R^2$ 가 높아졌다. . print(knr.score(test_input,test_target)) . 0.9746459963987609 . 테스트 세트의 점수가 훈련 세트보다 낮아졌다 . $ to $ 과소적합 문제 해결( 너무 큰 차이도 없어서 과대적합이라고 볼 수 도 없다.) . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 회귀 : 임의의 수치를 예측하는 문제 (타깃값도 임의의 수치가 됨) . - k-최근접 이웃 회귀 : k최근접 이웃 알고리즘으로 회귀 문제를 해결. 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값을 평균하여 예측으로 삼는다. . - 결정계수 : 대표적인 회귀 문제의 성능 측정 도구입니다. 1에 가까울수록 좋고, 0에 가깝다면 성능이 나쁜 모델 . - 과대적합 : 모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높을 때 일어납니다. 모델이 훈련 세트에 너무 집착해서 데이터에 내재된 거시적인 패턴을 감지하지 못합니다 . - 과소적합 : 과대적합에 반대 . 훈련 세트와 테스트 세트 성능이 모두 동일하게 낮거나 테스트 세트 성능이 오히려 더 높을 때 일어납니다. 이런 경우 더 복잡한 모델을 사용해 훈련 트에 맞는 모델을 만들어야 합니다. . - 핵심 패키지 . - scikit-learn . 1 KNeighborsRegressor :k-최근접 이웃 회귀 모델을 만드는 사이킷런 클래스입니다. n_neighbors 매개변수로 이웃의 개수를 지정합니다. 기본값은 5입니다. 다른 매개변수는 KNeighborsClassifier 클래스와 거의 동일합니다.`2` mean absolute_error() : 회귀 모델의 평균 절댓값 오차를 계산합니다. 첫 번째 매개변수는 타깃, 두 번째 매개변수는 예측값을 전달합니다. 이와 비슷한 함수로는 평균 제곱 오차를 계산하는 mean_squared_error()가 있습니다.(이 함수는 타깃과 예측을 뺀 값을 제곱한 다음 전체 샘플에 대해 평균한 값을 반환합니다.) .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/21/%ED%98%BC%EA%B3%B5%EB%A8%B83%EC%B1%951%EC%9E%A5%EA%B2%B0%EC%A0%95%EA%B3%84%EC%88%98.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/21/%ED%98%BC%EA%B3%B5%EB%A8%B83%EC%B1%951%EC%9E%A5%EA%B2%B0%EC%A0%95%EA%B3%84%EC%88%98.html",
            "date": " • Feb 21, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "혼공머 02-2",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split . &#45936;&#51060;&#53552; &#51456;&#48708; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . &#47532;&#49828;&#53944; &#48537;&#51060;&#44592; &#50672;&#49845; . np.column_stack(([1,2,3],[4,5,6])) . array([[1, 4], [2, 5], [3, 6]]) . fish_data = np.column_stack((fish_length, fish_weight)) print(fish_data[:5]) . [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ]] . 가지런히 정렬된 모습으로 출력된다. . &#50896;&#49548;&#44032; &#54616;&#45208;&#51064; &#47532;&#49828;&#53944; &#47564;&#46308;&#44592; . np.ones(5) . array([1., 1., 1., 1., 1.]) . np.zeros(5) . array([0., 0., 0., 0., 0.]) . fish_target = np.concatenate((np.ones(35),np.zeros(14))) # concatenate는 옆으로 이어 붙인다. . fish_target . array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#54984;&#47144; &#49464;&#53944;&#50752; &#53580;&#49828;&#53944; &#49464;&#53944; &#45208;&#45572;&#44592; . train_test_split() &#51076;&#54252;&#53944; &#54596;&#50836; . &#54984;&#47144;&#49464;&#53944;&#50752; &#53580;&#49828;&#53944; &#49464;&#53944; &#49373;&#49457;, random_state= &#51088;&#46041;&#51004;&#47196; &#47004;&#45924;&#49884;&#46300; &#49373;&#49457; . train_input , test_input , train_target , test_target = train_test_split(fish_data,fish_target,random_state=42) # 기본적으로 25%를 테스트 세트로 떼어냄 . print(train_input.shape , test_input.shape) . (36, 2) (13, 2) . print(train_target.shape , test_target.shape) . (36,) (13,) . print(test_target) # 도미와 빙어가 잘 섞였는지 보여줌, 세트비율 3.3 : 1 . [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] . 샘플링 편향 나타남 $ to$ stratify로 해결 . train_input , test_input , train_target , test_target = train_test_split(fish_data,fish_target,stratify = fish_target,random_state=42) . print(test_target) # 세트 비율 2.25 :1 . [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] . &#49688;&#49345;&#54620; &#46020;&#48120; &#54620;&#47560;&#47532; . knearest &#54644;&#48372;&#44592; . kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) . 1.0 . &#49352;&#47196;&#50868; &#45936;&#51060;&#53552; &#48516;&#47448; &#54644;&#48372;&#44592; . print(kn.predict([[25,150]])) . [0.] . 왜 0인지 확인하기 . plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 이상하게 분류된다.. . k최근접 이웃은 주변의 샘플 중에서 다수인 클래스를 예측으로 사용. . 거리가 가까운 데이터 확인 . distances , indexs = kn.kneighbors([[25, 150]]) . plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.scatter(train_input[indexs,0], train_input[indexs,1],marker=&#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 맷플랏립 마커 리스트 : https://bit.ly/matplotlib_marker . 세모와 가장 가까운 5개가 다이아몬드인데 4개가 빙어 . print(distances) # x축과 y축의 범위가 달라서 이상하게 보이는 것(스케일이 다르다) . [[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]] . &#44144;&#47532; &#44592;&#48152;&#51032; &#50508;&#44256;&#47532;&#51608;&#51012; &#51060;&#50857;&#54624; &#46412;&#45716; &#45936;&#51060;&#53552;&#47484; &#54364;&#54788;&#54616;&#45716; &#44592;&#51456;&#51060; &#45796;&#47476;&#47732; &#50508;&#44256;&#47532;&#51608;&#51060; &#50732;&#48148;&#47476;&#44172; &#50696;&#52769;&#54624; &#49688; &#50630;&#45796;! . 특성값을 일정한 기준으로 처리가 필요 :데이터 전처리 . &#54364;&#51456;&#51216;&#49688; &#51204;&#52376;&#47532; (standard score) . 평균을 빼고 표준편차를 나누어준다. . mean = np.mean(train_input, axis=0) std = np.std(train_input, axis=0) #axis=0 : 열 기준, axis=1 : 행 기준 . print(mean, std) . [ 27.29722222 454.09722222] [ 9.98244253 323.29893931] . train_scaled = (train_input - mean) / std #넘파이 배열 사이의 계산 : Broadcasting . &#51204;&#52376;&#47532; &#45936;&#51060;&#53552;&#47196; &#47784;&#45944; &#54984;&#47144;&#54616;&#44592; . plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 이상한 모양 $ to$ 값의 범위가 크게 달라졌기 때문 . 동일한 기준으로 분석할 샘플데이터 변환 . new = ([25, 150] - mean ) /std plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 축의 범위가 -1.5~1.5로 변경 . &#53580;&#49828;&#53944; &#49464;&#53944;&#47196; &#54217;&#44032; . kn.fit(train_scaled,train_target) . KNeighborsClassifier() . 같은 비율로 산점도를 그려야 함 . test_scaled = (test_input - mean)/std . 평가 . kn.score(test_scaled,test_target) . 1.0 . kneighbors()&#47196; &#51060; &#49368;&#54540;&#51032; k&#52572;&#44540;&#51217; &#51060;&#50883; &#44396;&#54616;&#44256; &#49328;&#51216;&#46020; . distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(new[0], new[1], marker = &#39;^&#39;) plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1],marker=&#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 세모에 가장 가까운 샘플은 모두 다이아몬드!! . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 데이터전처리 : 머신러닝 모델에 훈련 데이터를 주입하기 전에 가공하는 단계. 때로는 데이터 전처리에 많은 시간이 소모되기도 함. . - 표준점수 : 훈련세트의 스케일을 바꾸는 대표적인 방법 중 하나. 표준점수를 얻으려면 특성의 평균을 빼고 표준편차로 나눔. 반드시 훈련세트의 평균과 표준편차로 테스트 세트를 바꿔야 함 . - 브로드캐스팅 : 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능. . - 핵심 패키지 : scikit-learn의 train_test_split() &amp; kneighbors() .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/20/%ED%98%BC%EA%B3%B5%EB%A8%B82%EC%B1%952%EC%9E%A5%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/20/%ED%98%BC%EA%B3%B5%EB%A8%B82%EC%B1%952%EC%9E%A5%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "혼공머 02-1",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier . &#51648;&#46020;&#54617;&#49845;&#44284; &#48708;&#51648;&#46020;&#54617;&#49845; . 지도학습 :훈련하기 위한 데이터와 정답이 필요&gt; 비지도학습 :타깃없이 입력데이터만 사용 . 훈련데이터 = 입력 + 타깃 . &#47532;&#49828;&#53944; &#51456;&#48708; . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#49373;&#49457; . fish_data = [[l,w] for l, w in zip(fish_length, fish_weight)] . fish_target = [1]*35 + [0]*14 . &#51064;&#45937;&#49828; &#51648;&#51221; . kn = KNeighborsClassifier() . print(fish_data[0:5]) # 슬라이싱 출력 . [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] . train_input = fish_data[:35] #훈련 세트로 타깃값 중 0부터 34번째 인덱스까지 사용 train_target = fish_target[:35] #테스트 세트로 입력값 중 35번째부터 마지막 인덱스까지 사용 test_input = fish_data[35:] #테스트 세트로 타깃값 중 35번째부터 마지막 인덱스까지 사용 test_target = fish_target[35:] . kn = kn.fit(train_input, train_target) # 훈련 세트로 fit 이용 kn.score(test_input,test_target) # 테스트 세트로 score 이용 . 0.0 . 34번째 까지 방어니까 제대로된 훈련데이터가 아님 $ to $ 샘플링 편향 . &#45796;&#49884; &#51064;&#45937;&#49828;&#47484; &#47004;&#45924;&#51004;&#47196; &#51648;&#51221; . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr.shape) # shape : (샘플수 , 특성수) . (49, 2) . &#53440;&#44611;&#51008; &#49368;&#54540;&#44284; &#54632;&#44760; &#51060;&#46041;&#54644;&#50556; &#54620;&#45796;. (&#51064;&#45937;&#49828;&#47484; &#44592;&#50613;&#54616;&#45716; &#44163;&#51060; &#51473;&#50836;, &#44536;&#45285; &#47004;&#45924;&#51004;&#47196; &#49438;&#50612;&#46020; &#46120;) . np.random.seed(42) index = np.arange(49) np.random.shuffle(index) . print(index) . [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] . &#47004;&#45924;&#54616;&#44172; 35&#44060; &#49368;&#54540;&#51012; &#54984;&#47144; &#49464;&#53944;&#47196; &#47564;&#46308;&#44592; . train_input = input_arr[index[:35]] train_target = target_arr[index[:35]] . &#45208;&#47672;&#51648;&#47196; &#53580;&#49828;&#53944; &#49464;&#53944; &#47564;&#46308;&#44592; . test_input = input_arr[index[35:]] test_target = target_arr[index[35:]] . &#51096; &#49438;&#51064;&#51648; &#49328;&#51216;&#46020;&#47196; &#48372;&#44592; . plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(test_input[:,0], test_input[:,1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 파란색 :훈련세트, 주황색 : 테스트세트 . &#46160; &#48264;&#51704; &#47672;&#49888;&#47084;&#45789; &#54532;&#47196;&#44536;&#47016; . kn = kn.fit(train_input,train_target) . kn.score(test_input,test_target) . 1.0 . 정확도 100% . &#51221;&#54869;&#46020; &#54869;&#51064; . kn.predict(test_input) . array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) . test_target . array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) . 정확히 일치 . 훈련할 때 들어 있지 않은 샘플로 테스트 했기 때문에 올바르게 평가한 것! . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 지도학습 : 입력과 타깃을 전달하여 모델을 훈련한 다음 새로운 데이터를 예측하는 데 활용.(k-최근접이웃이 지도학습 알고리즘) . - 비지도학습 : 타깃데이터가 없다. 따라서 무엇을 예측하는 것이 아니라 입력 데이터에서 어떤 특징을 찾는데 주로 활용함. . - 훈련세트 : 모델을 훈련할 때 사용하는 데이터 , 보통 훈련세트가 클수록 좋다. 테스트세트를 제외한 모든 데이터 활용. . - 테스트세트 : 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많음. 전체 데이터가 아주 크다면 1%정도만 해도 충분 . - 사용 패키지 : numpy .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/19/%ED%98%BC%EA%B3%B5%EB%A8%B82%EC%B1%951%EC%9E%A5%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/19/%ED%98%BC%EA%B3%B5%EB%A8%B82%EC%B1%951%EC%9E%A5%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.html",
            "date": " • Feb 19, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "혼공머 01-3",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier . &#46020;&#48120; &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] . plt.scatter(bream_length, bream_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show #scatter:산점도 . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . &#48169;&#50612; &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . https://gist.github.com/rickiepark/1e89fe2a9d4ad92bc9f073163c9a37a7 . smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . plt.scatter(bream_length, bream_weight) plt.scatter(smelt_length, smelt_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . K-nearest Neighbors &#50508;&#44256;&#47532;&#51608; . 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 채용 . 데이터가 아주 많은 경우 사용하기 어려움 . 기본값은 5 이며 n_neigbors 로 바꿀 수 있다 . length = bream_length + smelt_length weigth = bream_weight + smelt_weight . fish_data = [[l,w] for l, w in zip(length,weigth) ] . print(fish_data) . [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]] . # 도미를 1 방어를 0 으로 가정 fish_target = [1] * 35 + [0] * 14 print(fish_target) . [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] . kn = KNeighborsClassifier() . kn.fit(fish_data,fish_target) #fit : 주어진 데이터로 알고리즘 훈련 . KNeighborsClassifier() . kn.score(fish_data,fish_target) # score : 정확도 (분류) = 맞힌 데이터 수 / 전체 데이터 수 . 1.0 . &#49352;&#47196;&#50868; &#45936;&#51060;&#53552;&#47484; &#45347;&#50612;&#49436; &#48516;&#47448;&#54644;&#48372;&#44592; . kn.predict([[30,600]]) # predict : 새로운 데이터의 정답을 예측 . array([1]) . print(kn._fit_X) # _fit_X 속성에 전달한 fish_data를 전부 가지고 있음 . [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 6.7] [ 10.5 7.5] [ 10.6 7. ] [ 11. 9.7] [ 11.2 9.8] [ 11.3 8.7] [ 11.8 10. ] [ 11.8 9.9] [ 12. 9.8] [ 12.2 12.2] [ 12.4 13.4] [ 13. 12.2] [ 14.3 19.7] [ 15. 19.9]] . print(kn._y) . [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] . &#48276;&#50948;&#47484; 49&#47196; &#51312;&#51221; &#50696;&#49884; . 예측이 좋지 않아진다 . kn49 = KNeighborsClassifier(n_neighbors=49) # 참고 데이터를 49개로 한 kn49 모델 . kn49.fit(fish_data,fish_target) . KNeighborsClassifier(n_neighbors=49) . kn49.score(fish_data,fish_target) . 0.7142857142857143 . &#50696;&#51228; . 기본값을 5~49 로 바꾸면서 1.0 아래로 점수가 내려가기 시작하는 이웃의 개수 찾기 . kn = KNeighborsClassifier() kn.fit(fish_data,fish_target) . KNeighborsClassifier() . for n in range(5,50): kn.n_neighbors = n score = kn.score(fish_data,fish_target) if score &lt; 1 : print(n, score) break . 18 0.9795918367346939 . n = 18 에서부터 1보다 작아진다 . . &#47560;&#47924;&#47532; . * 키워드로 끝내는 핵심 포인트 . - 특성 : 데이터를 표현하는 하나의 성질. 이 절에서 생선 데이터 각각을 길이와 무게 특성으로 나타냄. . - 훈련 : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정 ( 사이킷런의 fit 메서드) . - k-최근접 이웃 알고리즘 : 가장 간단한 머신러닝 알고리즘 , 사실 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부. . - 모델 : 머신러닝 프로그램에서 알고리즘이 구현된 객체 . - 정확도 : 정확한 답을 몇개 맞혔는지 백분율로 나타낸 값. 사이킷런에서는 0~1 사이의 값으로 출력 . - 사용 패키지 : matplolib , scikit-learn .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/18/%ED%98%BC%EA%B3%B5%EB%A8%B81%EC%B1%953%EC%9E%A5knearest.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/18/%ED%98%BC%EA%B3%B5%EB%A8%B81%EC%B1%953%EC%9E%A5knearest.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "혼공머 01-3",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 from sklearn.neighbors import KNeighborsClassifier . &#46020;&#48120; &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] . plt.scatter(bream_length, bream_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show #scatter:산점도 . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . &#48169;&#50612; &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . https://gist.github.com/rickiepark/1e89fe2a9d4ad92bc9f073163c9a37a7 . smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . plt.scatter(bream_length, bream_weight) plt.scatter(smelt_length, smelt_weight) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . K-nearest Neighbors &#50508;&#44256;&#47532;&#51608; . 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 채용 . 데이터가 아주 많은 경우 사용하기 어려움 . 기본값은 5 이며 n_neigbors 로 바꿀 수 있다 . length = bream_length + smelt_length weigth = bream_weight + smelt_weight . fish_data = [[l,w] for l, w in zip(length,weigth) ] . print(fish_data) . [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]] . # 도미를 1 방어를 0 으로 가정 fish_target = [1] * 35 + [0] * 14 print(fish_target) . [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] . kn = KNeighborsClassifier() . kn.fit(fish_data,fish_target) #fit : 주어진 데이터로 알고리즘 훈련 . KNeighborsClassifier() . kn.score(fish_data,fish_target) # score : 정확도 (분류) = 맞힌 데이터 수 / 전체 데이터 수 . 1.0 . &#49352;&#47196;&#50868; &#45936;&#51060;&#53552;&#47484; &#45347;&#50612;&#49436; &#48516;&#47448;&#54644;&#48372;&#44592; . kn.predict([[30,600]]) # predict : 새로운 데이터의 정답을 예측 . array([1]) . print(kn._fit_X) # _fit_X 속성에 전달한 fish_data를 전부 가지고 있음 . [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 6.7] [ 10.5 7.5] [ 10.6 7. ] [ 11. 9.7] [ 11.2 9.8] [ 11.3 8.7] [ 11.8 10. ] [ 11.8 9.9] [ 12. 9.8] [ 12.2 12.2] [ 12.4 13.4] [ 13. 12.2] [ 14.3 19.7] [ 15. 19.9]] . print(kn._y) . [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] . &#48276;&#50948;&#47484; 49&#47196; &#51312;&#51221; &#50696;&#49884; . 예측이 좋지 않아진다 . kn49 = KNeighborsClassifier(n_neighbors=49) # 참고 데이터를 49개로 한 kn49 모델 . kn49.fit(fish_data,fish_target) . KNeighborsClassifier(n_neighbors=49) . kn49.score(fish_data,fish_target) . 0.7142857142857143 . &#50696;&#51228; . 기본값을 5~49 로 바꾸면서 1.0 아래로 점수가 내려가기 시작하는 이웃의 개수 찾기 . kn = KNeighborsClassifier() kn.fit(fish_data,fish_target) . KNeighborsClassifier() . for n in range(5,50): kn.n_neighbors = n score = kn.score(fish_data,fish_target) if score &lt; 1 : print(n, score) break . 18 0.9795918367346939 . n = 18 에서부터 1보다 작아진다 .",
            "url": "https://cjfal.github.io/dj/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/18/%ED%98%BC%EA%B3%B5%EB%A8%B801_3_knearest.html",
            "relUrl": "/python/%ED%98%BC%EA%B3%B5%EB%A8%B8/2022/02/18/%ED%98%BC%EA%B3%B5%EB%A8%B801_3_knearest.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "회귀분석 5장",
            "content": "5.2 . - 어떤 화학반응에서 촉매의 양(x)이 합성물의 소득량(y)에 어떻게 영향을 끼치는지 알아보기 위해 12번 실험하여 다음의 자료를 얻었다. . x(g) 1 1 1 2 2 2 4 4 4 8 8 8 . y(g) | 13.5 | 15.4 | 16.1 | 18.2 | 19.6 | 20.2 | 21.8 | 22.2 | 23.1 | 23.6 | 24.7 | 24.9 | . x &lt;- c(1,1,1,2,2,2,4,4,4,8,8,8) y &lt;- c(13.5,15.4,16.1,18.2,19.6,20.2,21.8,22.2,23.1,23.6,24.7,24.9) . 1) x&#50640;&#45824;&#54620; y&#51032; &#49328;&#51216;&#46020;&#47484; &#44536;&#47140;&#46972;. . plot(x,y,col=&quot;red&quot;) . 2) $ log_{10} x$&#47484; &#44228;&#49328;&#54616;&#44256; $ log_{10} x$&#50640; &#45824;&#54620; y&#44050;&#51032; &#49328;&#51216;&#46020;&#47484; &#44536;&#47140;&#46972;. . logx &lt;- log(x,base=10) logx . &lt;ol class=list-inline&gt;0 | 0 | 0 | 0.301029995663981 | 0.301029995663981 | 0.301029995663981 | 0.602059991327962 | 0.602059991327962 | 0.602059991327962 | 0.903089986991944 | 0.903089986991944 | 0.903089986991944 | &lt;/ol&gt; plot(logx,y,col=&quot;blue&quot;) . par(mfrow=c(1,2)) plot(x,y,col=&quot;red&quot;) plot(logx,y,col=&quot;blue&quot;,xlim=c(0,8)) # xlim 으로 x범위 조정 . 3) &#50948; 1)&#44284; 2) &#51473; &#50612;&#45712; &#44163;&#51060; &#45908; &#49440;&#54805;&#50640; &#44032;&#44620;&#50868;&#44032;? . 2)의 로그 그래프가 같은 범위에서 더 선형에 가깝다. . . 5.3 . - 연습문제 5.2의 자료에 대해 다음 질문에 답하여라. . 1) $ y = beta_0 + beta_1 x + epsilon $ &#47484; &#51201;&#54633;&#49884;&#53412;&#44256; $MSE$ &#47484; &#44396;&#54616;&#50668;&#46972;. . lm53 &lt;- lm(y~x) coefficients(lm53) . &lt;dl class=dl-inline&gt;(Intercept)15.8130434782609x1.18985507246377&lt;/dl&gt; $y= 15.51 + 1.19x$ . anova(lm53) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x 1 | 122.10888 | 122.108877 | 34.1147 | 0.0001636405 | . Residuals10 | 35.79362 | 3.579362 | NA | NA | . $MSE$ :Mean Square Residuals = 3.579362 . 2) $ y = beta_0 + beta_1 log_{10} x + epsilon $ &#47484; &#44032;&#51221;&#54616;&#44256; $MSE$ &#47484; &#44228;&#49328;&#54616;&#50668;&#46972;. . lm53log &lt;- lm(y~logx) anova(lm53log) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . logx 1 | 146.32817 | 146.328167 | 126.4247 | 5.374543e-07 | . Residuals10 | 11.57433 | 1.157433 | NA | NA | . $MSE$ :Mean Square Residuals = 1.157433 . 3) &#50948; 1)&#44284; 2)&#51032; &#47784;&#54805; &#51473; &#50612;&#45712; &#44163;&#51060; &#45908; &#51201;&#54633;&#54620;&#51648; $MSE$ &#47484; &#51060;&#50857;&#54616;&#50668; &#48708;&#44368;&#54616;&#50668;&#46972;. . $ log_{10} x$ 에서의 $MSE$ 가 더 작다. 즉, 2)의 모형이 더 적합하다. . . 5.12 . - 다음 과 같은 자료에 대하여 . x 2 6 10 . y | 4 | 7 | 4 | . 1) &#45800;&#49692;&#54924;&#44480;&#47784;&#54805; $ y = beta_0 + beta_1 x + epsilon $ , $ epsilon sim N(0, sigma^2)$&#51012; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#51649;&#49440;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. &#46608; $ beta_1$ &#51032; 90% &#49888;&#47280;&#44396;&#44036;&#51012; &#44396;&#54616;&#50668;&#46972;. . x &lt;- c(2, 6, 10) y &lt;- c(4, 7, 4) lm512 &lt;- lm(y~x) coefficients(lm512) confint(lm512,level = 0.90) # 기울기와 절편의 90% 신뢰구간 . &lt;dl class=dl-inline&gt;(Intercept)5x-4.81432375190295e-16&lt;/dl&gt; A matrix: 2 × 2 of type dbl 5 %95 % . (Intercept)-13.676329 | 23.676329 | . x -2.733935 | 2.733935 | . 기울기 $ beta_1$의 90% 신뢰구간:$(-2.733935, 2.733935)$ . 2) $ y = beta_0 + beta_1 x + epsilon $ , $ epsilon sim N(0,k^2 {x_i}^2)$ &#51012; &#44032;&#51221;&#54616;&#50668; &#44032;&#51473;&#54924;&#44480;&#51649;&#49440;&#51012; &#52628;&#51221;&#54616;&#44256; $ beta_1$ &#51032; 90% &#49888;&#47280;&#44396;&#44036;&#51012; &#44396;&#54616;&#50668;&#46972;. . w512 &lt;- sqrt(1/var(x)) . 3) &#50948; 1), 2)&#51032; &#44208;&#44284;&#47484; &#48708;&#44368;&#54616;&#50668;&#46972; . . 5.14 . - 다음은 어떤 컴퓨터 부품의 과거 14개월(x) 동안의 판매액(y)에 관한 자료이다. . x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 . y | 6.0 | 6.3 | 6.1 | 6.8 | 7.5 | 8.0 | 8.1 | 8.5 | 9.0 | 8.7 | 7.9 | 8.2 | 8.4 | 9.0 | . 1) &#45936;&#51060;&#53552;&#51032; &#49328;&#51216;&#46020;&#47484; &#44536;&#47140;&#46972;. . x &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14) y &lt;- c(6.0,6.3,6.1,6.8,7.5,8.0,8.1,8.5,9.0,8.7,7.9,8.2,8.4,9.0) plot(x,y) . 2) &#45800;&#49692;&#54924;&#44480;&#47784;&#54805;&#51012; &#44032;&#51221;&#54616;&#44256; &#51092;&#52264;&#51032; &#49328;&#51216;&#46020;&#47484; &#45208;&#53440;&#45236;&#44256; &#51088;&#44592;&#49345;&#44288;&#51060; &#51316;&#51116;&#54616;&#45716;&#51648;&#47484; &#44160;&#53664;&#54616;&#50668;&#46972;. . lm514 &lt;- lm(y~x) coefficients(lm514) . &lt;dl class=dl-inline&gt;(Intercept)6.13296703296704x0.215604395604395&lt;/dl&gt; err &lt;- function(A){ print(0.215604395604395 * A + 6.13296703296704) } . plot(err(x)-y) . [1] 6.348571 6.564176 6.779780 6.995385 7.210989 7.426593 7.642198 7.857802 [9] 8.073407 8.289011 8.504615 8.720220 8.935824 9.151429 . 3) Durbin-Watson d &#53685;&#44228;&#47049;&#51032; &#44050;&#51012; &#44396;&#54616;&#50668; $H_0 : rho = 0 , H_1 : rho neq 0 $ &#51012; $ alpha = 0.05 $ &#47196; &#44160;&#51221;&#54616;&#50668;&#46972;. . 4) &#50948; 3)&#51032; &#44160;&#51221;&#44208;&#44284;&#45716; 2)&#51032; &#49328;&#51216;&#46020;&#50640;&#49436; &#50619;&#51008; &#45712;&#45196;&#44284; &#51068;&#52824;&#54616;&#45716;&#51648; &#45436;&#51032;&#54616;&#50668;&#46972;. .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/12/31/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D5%EC%9E%A5.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/12/31/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D5%EC%9E%A5.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Title",
            "content": "파이썬 입문(1분반) 기말고사 - 최규빈교수님 . 201514142 김동준 에너지화학공학과 . 1 . import pandas as pd import numpy as np import random as rd def upgrade(day,usernum): success = [1,1,1,0,0,0,0,0,0,0] user = [0] while len(user) &lt; day+1: if (user[-1] == 5): user.append(5) elif (rd.choice(success) == 1): user.append(user[-1]+1) else: user.append(user[-1]) user_df = pd.DataFrame(user) user_df.columns=[&#39;user%s&#39;%usernum] return(user_df) def last(numd,numuser): lastday = pd.DataFrame({&#39;0&#39;:[0]*(numd+1)}) for numuser in range(1,numuser+1): lastday[&#39;user%s&#39;%numuser] = upgrade(numd,numuser) lastdayreal=lastday.iloc[:,1:1000] return(lastdayreal) . last(62,100) . user1 user2 user3 user4 user5 user6 user7 user8 user9 user10 ... user91 user92 user93 user94 user95 user96 user97 user98 user99 user100 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | . 2 0 | 2 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | ... | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | . 3 0 | 2 | 2 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | ... | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | . 4 0 | 2 | 2 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | ... | 1 | 2 | 0 | 1 | 0 | 0 | 0 | 0 | 2 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 58 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 59 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 60 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 61 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 62 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 63 rows × 100 columns . 1 62&#51068;&#52264;&#50640; &#44053;&#54868;&#47484; &#49457;&#44277;&#54620; &#49324;&#46988;&#51032; &#49688; &#44228;&#49328; (&#54665;&#51032; &#47560;&#51648;&#47561;&#51032; bool&#54805;&#51012; &#45908;&#54616;&#50668; &#54869;&#51064;) . sum(last(62,100).iloc[-1,:] == 5) . 100 . . . 2 -(1),(2) . import pandas as pd import numpy as np import random as rd success = [1,1,1,1,1,1,1,0,0,0] def upgrade(day,usernum): user = [0] countzero = [0] while len(user) &lt; day+1: if (user[-1] == 5): user.append(5) elif (rd.choice(success) == 0): user.append(user[-1]) if (user[-1] == 0): continue else: countzero[0] = countzero[0] + 1 else: user.append(user[-1]+1) if countzero[0] == 2: user[-1] = 0 countzero[0] = 0 else: continue user_df = pd.DataFrame(user) user_df.columns=[&#39;user%s&#39;%usernum] return(user_df) def last(numd,numuser): lastday = pd.DataFrame({&#39;0&#39;:[0]*(numd+1)}) for numuser in range(1,numuser+1): lastday[&#39;user%s&#39;%numuser] = upgrade(numd,numuser) lastdayreal=lastday.iloc[:,1:1000] return(lastdayreal) . last(62,100) . user1 user2 user3 user4 user5 user6 user7 user8 user9 user10 ... user91 user92 user93 user94 user95 user96 user97 user98 user99 user100 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 1 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | . 2 2 | 1 | 1 | 1 | 2 | 1 | 2 | 1 | 1 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 1 | 1 | 2 | 2 | 0 | . 3 3 | 1 | 0 | 2 | 2 | 2 | 2 | 0 | 2 | 3 | ... | 3 | 3 | 3 | 2 | 3 | 1 | 2 | 3 | 3 | 0 | . 4 3 | 0 | 1 | 3 | 3 | 3 | 3 | 0 | 3 | 4 | ... | 4 | 4 | 4 | 3 | 3 | 2 | 3 | 4 | 4 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 58 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 59 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 60 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 61 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 62 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 63 rows × 100 columns . 2 . (1) 이 경우 62일의 방학뒤에 100명의 유저중 대략 몇명정도 +5 강화상태에 있겠는가? 시뮬레이션을 활용하여 추론하라. (단, +5강화에 성공하지 못한 모 든 유저는 반드시 하루에 한번 강화를 시도해야 한다고 가정하자.) . last 에 numd = 62 , numuser = 100 대입 . sum(last(62,100).iloc[-1,:] == 5) . 100 . . . (2) 31번째 시도 이후 대략 몇명의 유저가 +5 강화상태에 있겠는가? . last 에 numd = 31 , numuser = 100 대입 . sum(last(31,100).iloc[-1,:] == 5) . 97 . . . 2 (3) . import pandas as pd import numpy as np import random as rd success = [1,1,1,1,1,1,1,0,0,0] def upgrade(day,usernum): user = [0] countzero = [0] while len(user) &lt; day+1: if (user[-1] == 5): user.append(5) elif (rd.choice(success) == 0): user.append(user[-1]) if (user[-1] == 0): continue else: countzero[0] = countzero[0] + 1 else: user.append(user[-1]+1) if countzero[0] == 2: user[-1] = 0 countzero[0] = 0 else: continue user_df = pd.DataFrame(user) user_df.columns=[&#39;user%s&#39;%usernum] return(user_df) def last(numd,numuser): lastday = pd.DataFrame({&#39;0&#39;:[0]*(numd+1)}) for numuser in range(1,numuser+1): lastday[&#39;user%s&#39;%numuser] = upgrade(numd,numuser) if sum(lastday.iloc[-1,1:1000] == 5) &gt;= 50: success = [1,1,1,1,1,1,1,1,1,0] lastdayreal=lastday.iloc[:,1:1000] return(lastdayreal) . last(62,100) . user1 user2 user3 user4 user5 user6 user7 user8 user9 user10 ... user91 user92 user93 user94 user95 user96 user97 user98 user99 user100 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 1 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | 0 | . 2 1 | 0 | 2 | 2 | 1 | 2 | 2 | 2 | 0 | 1 | ... | 1 | 1 | 2 | 0 | 1 | 2 | 2 | 2 | 1 | 0 | . 3 2 | 1 | 3 | 2 | 2 | 2 | 2 | 3 | 1 | 2 | ... | 2 | 2 | 3 | 1 | 0 | 3 | 3 | 2 | 0 | 1 | . 4 0 | 2 | 3 | 3 | 3 | 3 | 3 | 4 | 2 | 3 | ... | 3 | 3 | 4 | 2 | 1 | 3 | 4 | 3 | 1 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 58 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 59 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 60 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 61 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 62 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . 63 rows × 100 columns . 2_(3) . 100명의 유저중 50명이상의 유저가 +5 강화상태에 도달하는 순간 모든 유저의 강화성공확률을 90%로 증가시킨다고 하자. 62일의 방학뒤에 100명의 유저 중 몇명 정도가 +5 강화상태에 있겠는가? . last 에 numd = 62 , numuser = 100 대입 . sum(last(62,100).iloc[-1,:] == 5) . 100 .",
            "url": "https://cjfal.github.io/dj/2021/12/31/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9E%85%EB%AC%B8%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC%EB%AC%B8%EC%A0%9C1,2_201514142_%EA%B9%80%EB%8F%99%EC%A4%80.html",
            "relUrl": "/2021/12/31/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9E%85%EB%AC%B8%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC%EB%AC%B8%EC%A0%9C1,2_201514142_%EA%B9%80%EB%8F%99%EC%A4%80.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "데이터시각화",
            "content": "fig.add_axes() ## 액시즈를 fig에 추가하라. fig.axes ## 현재 fig에 있는 액시즈 정보 . &#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 import cv2 as cv from scipy import stats . &#50976;&#50857;&#54620; &#51105;&#44592;&#49696; . 코드 맨위 &quot; #collapse &quot; : 복잡한 코드가 주피터에 올라갈 때 인풋을 숨길 수 있음 #숨기기 . pd.concat([df1,df2]) : 데이터프레임 합치기 . ignore_index=True -&gt; 기존에 있던 인덱스를 무시해라 . ${0 sim9} + {0 sim9}$ (기존) -&gt; 0~19 (무시) . 코드 맨위 &quot; #hide &quot; : 인풋 아웃풋 둘다 숨기기 . pd.DataFrame({&#39;열이름지정1&#39;:y1(리스트나 함수),&#39;열이름지정2&#39;:[&#39;A&#39;]*len(y1)(리스트나 함수)}) . $ to$ 데이터 프레임 생성 . HTML(저장한플랏이름.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) . 생성된 플랏을 html로 바꾸어 블로그에서 읽을 수 있게 함, 패키지 필요, IPython.display의 HTML . 숫자형.round(n) #소수 n째자리에서 반올림 . np.random.seed(아무숫자) : 값이 안변하도록 시드설정 . 이미지 불러오기 . img = cv.imread(&#39;사진이름.확장자&#39;,흑백원하면 0(안쓰면 컬러) ) :wd에 있어야함 . np.corrcoef([x,y]) 상관계수 구하기 . k=np.linspace(-2,2,9) #-2 부터 2까지 9개의 등분 k . array([-2. , -1.5, -1. , -0.5, 0. , 0.5, 1. , 1.5, 2. ]) . range(n) #n 까지의 자연수 출력 . &#50937;&#49324;&#51060;&#53944;&#50640; &#44277;&#44060;&#46108; csv &#48520;&#47084;&#50724;&#44592; . pd.read_csv(&#39;웹주소.csv&#39;) . - 깃허브 저장소에 아예 데이터만 따로 모아서 관리하는 것도 좋은 방법입니다. . . &#50668;&#47084;&#44536;&#47548;&#51012; &#44536;&#47532;&#44592; . &#46972;&#51064;&#54540;&#46991;&#51012; &#44536;&#47532;&#45716; &#48169;&#48277; . import matplotlib.pyplot as plt x=[1,2,3,4] y=[1,2,4,3] plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7f7a165e8130&gt;] . &#49328;&#51216;&#46020; &#44536;&#47532;&#44592; (&#50741;&#49496;&#47564; &#48148;&#44988;&#44163;) . import matplotlib.pyplot as plt x=[1,2,3,4] y=[1,2,4,3] plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe22e3e5d90&gt;] . (1) &#44217;&#52432;&#44536;&#47532;&#44592; . x=[1,2,3,4] y=[1,2,4,3] x=np.arange(-5,5,0.1) y=2*x+np.random.normal(loc=0,scale=1,size=100) plt.plot(x,y,&#39;.b&#39;) plt.plot(x,2*x,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7a1439fa60&gt;] . (2) &#46384;&#47196;&#44536;&#47532;&#44592; - subplots . x=[1,2,3,4] y=[1,2,4,3] _, axs = plt.subplots(2,2) # 2x2의 서브플랏을 만들겠다. 따로그리기의 기본 # 리턴값이 (fig,axs) 임 그래서 fig자리는 어느것이든 상관이 없어서 아무것도 안쓰려고 _ 를 쓴것 axs[0,0].plot(x,y,&#39;o:r&#39;) axs[0,1].plot(x,y,&#39;Xb&#39;) axs[1,0].plot(x,y,&#39;xm&#39;) axs[1,1].plot(x,y,&#39;.--k&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fe22d944df0&gt;] . . lambda . lambda &#49324;&#50857;&#48277; . f = lambda x,y,z : x+y+z ## lambda 입력:출력 #이자체가 오브젝트로 취급된다. . f(2,3,4) . 9 . lambda &#46356;&#54260;&#53944;&#51077;&#47141;&#44050; . x= (lambda a=&#39;fee&#39;,b=&#39;fie&#39;,c=&#39;foe&#39;: a+b+c) . x(&#39;wee&#39;) . &#39;weefiefoe&#39; . lambda&#51032; &#47532;&#49828;&#53944;&#54868; . l = [lambda x: x**2, lambda x: x**3, lambda x: x**4] #리스트안에 람다 3개 . for f in l: print(f(2)) . 4 8 16 . lambda&#51032; &#46357;&#49492;&#45320;&#47532;&#54868; . dct={&#39;f1&#39;: (lambda x: x+1), &#39;f2&#39;: (lambda x: x+22), &#39;f3&#39;: (lambda x: x+333)} . dct[&#39;f1&#39;](1), dct[&#39;f2&#39;](1), dct[&#39;f3&#39;](1) . (2, 23, 334) . lambda&#51032; &#51312;&#44148;&#48512; &#52636;&#47141; . lower = lambda x,y : x if x&lt;y else y . lower(&#39;a&#39;,&#39;b&#39;) . &#39;a&#39; . lower(&#39;c&#39;,&#39;b&#39;) . &#39;b&#39; . lambda expression &#51012; return &#51077;&#47141;&#44032;&#45733; . def action(x): return (lambda y: x+y) #리턴을 람다로 . act = action(99) ## act를 99+y를 수행하는 함수로 저장 act2 = action(98) ## act2를 98+y를 수행하는 함수로 저장 . action은 마치 함수를 만드는 함수같다.. | . print(act(2)) # act안에 y에 2를 넣겠다. 99+2 print(act2(2)) # act2안에 y에 2를 넣겠다. 98+2 . 101 100 . &#50696;&#51228;6&#51032; &#48156;&#51204; . action = lambda x: (lambda y: x+y) # 람다 출력에 또 람다 . act= action(99) #x에 99를 넣은 함수 act 선언 act2=action(98) #x에 98를 넣은 함수 act2 선언 . print(act(2)) print(act2(2)) . 101 100 . 괄호를 생략하여 선언하면 . action = lambda x: lambda y: x+y act= action(99) act2=action(98) print(act(2)) print(act2(2)) # 똑같다. . 101 100 . . map . map? #map(func, *iterables) --&gt; map 개체 #다음 인수를 사용하여 함수를 계산하는 반복자를 만듭니다. #각 반복 사항 최단 시간이 소진되면 중지됩니다. . Init signature: map(self, /, *args, **kwargs) Docstring: map(func, *iterables) --&gt; map object Make an iterator that computes the function using arguments from each of the iterables. Stops when the shortest iterable is exhausted. Type: type Subclasses: . map &#49324;&#50857;&#48169;&#48277; . def inc(x): return x+1 #임시로 쓸건데 공간도 차지하고 좀 그럼 $ to$ 람다로 처리 . list(map(inc,[1,2,3,4])) . [2, 3, 4, 5] . &#50696;&#51228;1&#51032; &#48320;&#54805;(&#46988;&#45796;&#49324;&#50857;) . list(map(lambda x: x+1,[1,2,3,4])) . [2, 3, 4, 5] . list(map(def inc(x): return x+1,[1,2,3,4])) #안됨 . File &#34;&lt;ipython-input-69-e70d258d54b1&gt;&#34;, line 1 list(map(def inc(x): return x+1,[1,2,3,4])) #안됨 ^ SyntaxError: invalid syntax . 함수명을 쓰는 자리에 lambda로 표현한 오브젝트 자체를 전달할 수 있다. $ to$ 코드가 간단하다. | . map&#44284; &#47532;&#49828;&#53944;&#52980;&#54532;&#47532;&#54760;&#49496; &#48708;&#44368; . (함수선언) . f = lambda x: &#39;X&#39; in x . f(&#39;X1&#39;),f(&#39;X2&#39;),f(&#39;Y1&#39;),f(&#39;Y2&#39;) . (True, True, False, False) . (map) . list(map(f,[&#39;X1&#39;,&#39;X2&#39;,&#39;Y3&#39;,&#39;Y4&#39;])) . [True, True, False, False] . (리스트컴프리헨션과 비교) . [f(x) for x in [&#39;X1&#39;,&#39;X2&#39;,&#39;Y3&#39;,&#39;Y4&#39;]] . [True, True, False, False] . &#46160;&#44060;&#51032; &#51077;&#47141;&#51012; &#48155;&#45716; &#54632;&#49688;(pow) map, &#47532;&#49828;&#53944;&#52980;&#54532;&#47532;&#54760;&#49496; &#48708;&#44368; . (함수소개) . pow(2,4) #2를 4제곱 . 16 . (map) . list(map(pow,[2,2,2,3,3,3],[0,1,2,0,1,2])) . [1, 2, 4, 1, 3, 9] . (리스트컴프리헨션과 비교) . [pow(x,y) for x,y in zip([2,2,2,3,3,3],[0,1,2,0,1,2])] #zip이라는 새로운 오브젝트 생성 . [1, 2, 4, 1, 3, 9] . map은 (하나의 함수,다양한 입력)인 경우 사용가능 . l=[lambda x: x+1, lambda x: x+2, lambda x: x+3 ] . list(map(l,[100,200,300])) . TypeError Traceback (most recent call last) &lt;ipython-input-78-dcc049c06067&gt; in &lt;module&gt; -&gt; 1 list(map(l,[100,200,300])) TypeError: &#39;list&#39; object is not callable . 리스트컴프리헨션은 (다양한함수,다양한입력)이 가능함 . [l[i](x) for i,x in zip([0,1,2],[100,200,300])] . [101, 202, 303] . 리스트컴프리헨션은 (다양한함수,다양한입력)이 가능함 . [l[i](x) for i,x in zip([0,1,2],[100,200,300])] . [101, 202, 303] . 종합:map을 리스트컴프리헨션과 비교 . (1) 반복인덱스를 쓰지 않는 장점 . (2) 좀 더 제약적으로 사용할 수 밖에 없다는 단점 . &#50529;&#49884;&#51592;&#47484; &#51060;&#50857;&#54616;&#50668; 2$ times$2 &#49436;&#48652;&#54540;&#46991; &#44536;&#47532;&#44592; . fig = plt.figure() fig.axes . [] . &lt;Figure size 432x288 with 0 Axes&gt; . fig.subplots(2,2) fig.axes . [&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;] . ax1,ax2,ax3,ax4=fig.axes . ax1.plot([1,2,3],&#39;ob&#39;) ax2.plot([1,2,3],&#39;or&#39;) ax3.plot([1,2,3],&#39;ok&#39;) ax4.plot([1,2,3],&#39;oy&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff0fb177430&gt;] . fig . - 단계적으로 코드를 실행하고 싶을때 . x=[1,2,3,4] y=[1,2,4,3] . _, axs = plt.subplots(2,2) . axs[0,0].plot(x,y,&#39;o:r&#39;) axs[0,1].plot(x,y,&#39;Xb&#39;) axs[1,0].plot(x,y,&#39;xm&#39;) axs[1,1].plot(x,y,&#39;.--k&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff0faaa0940&gt;] . 어? 그림을 볼려면 어떻게 하지? | . _ . plt.subplots()&#47484; 2$ times$2 subplot &#44536;&#47532;&#44592; -- &#50529;&#49884;&#51592;&#47484; &#44033;&#44033; &#48320;&#49688;&#47749;&#51004;&#47196; &#51200;&#51109; . x=[1,2,3,4] y=[1,2,4,3] fig, axs = plt.subplots(2,2) . (ax1,ax2), (ax3,ax4) = axs . ax1.plot(x,y,&#39;o:r&#39;) ax2.plot(x,y,&#39;Xb&#39;) ax3.plot(x,y,&#39;xm&#39;) ax4.plot(x,y,&#39;.--k&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff0fb3650d0&gt;] . fig . &#51228;&#47785;&#49444;&#51221; . &#50696;&#51228;1: plt.plot() . x=[1,2,3] y=[1,2,2] . plt.plot(x,y) plt.title(&#39;title&#39;) . Text(0.5, 1.0, &#39;title&#39;) . &#50696;&#51228;2: &#50529;&#49884;&#51592;&#47484; &#51060;&#50857; . fig = plt.figure() fig.subplots() . &lt;AxesSubplot:&gt; . ax1=fig.axes[0] . ax1.set_title(&#39;title&#39;) . Text(0.5, 1.0, &#39;title&#39;) . fig . - 문법을 잘 이해했으면 각 서브플랏의 제목을 설정하는 방법도 쉽게 알 수 있다. . &#50696;&#51228;3: subplot&#50640;&#49436; &#44033;&#44033;&#51032; &#51228;&#47785;&#49444;&#51221; $ star$ . fig, ax = plt.subplots(2,2) . (ax1,ax2),(ax3,ax4) =ax . ax1.set_title(&#39;title1&#39;) ax2.set_title(&#39;title2&#39;) ax3.set_title(&#39;title3&#39;) ax4.set_title(&#39;title4&#39;) . Text(0.5, 1.0, &#39;title4&#39;) . fig #뭔가 엉성함 . - 보기싫음 $ to$ 서브플랏의 레이아웃 재정렬 . fig.tight_layout() # 외우세요.. fig #깔끔해짐 . &#50696;&#51228;4: &#50529;&#49884;&#51592;&#51032; &#51228;&#47785; + Figure&#51228;&#47785; . fig.suptitle(&#39;sup title&#39;) . Text(0.5, 0.98, &#39;sup title&#39;) . fig . fig.tight_layout() . fig . &#52629;&#48276;&#50948; &#49444;&#51221; . plt.xlim(-1,5) # plt 이용 plt.ylim(3,7) ax1.set_xlim(-10,110) #axs 이용 ax1.set_ylim(-5,5) . .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/12/31/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%9E%A1%EA%B8%B0%EC%88%A0.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/12/31/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%9E%A1%EA%B8%B0%EC%88%A0.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Title",
            "content": "library(MASS) . install.packages(&quot;ISLR2&quot;) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . install.packages(&quot;tree&quot;) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . library(ISLR2) . Attaching package: ‘ISLR2’ The following object is masked from ‘package:MASS’: Boston . library(tree) . . High &lt;- factor(ifelse(Boston$medv &lt; 20 , &quot;low&quot;, &quot;high&quot;)) Boston[,&quot;medv1&quot;] &lt;- data.frame(High) . . attach(Boston) . tree.medv1 &lt;- tree(medv1 ~.-medv,Boston) plot(tree.medv1) text(tree.medv1, pretty = 0) summary(tree.medv1) . Classification tree: tree(formula = medv1 ~ . - medv, data = Boston) Variables actually used in tree construction: [1] &#34;lstat&#34; &#34;rm&#34; &#34;dis&#34; &#34;nox&#34; &#34;indus&#34; &#34;tax&#34; &#34;rad&#34; &#34;age&#34; Number of terminal nodes: 17 Residual mean deviance: 0.3498 = 171.1 / 489 Misclassification error rate: 0.07905 = 40 / 506 . 가장 중요한 변수는 lstat 인것으로 보인다. . set.seed(201514142) cv.medv1 &lt;- cv.tree(tree.medv1, FUN = prune.misclass) names(cv.medv1) cv.medv1 . &lt;ol class=list-inline&gt;&#39;size&#39; | &#39;dev&#39; | &#39;k&#39; | &#39;method&#39; | &lt;/ol&gt; $size [1] 17 13 9 7 2 1 $dev [1] 94 94 91 91 89 210 $k [1] -Inf 0 2 3 4 136 $method [1] &#34;misclass&#34; attr(,&#34;class&#34;) [1] &#34;prune&#34; &#34;tree.sequence&#34; . length(Boston$medv1) . 506 set.seed(201514142) train &lt;- sample(1:nrow(Boston), 250) Boston.test &lt;- Boston[-train, ] medv1.test &lt;- medv1[-train] tree.Boston &lt;- tree(medv1 ~ . - medv, Boston, subset = train) . tree.pred &lt;- predict(tree.Boston, Boston.test, type = &quot;class&quot;) table(tree.pred, medv1.test) . medv1.test tree.pred high low high 122 17 low 25 92 . (122+92)/250 . 0.856 85.6% 의 테스트 데이터의 정확도를 보인다. . 9노드의 트리를 얻기위한 함수 코드 . prune.medv1 &lt;- prune.misclass(tree.medv1, best = 9) plot(prune.medv1) text(prune.medv1, pretty = 0) . tree.pred1 &lt;- predict(prune.medv1, Boston.test, type = &quot;class&quot;) table(tree.pred1, medv1.test) . medv1.test tree.pred1 high low high 133 12 low 14 97 . (133+97) / 250 . 0.92 92% 로 정확성 향상 . . set.seed(201514142) train &lt;- sample(1:nrow(Boston), nrow(Boston) / 2) tree.boston &lt;- tree(medv1 ~ .-medv, Boston, subset = train) summary(tree.boston) . Classification tree: tree(formula = medv1 ~ . - medv, data = Boston, subset = train) Variables actually used in tree construction: [1] &#34;lstat&#34; &#34;rm&#34; &#34;nox&#34; &#34;dis&#34; &#34;ptratio&#34; &#34;age&#34; Number of terminal nodes: 13 Residual mean deviance: 0.2084 = 50.02 / 240 Misclassification error rate: 0.05138 = 13 / 253 . 트리를 구성하는 데 6개의 변수만 사용되었다. . plot(tree.boston) text(tree.boston, pretty = 0) . 가지치기로 성능을 향상 . prune.boston &lt;- prune.tree(tree.boston, best = 5) plot(prune.boston) text(prune.boston, pretty = 0) . . install.packages(&quot;gbm&quot;) library(gbm) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) Loaded gbm 2.1.8 . set.seed(1) boost.boston &lt;- gbm(medv1 ~ . -medv, data = Boston[train, ], distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4) . summary(boost.boston) . A data.frame: 12 × 2 varrel.inf . &lt;chr&gt;&lt;dbl&gt; . lstatlstat | 44.0647732 | . rmrm | 14.1350606 | . ageage | 8.0966246 | . taxtax | 6.6639979 | . crimcrim | 6.4895353 | . noxnox | 5.9589193 | . disdis | 5.9119778 | . ptratioptratio | 3.9765543 | . indusindus | 3.3378129 | . radrad | 0.7925498 | . chaschas | 0.4191571 | . znzn | 0.1530372 | . plot(boost.boston, i = &quot;rm&quot;) . plot(boost.boston, i = &quot;lstat&quot;) . plot(boost.boston, i = &quot;age&quot;) . yhat.boost &lt;- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000) . plot(yhat.boost) . boost.boston &lt;- gbm(medv1 ~ .-medv, data = Boston[train, ], distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F) yhat.boost &lt;- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000) . Classification Trees에서 노드를 9개로 한 나무모형이 92%의 정확도로 성능이 가장 좋은것으로 예측된다. .",
            "url": "https://cjfal.github.io/dj/2021/12/31/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5.html",
            "relUrl": "/2021/12/31/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "회귀분석 10장 연습문제R",
            "content": "10.2) . 예제 10.2의 가정된 모형에서 교호작용 효과에 대한 검정결과 유의하지 않게 판정되었다. 교호작용이 없는 모형 . $y_j= beta_0+ beta_1x_{1j}+ beta_2x_{2j}+ epsilon_j$ . 를 가정하고 분산분석표를 작성하여 예의 결과와 비교하여라 . &#50696;&#51228; 10.2) . 다음 자료는 새로운 제조방법$(x_2)$ A, B에 의하여 제조된 벨트 16개에 충격$(x_1)$을 가한 후 수명을 조사한 것이다. . 일련번호 $y$(수명) $x_1$(충격) $x_2$(제조방법) . 1 | 18 | 61 | A | . 2 | 17.4 | 72 | A | . 3 | 14.5 | 85 | A | . 4 | 14 | 84 | A | . 5 | 13.4 | 98 | A | . 6 | 24.4 | 53 | A | . 7 | 22.7 | 54 | A | . 8 | 12.7 | 89 | A | . 9 | 27.1 | 77 | B | . 10 | 25.4 | 88 | B | . 11 | 33.5 | 76 | B | . 12 | 35.6 | 59 | B | . 13 | 26.1 | 91 | B | . 14 | 36.8 | 65 | B | . 15 | 34.9 | 81 | B | . 16 | 43.6 | 51 | B | . EX102 &lt;- read.csv(&quot;10dot2.csv&quot;,header=T) . EX102[,&quot;manu1&quot;] &lt;- c(0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1) . head(EX102) . A data.frame: 6 × 5 numlifeworkmanumanu1 . &lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;chr&gt;&lt;dbl&gt; . 11 | 18.0 | 61 | A | 0 | . 22 | 17.4 | 72 | A | 0 | . 33 | 14.5 | 85 | A | 0 | . 44 | 14.0 | 84 | A | 0 | . 55 | 13.4 | 98 | A | 0 | . 66 | 24.4 | 53 | A | 0 | . attach(EX102) . gyo &lt;- work*manu1 gyo . &lt;ol class=list-inline&gt;0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 77 | 88 | 76 | 59 | 91 | 65 | 81 | 51 | &lt;/ol&gt; fit1 &lt;- lm(life~work+manu1) . fitgyo &lt;- lm(life~work+manu1+gyo) . summary(fitgyo) . Call: lm(formula = life ~ work + manu1 + gyo) Residuals: Min 1Q Median 3Q Max -4.3938 -1.1350 0.0171 1.6701 4.9848 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 35.16364 4.31882 8.142 3.14e-06 *** work -0.24196 0.05669 -4.268 0.00109 ** manu1 26.71723 6.71235 3.980 0.00183 ** gyo -0.15268 0.08916 -1.712 0.11252 Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 2.557 on 12 degrees of freedom Multiple R-squared: 0.9441, Adjusted R-squared: 0.9301 F-statistic: 67.56 on 3 and 12 DF, p-value: 8.73e-08 . anova(fitgyo) . A anova: 4 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . work 1 | 353.81512 | 353.815123 | 54.132838 | 8.764291e-06 | . manu1 1 | 951.69664 | 951.696636 | 145.607229 | 4.545839e-08 | . gyo 1 | 19.16497 | 19.164974 | 2.932194 | 1.125250e-01 | . Residuals12 | 78.43264 | 6.536053 | NA | NA | . 연습문제와의 비교 . summary(fit1) . Call: lm(formula = life ~ work + manu1) Residuals: Min 1Q Median 3Q Max -4.7121 -1.5151 -0.1434 1.3539 4.3026 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 39.76136 3.62542 10.967 6.10e-08 *** work -0.30368 0.04689 -6.476 2.08e-05 *** manu1 15.43382 1.37079 11.259 4.47e-08 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 2.74 on 13 degrees of freedom Multiple R-squared: 0.9304, Adjusted R-squared: 0.9197 F-statistic: 86.95 on 2 and 13 DF, p-value: 2.987e-08 . anova(fit1) . A anova: 3 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . work 1 | 353.81512 | 353.815123 | 47.12817 | 1.144082e-05 | . manu1 1 | 951.69664 | 951.696636 | 126.76597 | 4.471157e-08 | . Residuals13 | 97.59762 | 7.507509 | NA | NA | . 연습문제에서 적합한 모형은 . $ hat{y} = 39.76136 -0.30368x_1 + 15.43382x_2$ 이다. . 예제의 모형에서 교호작용효과를 무시할 수 있다고 결론 내렸으므로 연습문제의 적합이 적절하다. . . 10.4) . 다음 자료에 대하여 . x y . 2.5 | 65 | . 4.4 | 34 | . 4.5 | 40 | . 1.4 | 80 | . 4.7 | 30 | . 3.5 | 57 | . 2.5 | 72 | . 3.8 | 48 | . 1) y&#51032; x&#50640; &#45824;&#54620; &#49328;&#51216;&#46020;&#47484; &#44536;&#47140;&#48372;&#44256; &#47751; &#44060;&#51032; &#44396;&#44036;&#51004;&#47196; &#45208;&#45572;&#50612; &#48516;&#49437;&#54616;&#47732; &#51339;&#51008;&#51648;&#47484; &#49444;&#47749;&#54616;&#44256; &#50612;&#46500; &#51216;$(x_p)$&#50640;&#49436; &#45208;&#45572;&#45716; &#44163;&#51060; &#51339;&#51008;&#51648;&#47484; &#49444;&#47749;&#54616;&#50668;&#46972;. . EX104 &lt;- read.csv(&quot;104.csv&quot;,header=T) attach(EX104) . plot(x4,y4,col=&quot;red&quot;) . $x_p = 3.8$ 에서 나누는것이 좋아보인다. . 2) &#50948; &#51088;&#47308;&#47484; &#46160; &#44396;&#44036;&#51004;&#47196;&#47564; &#45208;&#45572;&#47140;&#44256; &#54620;&#45796;&#47732; &#50612;&#46500; &#51216;$(x_p)$&#50640;&#49436; &#45208;&#45572;&#45716; &#44163;&#51060; &#51339;&#51008;&#51648;&#47484; &#49444;&#47749;&#54616;&#44256; &#44536; &#51216;&#51012; &#44221;&#44228;&#47196; &#54616;&#50668; &#45796;&#51020; &#47784;&#54805;&#51012; &#51201;&#54633;&#49884;&#53020; &#48372;&#50500;&#46972;. . $y_j = beta_0 + beta_1x_{1j} + beta_2(x_{1j}-x_p)x_{2j}+ epsilon_j$ . $x_{2j} = 1, 만약 x_{1j} &gt; x_p 이면$ . $x_{2j} = 0, 만약 x_{1j} leq x_p 이면$ . x4_2 &lt;- function(x4) ifelse(x4&gt;3.8,1,0) . x4_2(x4) . &lt;ol class=list-inline&gt;0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | &lt;/ol&gt; EX104[,&quot;x4_&quot;] = (x4-3.8)*x4_2(x4) EX104$x4_ . &lt;ol class=list-inline&gt;0 | 0.6 | 0.7 | 0 | 0.9 | 0 | 0 | 0 | &lt;/ol&gt; fit104 &lt;- lm(y4~x4+x4_,data=EX104) . summary(fit104) . Call: lm(formula = y4 ~ x4 + x4_, data = EX104) Residuals: 1 2 3 4 5 6 7 8 -2.421 -3.663 4.531 -1.462 -1.082 2.344 4.579 -2.826 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 99.333 5.907 16.817 1.36e-05 *** x4 -12.765 2.046 -6.238 0.00155 ** x4_ -9.173 6.201 -1.479 0.19911 Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 3.939 on 5 degrees of freedom Multiple R-squared: 0.9668, Adjusted R-squared: 0.9535 F-statistic: 72.71 on 2 and 5 DF, p-value: 0.0002015 . $ hat{y} = 99.333 -12.765x_1 -9.173x_2$ 을 추정할 수 있다. . 3) &#44480;&#47924;&#44032;&#49444; $H_0 : beta_2 = 0$, &#45824;&#47549;&#44032;&#49444; $H_1 : beta_2 neq 0 $ &#47484; &#50976;&#51032;&#49688;&#51456; 0.05&#47196; &#44160;&#51221;&#54616;&#50668;&#46972;. . anova(fit104) . A anova: 3 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x41 | 2221.98174 | 2221.98174 | 143.230386 | 7.182332e-05 | . x4_1 | 33.95155 | 33.95155 | 2.188539 | 1.991072e-01 | . Residuals5 | 77.56670 | 15.51334 | NA | NA | . p-value 가 0에 근접하여 영가설을 기각한다. 즉, $ beta_2$ 는 0이아니라는 것을 알 수 있다. . . 10.5) . 다음의 자료는 모든 조건이 같은 60마리의 실험용 동물을 각각 30마리씩 두 그룹으로 나누어 한 그룹에 A, 또 다른 그룹에는 B의 안정제를 $5, 10, 20mg$ 씩 투약했을 때 관찰된 불안도$(y)$이다. $y, x_1,x_2$를 각각 불안도, 약의 복용량, 약의 종류라고 하고 다음 질문에 답하여라. . 5 10 20 . (A) | 15 | 16 | 18 | 16 | 20 | 17 | . (A) | 16 | 15 | 17 | 15 | 19 | 18 | . (A) | 18 | 16 | 18 | 19 | 21 | 21 | . (A) | 13 | 17 | 19 | 18 | 18 | 20 | . (A) | 19 | 15 | 20 | 16 | 19 | 17 | . (B) | 16 | 15 | 19 | 18 | 24 | 23 | . (B) | 17 | 15 | 21 | 20 | 25 | 24 | . (B) | 18 | 18 | 22 | 21 | 23 | 22 | . (B) | 17 | 17 | 23 | 22 | 25 | 26 | . (B) | 15 | 16 | 20 | 19 | 25 | 24 | . 1) &#51648;&#49884;&#48320;&#49688;&#47484; &#51060;&#50857;&#54616;&#50668; &#51088;&#47308;&#50640; &#45824;&#54620; &#51201;&#51208;&#54620; &#47784;&#54805;&#51012; &#49444;&#51221;&#54616;&#50668;&#46972;. . 4.24-0.013x . EX105 &lt;- read.csv(&quot;105.csv&quot;,header = T) . EX105 . A data.frame: 60 × 4 dosageunrestkindkindn . &lt;int&gt;&lt;int&gt;&lt;chr&gt;&lt;int&gt; . 5 | 15 | A | 0 | . 5 | 16 | A | 0 | . 5 | 16 | A | 0 | . 5 | 15 | A | 0 | . 5 | 18 | A | 0 | . 5 | 16 | A | 0 | . 5 | 13 | A | 0 | . 5 | 17 | A | 0 | . 5 | 19 | A | 0 | . 5 | 15 | A | 0 | . 10 | 18 | A | 0 | . 10 | 16 | A | 0 | . 10 | 17 | A | 0 | . 10 | 15 | A | 0 | . 10 | 18 | A | 0 | . 10 | 19 | A | 0 | . 10 | 19 | A | 0 | . 10 | 18 | A | 0 | . 10 | 20 | A | 0 | . 10 | 16 | A | 0 | . 20 | 20 | A | 0 | . 20 | 17 | A | 0 | . 20 | 19 | A | 0 | . 20 | 18 | A | 0 | . 20 | 21 | A | 0 | . 20 | 21 | A | 0 | . 20 | 18 | A | 0 | . 20 | 20 | A | 0 | . 20 | 19 | A | 0 | . 20 | 17 | A | 0 | . 5 | 16 | B | 1 | . 5 | 15 | B | 1 | . 5 | 17 | B | 1 | . 5 | 15 | B | 1 | . 5 | 18 | B | 1 | . 5 | 18 | B | 1 | . 5 | 17 | B | 1 | . 5 | 17 | B | 1 | . 5 | 15 | B | 1 | . 5 | 16 | B | 1 | . 10 | 21 | B | 1 | . 10 | 20 | B | 1 | . 10 | 22 | B | 1 | . 10 | 21 | B | 1 | . 10 | 23 | B | 1 | . 10 | 22 | B | 1 | . 10 | 20 | B | 1 | . 10 | 19 | B | 1 | . 10 | 24 | B | 1 | . 10 | 23 | B | 1 | . 20 | 25 | B | 1 | . 20 | 24 | B | 1 | . 20 | 23 | B | 1 | . 20 | 22 | B | 1 | . 20 | 25 | B | 1 | . 20 | 26 | B | 1 | . 20 | 25 | B | 1 | . 20 | 24 | B | 1 | . 20 | 24 | B | 1 | . 20 | 23 | B | 1 | . one &lt;- rep(1,length(EX105$kind)) . &lt;ol class=list-inline&gt;1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | &lt;/ol&gt; X105 &lt;- matrix(c(one,EX105$dosage,EX105$kindn),length(EX105$kind),3) . y105 &lt;- matrix(c(EX105$unrest),length(EX105$unrest),1) . solve(t(X105)%*%X105)%*%t(X105)%*%y105 . A matrix: 3 × 1 of type dbl 13.6333333 | . 0.3342857 | . 3.1333333 | . 2) (1)&#51032; &#47784;&#54805;&#51012; &#52572;&#49548;&#51228;&#44273;&#48277;&#50640; &#51032;&#54644; &#52628;&#51221;&#54616;&#44256; &#48516;&#49328;&#48516;&#49437;&#54364;&#47484; &#51089;&#49457;&#54616;&#50668;&#46972;. . fit105 &lt;- lm(EX105$unrest~EX105$dosage+EX105$kindn) . summary(fit105) . Call: lm(formula = EX105$unrest ~ EX105$dosage + EX105$kindn) Residuals: Min 1Q Median 3Q Max -3.4381 -1.3488 -0.1095 1.1548 3.8905 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 13.63333 0.57751 23.607 &lt; 2e-16 *** EX105$dosage 0.33429 0.03949 8.465 1.17e-11 *** EX105$kindn 3.13333 0.49251 6.362 3.65e-08 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1.907 on 57 degrees of freedom Multiple R-squared: 0.663, Adjusted R-squared: 0.6512 F-statistic: 56.07 on 2 and 57 DF, p-value: 3.447e-14 . anova(fit105) . A anova: 3 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . EX105$dosage 1 | 260.7429 | 260.742857 | 71.66357 | 1.165330e-11 | . EX105$kindn 1 | 147.2667 | 147.266667 | 40.47534 | 3.647996e-08 | . Residuals57 | 207.3905 | 3.638429 | NA | NA | . 3) &#51088;&#47308;&#50640; $y= beta_0+ beta_1x_1+ beta_2x_2+ epsilon$&#47484; &#51201;&#54633;&#49884;&#53412;&#44256; &#54924;&#44480;&#49440;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. . $ to b_0 = 13.63 , b_1 = 0.3343 , b_2 = 3.133 $ . 반응 함수는 $ hat{y} = 13.63 + 0.3343x_1 + 3.133x_2 $ 이다. . 이 결과는 많은양 복용시 불안도가 올라가고, B의 안정제의 불안도가 3정도 더 높다는 것을 보여준다. . 4) &#44368;&#54840;&#51089;&#50857;&#51060; &#51080;&#45716;&#51648; &#44160;&#51221;&#54616;&#50668;&#46972;. $ alpha = 0.05$ . EX105[,&quot;inter&quot;] &lt;- EX105$dosage * EX105$kindn . fit105int &lt;- lm(EX105$unrest~EX105$dosage+EX105$kindn+EX105$inter) . summary(fit105int) . Call: lm(formula = EX105$unrest ~ EX105$dosage + EX105$kindn + EX105$inter) Residuals: Min 1Q Median 3Q Max -3.2571 -1.2250 -0.1714 0.9357 4.1286 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 15.30000 0.65419 23.388 &lt; 2e-16 *** EX105$dosage 0.19143 0.04945 3.871 0.000286 *** EX105$kindn -0.20000 0.92516 -0.216 0.829634 EX105$inter 0.28571 0.06994 4.085 0.000142 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1.689 on 56 degrees of freedom Multiple R-squared: 0.7404, Adjusted R-squared: 0.7265 F-statistic: 53.23 on 3 and 56 DF, p-value: &lt; 2.2e-16 . $ to kindn$ 의 pvalue가 상당히 크게 측정되어 교호작용은 없는것으로 판단된다. . . 10.7) . 다음 자료는 어느 초등 학교에서 안경을 쓴 학생들을 학년별로 조사한 자료이다. . 학년별 1 2 3 4 5 6 . 전체학생수 | 250 | 252 | 151 | 204 | 202 | 195 | . 안경착용수 | 15 | 20 | 18 | 25 | 32 | 40 | . 1)&#47196;&#51648;&#49828;&#54001; &#48152;&#51025;&#54632;&#49688;&#47484; &#52628;&#51221;&#54616;&#50668;&#46972;. . EX107 &lt;- read.csv(&quot;107.csv&quot;,header=T) attach(EX107) . head(EX107) . A data.frame: 6 × 3 gradestudentglasses . &lt;int&gt;&lt;int&gt;&lt;int&gt; . 11 | 250 | 15 | . 22 | 252 | 20 | . 33 | 151 | 18 | . 44 | 204 | 25 | . 55 | 202 | 32 | . 66 | 195 | 40 | . EX107[,&quot;P&quot;] &lt;- glasses/student EX107 . A data.frame: 6 × 4 gradestudentglassesP . &lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;dbl&gt; . 1 | 250 | 15 | 0.06000000 | . 2 | 252 | 20 | 0.07936508 | . 3 | 151 | 18 | 0.11920530 | . 4 | 204 | 25 | 0.12254902 | . 5 | 202 | 32 | 0.15841584 | . 6 | 195 | 40 | 0.20512821 | . logit &lt;- function(x) log(x/(1-x)) . EX107[,&quot;Pstar&quot;] &lt;- logit(EX107$P) EX107 . A data.frame: 6 × 5 gradestudentglassesPPstar . &lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt; . 1 | 250 | 15 | 0.06000000 | -2.751535 | . 2 | 252 | 20 | 0.07936508 | -2.451005 | . 3 | 151 | 18 | 0.11920530 | -1.999977 | . 4 | 204 | 25 | 0.12254902 | -1.968510 | . 5 | 202 | 32 | 0.15841584 | -1.670063 | . 6 | 195 | 40 | 0.20512821 | -1.354546 | . temp = 0 for (i in 1:6){ temp = EX107$student[i]*EX107$P[i]*(1-EX107$P[i]) print(temp) } . [1] 14.1 [1] 18.4127 [1] 15.8543 [1] 21.93627 [1] 26.93069 [1] 31.79487 . a &lt;- c(14.1,0,0,0,0,0) b &lt;- c(0,18.4127,0,0,0,0) c &lt;- c(0,0,15.8543,0,0,0) d &lt;- c(0,0,0,21.93627,0,0) e &lt;- c(0,0,0,0,26.93069,0) f &lt;- c(0,0,0,0,0,31.79487) . W &lt;- matrix(c(a,b,c,d,e,f),6,6) W . A matrix: 6 × 6 of type dbl 14.1 | 0.0000 | 0.0000 | 0.00000 | 0.00000 | 0.00000 | . 0.0 | 18.4127 | 0.0000 | 0.00000 | 0.00000 | 0.00000 | . 0.0 | 0.0000 | 15.8543 | 0.00000 | 0.00000 | 0.00000 | . 0.0 | 0.0000 | 0.0000 | 21.93627 | 0.00000 | 0.00000 | . 0.0 | 0.0000 | 0.0000 | 0.00000 | 26.93069 | 0.00000 | . 0.0 | 0.0000 | 0.0000 | 0.00000 | 0.00000 | 31.79487 | . X &lt;- matrix(c(1,1,1,1,1,1,array(student)),6,2) X . A matrix: 6 × 2 of type dbl 1 | 250 | . 1 | 252 | . 1 | 151 | . 1 | 204 | . 1 | 202 | . 1 | 195 | . Xt &lt;- t(X) . pmat &lt;- matrix(c(array(EX107$Pstar)),6,1) pmat . A matrix: 6 × 1 of type dbl -2.751535 | . -2.451005 | . -1.999977 | . -1.968510 | . -1.670063 | . -1.354546 | . solve(Xt%*%W%*%X) %*% Xt %*%W %*% pmat . A matrix: 2 × 1 of type dbl 0.038849649 | . -0.009442624 | . $ beta_0 = 0.0388 , beta_1 = -0.00944 $ . $ hat{p^{*}} =0.0388 -0.00944student $ 이다. . 2)&#49352;&#47196; &#51204;&#54617;&#54644; &#50732; &#45348; &#54617;&#49373;&#51060; &#50504;&#44221;&#51012; &#52265;&#50857;&#54624; &#54869;&#47456;&#51008;? . 0.0388 -0.00944* 4 . 0.00104 3) $ hat{p}^{*}$&#47484; $x$&#50640; &#45824;&#54616;&#50668; &#46020;&#49884;&#54616;&#44256; &#47196;&#51648;&#49828;&#54001; &#48152;&#51025;&#54632;&#49688;&#47484; &#51201;&#54633;&#49884;&#53412;&#45716; &#44163;&#51060; &#53440;&#45817;&#54620;&#51648;&#47484; &#44160;&#53664;&#54616;&#50668;&#46972;. . plot(EX107$Pstar,EX107$student) . 타당하지 않다. .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/11/22/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D10%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/11/22/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D10%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "date": " • Nov 22, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "11주차 회귀분석 연습문제 8장 R 풀이",
            "content": "8.1) . 어떤 철 제품을 생산하는 공장에서 제품의 인장강도 ($kg/cm^2$)가 온도와 시간에 어떤영향을 받는가를 분석하기 위하여 다음과 같은 자료를 얻었다. . 공정온도:$x_1$ 시간(분):$x_2$ 인장강도:y . 175 | 15 | 120.4 | . 210 | 18 | 112.5 | . 192 | 20 | 95.4 | . 250 | 28 | 162.3 | . 245 | 25 | 160.2 | . 226 | 21 | 131.5 | . 260 | 32 | 157.6 | . 230 | 25 | 158.4 | . 205 | 24 | 149.6 | . 185 | 19 | 130.4 | . 1)&#49440;&#54805;&#54924;&#44480;&#47784;&#54805; $y= beta_0+ beta_1x_1+ beta_2x_2+ epsilon$ &#51060; &#49457;&#47549;&#54620;&#45796;&#44256; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#47784;&#54805;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. . fac &lt;- read.csv(&quot;8-1csv.csv&quot;) fac . A data.frame: 10 × 3 x1x2y_ . &lt;int&gt;&lt;int&gt;&lt;dbl&gt; . 175 | 15 | 120.4 | . 210 | 18 | 112.5 | . 192 | 20 | 95.4 | . 250 | 28 | 162.3 | . 245 | 25 | 160.2 | . 226 | 21 | 131.5 | . 260 | 32 | 157.6 | . 230 | 25 | 158.4 | . 205 | 24 | 149.6 | . 185 | 19 | 130.4 | . attach(fac) . lm81 &lt;- lm(y_~x1+x2,data=fac) . coef(lm81) . &lt;dl class=dl-inline&gt;(Intercept)35.3358543470464x10.204751085230047x22.55063256783478&lt;/dl&gt; 추정된 y는 $y=35.3359+ 0.2048x_1 + 2.5506x_2 $ 이다. . 2) &#52628;&#51221;&#46108; &#54924;&#44480;&#44228;&#49688;&#46308;&#50640; &#45824;&#54616;&#50668; $Var(b_0),Var(b_1),Var(b_2)$ &#47484; &#44396;&#54616;&#50668;&#46972;. . X &lt;- matrix(c(1,1,1,1,1,1,1,1,1,1,array(fac$x1),array(fac$x2)),10,3) beta &lt;- matrix(c(35.3359,0.2048,2.5506),3,1) . XtX &lt;- t(X) %*% X #[X&#39;X] XtX . A matrix: 3 × 3 of type dbl 10 | 2178 | 227 | . 2178 | 481940 | 50621 | . 227 | 50621 | 5385 | . XtXr &lt;- solve(XtX) # [X&#39;X]의 역함수 XtXr . A matrix: 3 × 3 of type dbl 8.99974754 | -0.065260222 | 0.234093777 | . -0.06526022 | 0.000637595 | -0.003242642 | . 0.23409378 | -0.003242642 | 0.020799716 | . $Var(b_0)$ : XtXr 의 대각 1번째 원소 $ times$ $ sigma^2$ . $8.9997 sigma^2$ . $Var(b_1)$ :XtXr 의 대각 2번째 원소 $ times$ $ sigma^2$&gt; $0.00064 sigma^2$ . $Var(b_2)$ :XtXr 의 대각 3번째 원소 $ times$ $ sigma^2$&gt; $0.0208 sigma^2$ . 3) &#52628;&#51221;&#46108; &#54924;&#44480;&#44228;&#49688; $b_1$&#44284; $b_2$&#51032; &#51032;&#48120;&#45716; &#47924;&#50631;&#51064;&#44032;? . $b_1$ : $1^ circ C$ 온도 증가에 따른 인장강도의 변화 . $b_2$ : $1$분 시간 흐름에 따른 인장강도의 변화 . 4) &#54924;&#44480;&#44228;&#49688; $ beta_1$ &#44284; $ beta_2$&#51032; 95% &#49888;&#47280;&#44396;&#44036;&#51012; &#44396;&#54616;&#50668;&#46972;. . confint(lm81) . A matrix: 3 × 2 of type dbl 2.5 %97.5 % . (Intercept)-79.4362406 | 150.107949 | . x1 -0.7612855 | 1.170788 | . x2 -2.9669628 | 8.068228 | . $ -0.7612855 &lt; beta_1 &lt; 1.170788 $ . $ -2.9669628 &lt; beta_2 &lt; 8.068228 $ . 5) &#48516;&#49328;&#48516;&#49437;&#54364;&#47484; &#51089;&#49457;&#54616;&#44256; $H_0 : beta_1 = beta_2 = 0 $ &#51012; &#50976;&#51032;&#49688;&#51456; 0.05&#47196; &#44160;&#51221;&#54616;&#50668;&#46972;. . anova(lm81) . A anova: 3 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x11 | 2747.5393 | 2747.5393 | 10.496050 | 0.01425777 | . x21 | 312.7796 | 312.7796 | 1.194869 | 0.31053404 | . Residuals7 | 1832.3821 | 261.7689 | NA | NA | . 분산분석표 . 자유도 제곱합 제곱평균 F . model | 2 | 3060 | 1530 | 6 | . error | 7 | 1832 | 262 | | . total | 9 | 4892 | | | . 유의수준 0.05 에서 F의 값보다 분산분석에서의 Fvalue(6)가 크므로 영가설을 기각하여 . $ beta_1$ 과 $ beta_2$ 가 다르다는 것을 알 수 있다. . 6) $x_1 = 200, x_2 = 20 $ &#50640;&#49436; $E(y)$ &#51032; 95% &#50696;&#52769;&#44396;&#44036;&#51012; &#44396;&#54616;&#50668;&#46972;. . q11 &lt;- data.frame(x1 = 200 , x2 = 20) . predict(lm81, q11, level=0.95 ,interval = &quot;predic&quot;) . A matrix: 1 × 3 of type dbl fitlwrupr . 1127.2987 | 86.41531 | 168.1821 | . $ hat{y} pm t times sqrt{x&#39;(X&#39;X)^{-1}x bullet MSE} $ . $ hat{y} = 127.3 , t = 2.365 , MSE = 262 , $ . $ x&#39;(X&#39;X)^{-1}x : x = c(1, 200, 20) $ 으로 계산 $ to 0.142$ . x_ &lt;- matrix(c(1,200,20),3,1) xt_ &lt;- t(x_) c_ &lt;- (xt_) %*% (XtXr) %*% (x_) c_ . A matrix: 1 × 1 of type dbl 0.1419628 | . 127.3 + 2.365*sqrt(0.142*262) . 141.725336838355 127.3 - 2.365*sqrt(0.142*262) . 112.874663161645 따라서 예측구간은 112.9 ~ 141.7 이다. . 7) &#44480;&#47924;&#44032;&#49444; $H_0 : beta_1 = beta_2$, &#45824;&#47549;&#44032;&#49444; $H_1 : beta_1 neq beta_2$ &#47484; &#50976;&#51032;&#49688;&#51456; 5%&#47196; &#44160;&#51221;&#54616;&#50668;&#46972;. . q = matrix(c(0,1,-1),3,1) b = matrix(c(35.3359,0.2048,2.5506),3,1) qb = t(q) %*% b qb . A matrix: 1 × 1 of type dbl -2.3458 | . qb/sqrt(t(q) %*% XtXr %*% q *262) #검정 t통계량 . A matrix: 1 × 1 of type dbl -0.8672862 | . 0.87 은 2.365보다 작으므로 영가설을 기각할 수 없다. 즉 $ beta_1 = beta_2 $ 가 성립한다. . 8) $x_1 =240 , x_2 =30 $ &#50640;&#49436; y&#51032; 95% &#50696;&#52769;&#44396;&#44036;&#51012; &#44396;&#54616;&#50668;&#46972;. . q10 &lt;- data.frame(x1 = 240 , x2 = 30) . predict(lm81, q10, level=0.95 ,interval = &quot;predic&quot;) . A matrix: 1 × 3 of type dbl fitlwrupr . 1160.9951 | 114.5839 | 207.4063 | . 예측구간 : (114.5839 , 207.4063)의 사이에 y가 존재할 확률이 95%라는 뜻이다. . . 8.10) . 다음의 자료에 대하여 . $y$ $x_1$ $x_2$ . 39 | 2 | 4 | . 42 | 2 | 4 | . 51 | 3 | 4 | . 48 | 3 | 4 | . 53 | 2 | 6 | . 49 | 2 | 6 | . 61 | 3 | 6 | . 60 | 3 | 6 | . 1) &#51473;&#54924;&#44480;&#47784;&#54805; $y= beta_0+ beta_1x_1+ beta_2x_2+ epsilon$&#47484; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#49440;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. . Q810 &lt;- read.csv(&quot;810.csv&quot;,head = T) Q810 . A data.frame: 8 × 3 y_10x1_10x2_10 . &lt;int&gt;&lt;int&gt;&lt;int&gt; . 39 | 2 | 4 | . 42 | 2 | 4 | . 51 | 3 | 4 | . 48 | 3 | 4 | . 53 | 2 | 6 | . 49 | 2 | 6 | . 61 | 3 | 6 | . 60 | 3 | 6 | . attach(Q810) . lm810 &lt;- lm(y_10~x1_10+x2_10) . coef(lm810) . &lt;dl class=dl-inline&gt;(Intercept)0.375000000000071x1_109.24999999999999x2_105.37499999999999&lt;/dl&gt; $ hat{y} = 0.375 + 9.25x_1 +5.375x_2 $ . 2) &#46021;&#47549;&#48320;&#49688; $x_1$&#47564;&#51012; &#44256;&#47140;&#54620; &#54924;&#44480;&#47784;&#54805; $y= beta_0+ beta_1x_1+ epsilon$&#47484; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#49440;&#51012; &#44396;&#54616;&#50668; &#48516;&#49328;&#48516;&#49437;&#54364;&#47484; &#51089;&#49457;&#54616;&#50668;&#46972;. . lm810_1 &lt;- lm(y_10 ~ x1_10) coef(lm810_1) . &lt;dl class=dl-inline&gt;(Intercept)27.25x1_109.25&lt;/dl&gt; $ hat{y} = 27.25 + 9.25x_1 $ . ANOVA . anova(lm810_1) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x1_101 | 171.125 | 171.12500 | 4.127638 | 0.08846031 | . Residuals6 | 248.750 | 41.45833 | NA | NA | . 3) &#46021;&#47549;&#48320;&#49688; $x_2$&#47564;&#51012; &#44256;&#47140;&#54620; &#54924;&#44480;&#47784;&#54805; $y= beta_0+ beta_2x_2+ epsilon$&#47484; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#49440;&#51012; &#44396;&#54616;&#50668; &#48516;&#49328;&#48516;&#49437;&#54364;&#47484; &#51089;&#49457;&#54616;&#50668;&#46972;. . lm810_2 &lt;- lm(y_10 ~ x2_10) coef(lm810_2) . &lt;dl class=dl-inline&gt;(Intercept)23.5x2_105.375&lt;/dl&gt; $ hat{y} = 23.5 +5.375x_2 $ . ANOVA . anova(lm810_2) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x2_101 | 231.125 | 231.12500 | 7.34702 | 0.03508095 | . Residuals6 | 188.750 | 31.45833 | NA | NA | . 4) &#45800;&#50948;&#44600;&#51060;&#48277;&#50640; &#51032;&#54616;&#50668; &#54364;&#51456;&#54868;&#46108; &#54924;&#44480;&#44228;&#49688;&#47484; &#44396;&#54616;&#50668; &#54924;&#44480;&#49440;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. . - . - . - . - . - . - . - . - . - . - . - . $y=0.6384x_1 + 0.7419x_2$ 을 추정할 수 있다. . 5) &#54364;&#51456;&#54868;&#46108; &#54924;&#44480;&#47784;&#54805;&#50640; &#51032;&#54616;&#50668; &#48516;&#49328;&#48516;&#49437;&#54364;&#47484; &#51089;&#49457;&#54616;&#50668;&#46972;. . - . - . - . - . - . - . - . - . - . - . - . 6) 4)&#50640;&#49436; &#44228;&#49328;&#46108; &#54924;&#44480;&#44228;&#49688;&#46308;&#51032; &#49345;&#44288;&#54665;&#47148;&#51012; &#44228;&#49328;&#54616;&#44256; &#49345;&#44288;&#51060; &#51316;&#51116;&#54616;&#45716;&#51648;&#47484; &#49444;&#47749;&#54616;&#50668;&#46972;. . - . - . - . - . . 8.15) . 다음과 같은 자료에 대하여 중회귀모형 $y= beta_0+ beta_1x_1+ beta_2x_2+ epsilon$를 가정하고 아래의 질문에 답하여라. . $y$ $x_1$ $x_2$ . 11 | -5 | 5 | . 11 | 4 | 4 | . 8 | -1 | 1 | . 2 | 2 | -3 | . 5 | 2 | -2 | . 5 | 3 | -2 | . 4 | 3 | -3 | . 1) &#48516;&#49328;&#48516;&#49437;&#54364;&#47484; &#51089;&#49457;&#54616;&#44256; $H_0 : beta_1 = beta_2 = 0 $ &#51012; $ alpha = 0.05 $&#47196; &#44160;&#51221;&#54616;&#50668;&#46972;. . y15 &lt;- c(11,11,8,2,5,5,4) x1_15 &lt;- c(-5,4,-1,2,2,3,3) x2_15 &lt;- c(5,4,1,-3,-2,-2,-3) lm815 &lt;- lm(y15 ~ x1_15+x2_15) coef(lm815) . &lt;dl class=dl-inline&gt;(Intercept)6.43310529293761x1_150.121032868679595x2_151.07700221182038&lt;/dl&gt; $y=6.43310529293761 + 0.121032868679595x_1 + 1.07700221182038x_2$ . anova(lm815) . A anova: 3 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x1_151 | 15.879334 | 15.8793343 | 20.48100 | 0.010612931 | . x2_151 | 54.733671 | 54.7336707 | 70.59492 | 0.001098159 | . Residuals4 | 3.101281 | 0.7753202 | NA | NA | . Fvalue 가 6.94 보다 크므로 영가설을 기각하여 $ beta_1$ 과 $ beta_2$ 가 다르다는 것을 알 수 있다. . 2) $R( beta_0), R( beta_1, beta_2| beta_0),R( beta_1| beta_0),R( beta_2| beta_0)$ &#51012; &#44033;&#44033; &#44396;&#54616;&#50668;&#46972;. . $R( beta_1, beta_2| beta_0)$ = 15.879334 . anova(lm815) . A anova: 3 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x1_151 | 15.879334 | 15.8793343 | 20.48100 | 0.010612931 | . x2_151 | 54.733671 | 54.7336707 | 70.59492 | 0.001098159 | . Residuals4 | 3.101281 | 0.7753202 | NA | NA | . $R( beta_1| beta_0) $=15.87933 . lm815a = lm(y15~x1_15) anova(lm815a) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x1_151 | 15.87933 | 15.87933 | 1.372815 | 0.2941169 | . Residuals5 | 57.83495 | 11.56699 | NA | NA | . $R( beta_2| beta_0)$ =70.01471 . lm815b = lm(y15~x2_15) anova(lm815b) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x2_151 | 70.01471 | 70.014706 | 94.62521 | 0.0001951318 | . Residuals5 | 3.69958 | 0.739916 | NA | NA | . $R( beta_0)$ = $ n( bar{y})^2$ =302.285714285714 . 7*mean(y15)^2 . 302.285714285714 3) $H_0 : beta_1 = 0 $ &#51012; &#48512;&#48516; F&#44160;&#51221;&#51004;&#47196; &#44032;&#49444;&#44160;&#51221;&#54616;&#50668;&#46972;. $ alpha = 0.05 $ . anova(lm815b,lm815) . A anova: 2 × 6 Res.DfRSSDfSum of SqFPr(&gt;F) . &lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . 15 | 3.699580 | NA | NA | NA | NA | . 24 | 3.101281 | 1 | 0.598299 | 0.7716799 | 0.4292997 | . Fvalue 는 0.7717 이고, 7.71 보다 작으므로 영가설을 기각하지 않는다. 즉, $ beta_1 = 0$ 이 성립한다. . 4) $H_0 : beta_2 = 0 $ &#51012; &#48512;&#48516; F&#44160;&#51221;&#51004;&#47196; &#44032;&#49444;&#44160;&#51221;&#54616;&#50668;&#46972;. $ alpha = 0.05, alpha = 0.01 $ . anova(lm815a,lm815) . A anova: 2 × 6 Res.DfRSSDfSum of SqFPr(&gt;F) . &lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . 15 | 57.834951 | NA | NA | NA | NA | . 24 | 3.101281 | 1 | 54.73367 | 70.59492 | 0.001098159 | . Fvalue = 70.595로 측정된다. . 유의수준 0.05일때 F값 7.71 보다 F값이 크며, 유의수준 0.01일때 F값인 21.2 보다도 F값이 크다. . 즉, 영가설은 어느 유의수준에서도 기각되어 $ beta_2$ 가 0이 아니라는 것으로 볼 수 있다. . . 8.16) . 중회귀모형 $y= X beta + epsilon, epsilon ~ N(0,I)$ 에서 $X$는 $n times (k+1)$ 이다. . 1) $Q_1 = y&#39;[I-X(X&#39;X)^{-1}X&#39;]y$ &#51032; &#48516;&#54252;&#45716;? . - . - . - . - . - . 2) $Q_2 = y&#39;X(X&#39;X)^{-1}X&#39;y$ &#51032; &#48516;&#54252;&#45716;? . - . - . - . - . - . 3) $Q_1$&#44284;$Q_2$&#44032; &#49436;&#47196; &#46021;&#47549;&#51076;&#51012; &#48372;&#50668;&#46972;. . - . - . - . - . - . 4) $ beta = 0 $ &#51060;&#46972; &#54624; &#46412; . $ frac{Q_2/(k+1)}{Q_1/(n-k-1)}$ 의 분포는? . - . - . - . - . - .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/11/17/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D8%EC%9E%A5%ED%92%80%EC%9D%B4.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/11/17/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D8%EC%9E%A5%ED%92%80%EC%9D%B4.html",
            "date": " • Nov 17, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "데이터시각화 시험공부3",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 import cv2 as cv from scipy import stats . . query&#47484; &#51060;&#50857;&#54620; &#54665;&#51032; &#49440;&#53469; . np.random.seed(1) df=pd.DataFrame(np.random.normal(size=(15,4)),columns=list(&#39;ABCD&#39;)) df . A&gt;0 and B&lt;0 &#51064; &#54665;&#51012; &#49440;&#53469; . df.query(&#39;A&gt;0 &amp; B&lt;0&#39;) #방법 1 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . 11 0.050808 | -0.636996 | 0.190915 | 2.100255 | . df.query(&#39;A&gt;0 and B&lt;0&#39;) # 방법 2 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . 11 0.050808 | -0.636996 | 0.190915 | 2.100255 | . A&lt;B&lt;C &#51064; &#54665;&#51012; &#49440;&#53469; . df.query(&#39;A&lt;B&lt;C&#39;) # 방법 1 . A B C D . 9 -1.117310 | 0.234416 | 1.659802 | 0.742044 | . 13 -1.142518 | -0.349343 | -0.208894 | 0.586623 | . A&gt;mean(A) &#51064; &#54665;&#51012; &#49440;&#53469; . df.A.mean() # A의 평균 구하기 . -0.018839420539994597 . df.query(&#39;A&gt;-0.018839420539994597&#39;) #방법 1 진짜 평균을 넣어줌 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . 11 0.050808 | -0.636996 | 0.190915 | 2.100255 | . 12 0.120159 | 0.617203 | 0.300170 | -0.352250 | . 14 0.838983 | 0.931102 | 0.285587 | 0.885141 | . . meanA=df.A.mean() meanA . -0.018839420539994597 . df.query(&#39;A&gt; @meanA&#39;) #방법2 외부에 지정된 함수를 쓰기위해 @ 를 붙여줌 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . 11 0.050808 | -0.636996 | 0.190915 | 2.100255 | . 12 0.120159 | 0.617203 | 0.300170 | -0.352250 | . 14 0.838983 | 0.931102 | 0.285587 | 0.885141 | . A&gt;mean(A) &#51060;&#44256;, A&lt;0.8 &#51064; &#44163;&#51012; &#49440;&#53469; . df.query(&#39; A&gt; @meanA and A&lt;0.8&#39;) # 방법1 . A B C D . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 11 0.050808 | -0.636996 | 0.190915 | 2.100255 | . 12 0.120159 | 0.617203 | 0.300170 | -0.352250 | . df.query(&#39; A&gt; @meanA&#39; &#39; and A&lt;0.8&#39;) # 방법 2, &#39;바로 옆 띄어쓰기가 진짜 중요 . NameError Traceback (most recent call last) &lt;ipython-input-2-f44db6161dcd&gt; in &lt;module&gt; -&gt; 1 df.query(&#39; A&gt; @meanA&#39; 2 &#39; and A&lt;0.8&#39;) # 방법 2, &#39;바로 옆 띄어쓰기가 진짜 중요 NameError: name &#39;df&#39; is not defined . &#45800;&#49692;&#51064;&#45937;&#49905; . - 0, 3:5, 9:11 에 해당하는 row를 뽑고싶다. $ to$ 칼럼이름을 index로 받아서 사용한다. . df.query(&#39;index==0 or 3&lt;=index &lt;=5 or 9&lt;=index &lt;=11&#39;) . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 3 -0.322417 | -0.384054 | 1.133769 | -1.099891 | . 4 -0.172428 | -0.877858 | 0.042214 | 0.582815 | . 5 -1.100619 | 1.144724 | 0.901591 | 0.502494 | . 9 -1.117310 | 0.234416 | 1.659802 | 0.742044 | . 10 -0.191836 | -0.887629 | -0.747158 | 1.692455 | . 11 0.050808 | -0.636996 | 0.190915 | 2.100255 | . - 응용사례1 . df.query(&#39;index==0 or index ==[8,9,10]&#39;) . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 8 -0.687173 | -0.845206 | -0.671246 | -0.012665 | . 9 -1.117310 | 0.234416 | 1.659802 | 0.742044 | . 10 -0.191836 | -0.887629 | -0.747158 | 1.692455 | . - 응용사례2 . i1= np.arange(3) i1 . array([0, 1, 2]) . df.query(&#39;index in @i1 or index==5&#39;) . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 5 -1.100619 | 1.144724 | 0.901591 | 0.502494 | . &#49884;&#44228;&#50676;&#51088;&#47308;&#50640;&#49436; &#53945;&#55176; &#50976;&#50857;&#54620; query . df2=pd.DataFrame(np.random.normal(size=(10,4)), columns=list(&#39;ABCD&#39;), index=pd.date_range(&#39;20201226&#39;,periods=10)) . df2.query( &#39; &quot;2020-12-27&quot;&lt;= index &lt;= &quot;2021-01-03&quot; &#39;) . A B C D . 2020-12-27 0.508051 | 0.622423 | -2.032805 | -0.317645 | . 2020-12-28 0.345579 | 1.451286 | 0.173676 | 0.064197 | . 2020-12-29 1.002694 | -0.912398 | -1.000506 | -1.645377 | . 2020-12-30 2.361380 | 0.991708 | -0.550669 | -0.235089 | . 2020-12-31 0.495589 | -2.142331 | -0.541152 | 0.103671 | . 2021-01-01 -1.143330 | -0.482436 | -0.486146 | 1.461083 | . 2021-01-02 0.327084 | -0.722183 | 0.665579 | -1.270762 | . 2021-01-03 1.809573 | 0.100709 | 0.802929 | 0.849023 | . df2.query( &#39; &quot;2020-12-27&quot;&lt;= index &lt;= &quot;2021-01-03&quot; &#39; &#39; and A+B &lt; C&#39;) . A B C D . 2020-12-31 0.495589 | -2.142331 | -0.541152 | 0.103671 | . 2021-01-01 -1.143330 | -0.482436 | -0.486146 | 1.461083 | . 2021-01-02 0.327084 | -0.722183 | 0.665579 | -1.270762 | . . groupby . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv&#39;) . df #항공 정보 . df.columns #열이름 나열 . https://github.com/PacktPublishing/Pandas-Cookbook/blob/master/data/descriptions/flights_description.csv | . 칼럼 설명 . - 데이터프레임을 여러개의 서브데이터프레임으로 나누는 기능 . - 단독으로 쓸 이유는 별로 없다. $ to$ 그룹을 나누고 어떠한 &quot;연산&quot;을 하기 위함 . df.groupby(by=&#39;AIRLINE&#39;) . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f3368772fa0&gt; . 데이터프레임을 각 항공사 별로 나눔 | . - 확인 . grouped_df = df.groupby(by=&#39;AIRLINE&#39;) . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv&#39;) . grouped_df.groups . 너무 보기 힘듬 | . - 보기좋은 형태로 확인 . list(grouped_df.groups) #항공사안 카테고리의 리스트 출력 . [&#39;AA&#39;, &#39;AS&#39;, &#39;B6&#39;, &#39;DL&#39;, &#39;EV&#39;, &#39;F9&#39;, &#39;HA&#39;, &#39;MQ&#39;, &#39;NK&#39;, &#39;OO&#39;, &#39;UA&#39;, &#39;US&#39;, &#39;VX&#39;, &#39;WN&#39;] . grouped_df.get_group(&#39;AA&#39;) #항공사별로 나눈 것의 특정 카테고리를 df로 보기 . MONTH DAY WEEKDAY AIRLINE ORG_AIR DEST_AIR SCHED_DEP DEP_DELAY AIR_TIME DIST SCHED_ARR ARR_DELAY DIVERTED CANCELLED . 3 1 | 1 | 4 | AA | DFW | DCA | 1555 | 7.0 | 126.0 | 1192 | 1935 | -7.0 | 0 | 0 | . 6 1 | 1 | 4 | AA | DFW | MSY | 1250 | 84.0 | 64.0 | 447 | 1410 | 83.0 | 0 | 0 | . 8 1 | 1 | 4 | AA | ORD | STL | 1845 | -5.0 | 44.0 | 258 | 1950 | -5.0 | 0 | 0 | . 15 1 | 1 | 4 | AA | DEN | DFW | 1445 | -6.0 | 93.0 | 641 | 1745 | 4.0 | 0 | 0 | . 26 1 | 1 | 4 | AA | LAX | AUS | 1430 | 33.0 | 157.0 | 1242 | 1925 | 41.0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 58470 12 | 31 | 4 | AA | DFW | FAT | 1020 | -3.0 | 196.0 | 1313 | 1156 | -2.0 | 0 | 0 | . 58475 12 | 31 | 4 | AA | IAH | CLT | 710 | 1.0 | 113.0 | 912 | 1037 | -12.0 | 0 | 0 | . 58476 12 | 31 | 4 | AA | DFW | TPA | 1020 | -3.0 | 121.0 | 929 | 1340 | -6.0 | 0 | 0 | . 58479 12 | 31 | 4 | AA | DFW | ELP | 1200 | 3.0 | 94.0 | 551 | 1250 | 13.0 | 0 | 0 | . 58487 12 | 31 | 4 | AA | SFO | DFW | 515 | 5.0 | 166.0 | 1464 | 1045 | -19.0 | 0 | 0 | . 8900 rows × 14 columns . for g in grouped_df.groups: . print(g) display(grouped_df.get_group(g)) $ to$ 리스트 카테고리들 전부를 확인 . AIRLINE&#51012; &#44592;&#51456;&#51004;&#47196; &#45936;&#51060;&#53552;&#54532;&#47112;&#51076;&#51012; &#45208;&#45572;&#44256; $ to$ ARR_DELAY&#50640; mean&#54632;&#49688;&#47484; &#51201;&#50857;: (AIRLINE $ to$ {ARR_DELAY: mean}) . - 방법1 (기본, agg와 딕셔너리 이용) . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:&#39;mean&#39;}) . ARR_DELAY . AIRLINE . AA 5.542661 | . AS -0.833333 | . B6 8.692593 | . DL 0.339691 | . EV 7.034580 | . F9 13.630651 | . HA 4.972973 | . MQ 6.860591 | . NK 18.436070 | . OO 7.593463 | . UA 7.765755 | . US 1.681105 | . VX 5.348884 | . WN 6.397353 | . - 방법2 ($ star star star$) . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:np.mean}) . ARR_DELAY . AIRLINE . AA 5.542661 | . AS -0.833333 | . B6 8.692593 | . DL 0.339691 | . EV 7.034580 | . F9 13.630651 | . HA 4.972973 | . MQ 6.860591 | . NK 18.436070 | . OO 7.593463 | . UA 7.765755 | . US 1.681105 | . VX 5.348884 | . WN 6.397353 | . - 방법3 , 나눠진 df에서 ARR_DELAY 뽑고, 평균함수적용 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(&#39;mean&#39;) . AIRLINE AA 5.542661 AS -0.833333 B6 8.692593 DL 0.339691 EV 7.034580 F9 13.630651 HA 4.972973 MQ 6.860591 NK 18.436070 OO 7.593463 UA 7.765755 US 1.681105 VX 5.348884 WN 6.397353 Name: ARR_DELAY, dtype: float64 . - 방법4 ($ star$) . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(np.mean) . AIRLINE AA 5.542661 AS -0.833333 B6 8.692593 DL 0.339691 EV 7.034580 F9 13.630651 HA 4.972973 MQ 6.860591 NK 18.436070 OO 7.593463 UA 7.765755 US 1.681105 VX 5.348884 WN 6.397353 Name: ARR_DELAY, dtype: float64 . - 방법5 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].mean() . AIRLINE AA 5.542661 AS -0.833333 B6 8.692593 DL 0.339691 EV 7.034580 F9 13.630651 HA 4.972973 MQ 6.860591 NK 18.436070 OO 7.593463 UA 7.765755 US 1.681105 VX 5.348884 WN 6.397353 Name: ARR_DELAY, dtype: float64 . - 방법2와 방법4는 사용자정의 함수를 쓸 수 있다는 장점이 있음 . - 방법6 . def f(x): return -np.mean(x) #사용자정의함수 지정 . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:f}) . ARR_DELAY . AIRLINE . AA -5.542661 | . AS 0.833333 | . B6 -8.692593 | . DL -0.339691 | . EV -7.034580 | . F9 -13.630651 | . HA -4.972973 | . MQ -6.860591 | . NK -18.436070 | . OO -7.593463 | . UA -7.765755 | . US -1.681105 | . VX -5.348884 | . WN -6.397353 | . - 방법7 . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:lambda x: -np.mean(x)}) . ARR_DELAY . AIRLINE . AA -5.542661 | . AS 0.833333 | . B6 -8.692593 | . DL -0.339691 | . EV -7.034580 | . F9 -13.630651 | . HA -4.972973 | . MQ -6.860591 | . NK -18.436070 | . OO -7.593463 | . UA -7.765755 | . US -1.681105 | . VX -5.348884 | . WN -6.397353 | . - 방법8 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(lambda x: -np.mean(x)) . AIRLINE AA -5.542661 AS 0.833333 B6 -8.692593 DL -0.339691 EV -7.034580 F9 -13.630651 HA -4.972973 MQ -6.860591 NK -18.436070 OO -7.593463 UA -7.765755 US -1.681105 VX -5.348884 WN -6.397353 Name: ARR_DELAY, dtype: float64 . &#51077;&#47141;&#51060; &#50668;&#47084;&#44060;&#51064; &#49324;&#50857;&#51088; &#51221;&#51032; &#54632;&#49688;&#51032; &#49324;&#50857; . def f(x,y): return np.mean(x)**y . - 방법1 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(f,2) . AIRLINE AA 30.721086 AS 0.694444 B6 75.561166 DL 0.115390 EV 49.485310 F9 185.794656 HA 24.730460 MQ 47.067715 NK 339.888677 OO 57.660681 UA 60.306954 US 2.826113 VX 28.610564 WN 40.926120 Name: ARR_DELAY, dtype: float64 . - 방법2 . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;: lambda x: f(x,2)}) . ARR_DELAY . AIRLINE . AA 30.721086 | . AS 0.694444 | . B6 75.561166 | . DL 0.115390 | . EV 49.485310 | . F9 185.794656 | . HA 24.730460 | . MQ 47.067715 | . NK 339.888677 | . OO 57.660681 | . UA 60.306954 | . US 2.826113 | . VX 28.610564 | . WN 40.926120 | . &#54876;&#50857; . AIRLINE,WEEKDAY $ to$ {CANCELLED: sum} . - 방법1~5 . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:&#39;sum&#39;}) . CANCELLED . AIRLINE WEEKDAY . AA 1 41 | . 2 9 | . 3 16 | . 4 20 | . 5 18 | . ... ... ... | . WN 3 18 | . 4 10 | . 5 7 | . 6 10 | . 7 7 | . 98 rows × 1 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:np.sum}) . CANCELLED . AIRLINE WEEKDAY . AA 1 41 | . 2 9 | . 3 16 | . 4 20 | . 5 18 | . ... ... ... | . WN 3 18 | . 4 10 | . 5 7 | . 6 10 | . 7 7 | . 98 rows × 1 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[&#39;CANCELLED&#39;].agg(&#39;sum&#39;) . AIRLINE WEEKDAY AA 1 41 2 9 3 16 4 20 5 18 .. WN 3 18 4 10 5 7 6 10 7 7 Name: CANCELLED, Length: 98, dtype: int64 . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[&#39;CANCELLED&#39;].agg(np.sum) . AIRLINE WEEKDAY AA 1 41 2 9 3 16 4 20 5 18 .. WN 3 18 4 10 5 7 6 10 7 7 Name: CANCELLED, Length: 98, dtype: int64 . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[&#39;CANCELLED&#39;].sum() . AIRLINE WEEKDAY AA 1 41 2 9 3 16 4 20 5 18 .. WN 3 18 4 10 5 7 6 10 7 7 Name: CANCELLED, Length: 98, dtype: int64 . AIRLINE,WEEKDAY $ to$ {CANCELLED: sum, mean} , {DIVERTED: sum, mean} . - 방법 1~4 (5번은 쓸 수 없다) . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:[&#39;sum&#39;,&#39;mean&#39;],&#39;DIVERTED&#39;:[&#39;sum&#39;,&#39;mean&#39;]}) . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:[np.sum,np.mean],&#39;DIVERTED&#39;:[np.sum,np.mean]}) . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[[&#39;CANCELLED&#39;,&#39;DIVERTED&#39;]].agg([&#39;sum&#39;,&#39;mean&#39;]) #컬럼을 리스트로 전달 . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[[&#39;CANCELLED&#39;,&#39;DIVERTED&#39;]].agg([np.sum,np.mean]) . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . . AIRLINE,WEEKDAY $ to$ {CANCELLED: sum, mean, size} , {AIR_TIME: mean,var} . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:[&#39;sum&#39;,&#39;mean&#39;,&#39;size&#39;],&#39;AIR_TIME&#39;:[&#39;mean&#39;,&#39;var&#39;]}) #딕셔너리 활용 . CANCELLED AIR_TIME . sum mean size mean var . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 1277 | 147.610569 | 5393.806723 | . 2 9 | 0.007341 | 1226 | 143.851852 | 5359.890719 | . 3 16 | 0.011949 | 1339 | 144.514005 | 5378.854539 | . 4 20 | 0.015004 | 1333 | 141.124618 | 4791.524627 | . 5 18 | 0.014151 | 1272 | 145.430966 | 5884.592076 | . ... ... ... | ... | ... | ... | ... | . WN 3 18 | 0.014118 | 1275 | 104.219920 | 2901.873447 | . 4 10 | 0.007911 | 1264 | 107.200800 | 2966.568935 | . 5 7 | 0.005828 | 1201 | 107.893635 | 3268.717093 | . 6 10 | 0.010132 | 987 | 109.247433 | 3152.753719 | . 7 7 | 0.006066 | 1154 | 107.602273 | 3183.126889 | . 98 rows × 5 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]) .agg({&#39;CANCELLED&#39;:[np.sum,np.mean,len],&#39;AIR_TIME&#39;:[np.mean,lambda x: np.std(x,ddof=1)**2]}) . CANCELLED AIR_TIME . sum mean len mean &lt;lambda_0&gt; . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 1277 | 147.610569 | 5393.806723 | . 2 9 | 0.007341 | 1226 | 143.851852 | 5359.890719 | . 3 16 | 0.011949 | 1339 | 144.514005 | 5378.854539 | . 4 20 | 0.015004 | 1333 | 141.124618 | 4791.524627 | . 5 18 | 0.014151 | 1272 | 145.430966 | 5884.592076 | . ... ... ... | ... | ... | ... | ... | . WN 3 18 | 0.014118 | 1275 | 104.219920 | 2901.873447 | . 4 10 | 0.007911 | 1264 | 107.200800 | 2966.568935 | . 5 7 | 0.005828 | 1201 | 107.893635 | 3268.717093 | . 6 10 | 0.010132 | 987 | 109.247433 | 3152.753719 | . 7 7 | 0.006066 | 1154 | 107.602273 | 3183.126889 | . 98 rows × 5 columns . grouping by continuous variable &#50672;&#49549;&#54805;&#48320;&#49688; &#44536;&#47353;&#54868; . df . MONTH DAY WEEKDAY AIRLINE ORG_AIR DEST_AIR SCHED_DEP DEP_DELAY AIR_TIME DIST SCHED_ARR ARR_DELAY DIVERTED CANCELLED . 0 1 | 1 | 4 | WN | LAX | SLC | 1625 | 58.0 | 94.0 | 590 | 1905 | 65.0 | 0 | 0 | . 1 1 | 1 | 4 | UA | DEN | IAD | 823 | 7.0 | 154.0 | 1452 | 1333 | -13.0 | 0 | 0 | . 2 1 | 1 | 4 | MQ | DFW | VPS | 1305 | 36.0 | 85.0 | 641 | 1453 | 35.0 | 0 | 0 | . 3 1 | 1 | 4 | AA | DFW | DCA | 1555 | 7.0 | 126.0 | 1192 | 1935 | -7.0 | 0 | 0 | . 4 1 | 1 | 4 | WN | LAX | MCI | 1720 | 48.0 | 166.0 | 1363 | 2225 | 39.0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 58487 12 | 31 | 4 | AA | SFO | DFW | 515 | 5.0 | 166.0 | 1464 | 1045 | -19.0 | 0 | 0 | . 58488 12 | 31 | 4 | F9 | LAS | SFO | 1910 | 13.0 | 71.0 | 414 | 2050 | 4.0 | 0 | 0 | . 58489 12 | 31 | 4 | OO | SFO | SBA | 1846 | -6.0 | 46.0 | 262 | 1956 | -5.0 | 0 | 0 | . 58490 12 | 31 | 4 | WN | MSP | ATL | 525 | 39.0 | 124.0 | 907 | 855 | 34.0 | 0 | 0 | . 58491 12 | 31 | 4 | OO | SFO | BOI | 859 | 5.0 | 73.0 | 522 | 1146 | -1.0 | 0 | 0 | . 58492 rows × 14 columns . - 목표: DIST를 적당한 구간으로 나누어 카테고리화 하고 그것을 바탕으로 groupby를 수행하자. . df.DIST.hist() #앞쪽에 몰려있음 . &lt;AxesSubplot:&gt; . df.DIST.describe() #대략적 통계값 산출 . count 58492.000000 mean 872.900072 std 624.996805 min 67.000000 25% 391.000000 50% 690.000000 75% 1199.000000 max 4502.000000 Name: DIST, dtype: float64 . - 구간을 아래와 같이 설정한다. . bins=[-np.inf, 400, 700, 1200, np.inf] # -무한대 ~400 ~700 ~1200 ~무한대 . - pd.cut()을 이용하여 각 구간의 observation을 카테고리화(mapping) 하자. . cuts=pd.cut(df.DIST,bins=bins) #(적용할 칼럼, bins=구간) cuts . 0 (400.0, 700.0] 1 (1200.0, inf] 2 (400.0, 700.0] 3 (700.0, 1200.0] 4 (1200.0, inf] ... 58487 (1200.0, inf] 58488 (400.0, 700.0] 58489 (-inf, 400.0] 58490 (700.0, 1200.0] 58491 (400.0, 700.0] Name: DIST, Length: 58492, dtype: category Categories (4, interval[float64]): [(-inf, 400.0] &lt; (400.0, 700.0] &lt; (700.0, 1200.0] &lt; (1200.0, inf]] . - cuts, AIRLINE $ to$ {DIVERTED: sum} . df.groupby([cuts,&#39;AIRLINE&#39;]).agg({&#39;DIVERTED&#39;:sum}) #구분별 항공사 diverted합 산출 . DIVERTED . DIST AIRLINE . (-inf, 400.0] AA 0 | . AS 0 | . B6 0 | . DL 1 | . EV 3 | . F9 0 | . HA 0 | . MQ 0 | . NK 0 | . OO 5 | . UA 2 | . US 0 | . VX 0 | . WN 1 | . (400.0, 700.0] AA 3 | . AS 0 | . B6 0 | . DL 12 | . EV 8 | . F9 1 | . HA 0 | . MQ 4 | . NK 1 | . OO 7 | . UA 1 | . US 0 | . VX 0 | . WN 2 | . (700.0, 1200.0] AA 10 | . AS 0 | . B6 1 | . DL 6 | . EV 4 | . F9 0 | . HA 0 | . MQ 1 | . NK 1 | . OO 5 | . UA 4 | . US 0 | . VX 0 | . WN 4 | . (1200.0, inf] AA 13 | . AS 0 | . B6 1 | . DL 5 | . EV 0 | . F9 1 | . HA 1 | . MQ 0 | . NK 3 | . OO 4 | . UA 12 | . US 1 | . VX 1 | . WN 8 | . - 아래와 비교해보자. . df.groupby([&#39;AIRLINE&#39;]).agg({&#39;DIVERTED&#39;:sum}) . DIVERTED . AIRLINE . AA 26 | . AS 0 | . B6 2 | . DL 24 | . EV 15 | . F9 2 | . HA 1 | . MQ 5 | . NK 5 | . OO 21 | . UA 19 | . US 1 | . VX 1 | . WN 15 | . - cuts을 이용하여 추가그룹핑을 하면 조금 다른 특징들을 데이터에서 발견할 수 있다. . AA항공사와 DL항공사는 모두 비슷한 우회횟수를 가지고 있음. | AA항공사는 700회이상의 구간에서 우회를 많이하고 DL항공사는 400~700사이에서 우회를 많이 한다. (패턴이 다름) 상세한 패턴 확인 가능 | . - 구간이름에 label을 붙이는 방법 labels=[] 이용 . bins . [-inf, 400, 700, 1200, inf] . cuts2=pd.cut(df.DIST,bins=bins,labels=[&#39;Q1&#39;,&#39;Q2&#39;,&#39;Q3&#39;,&#39;Q4&#39;]) cuts2 . 0 Q2 1 Q4 2 Q2 3 Q3 4 Q4 .. 58487 Q4 58488 Q2 58489 Q1 58490 Q3 58491 Q2 Name: DIST, Length: 58492, dtype: category Categories (4, object): [&#39;Q1&#39; &lt; &#39;Q2&#39; &lt; &#39;Q3&#39; &lt; &#39;Q4&#39;] . df.groupby(by=[cuts2,&#39;AIRLINE&#39;]).agg({&#39;DIVERTED&#39;:sum}) . DIVERTED . DIST AIRLINE . Q1 AA 0 | . AS 0 | . B6 0 | . DL 1 | . EV 3 | . F9 0 | . HA 0 | . MQ 0 | . NK 0 | . OO 5 | . UA 2 | . US 0 | . VX 0 | . WN 1 | . Q2 AA 3 | . AS 0 | . B6 0 | . DL 12 | . EV 8 | . F9 1 | . HA 0 | . MQ 4 | . NK 1 | . OO 7 | . UA 1 | . US 0 | . VX 0 | . WN 2 | . Q3 AA 10 | . AS 0 | . B6 1 | . DL 6 | . EV 4 | . F9 0 | . HA 0 | . MQ 1 | . NK 1 | . OO 5 | . UA 4 | . US 0 | . VX 0 | . WN 4 | . Q4 AA 13 | . AS 0 | . B6 1 | . DL 5 | . EV 0 | . F9 1 | . HA 1 | . MQ 0 | . NK 3 | . OO 4 | . UA 12 | . US 1 | . VX 1 | . WN 8 | . df.groupby(cuts2).agg({&#39;DIVERTED&#39;:len}) . DIVERTED . DIST . Q1 15027 | . Q2 14697 | . Q3 14417 | . Q4 14351 | . . tidy data . ggplot으로 그림그리기 좋은 데이터 + pandas로 query, group by 등을 쓰기 좋은 자료 . Each variable must have its own column. | Each observation must have its own row. | Each value must have its own cell. | . tidy data는 보통 세로로 긴 데이터를 가짐 . &#50696;&#51228; tidy data . &#54400;&#51060;1: stack + reset_index . - 문제의 깃헙주소로 들어가서 데이터를 관찰 $ to$ 좌측상단이 비워져있음(Unnamed: 0) $ to$ index_col=0 옵션을 사용 . url = &#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/state_fruit.csv&#39; df=pd.read_csv(url,index_col=0) df . Apple Orange Banana . Texas 12 | 10 | 40 | . Arizona 9 | 7 | 12 | . Florida 0 | 14 | 190 | . &#45936;&#51060;&#53552;&#48320;&#54805; . df.stack() # 우리가 원하는 형태가 아님 #멀티인덱스로 되어있어서 tidy가 아님, 칼럼도 없음 . Texas Apple 12 Orange 10 Banana 40 Arizona Apple 9 Orange 7 Banana 12 Florida Apple 0 Orange 14 Banana 190 dtype: int64 . df.stack().reset_index() # 인덱스가 0~8이고 범주형변수로 나뉘어짐 . level_0 level_1 0 . 0 Texas | Apple | 12 | . 1 Texas | Orange | 10 | . 2 Texas | Banana | 40 | . 3 Arizona | Apple | 9 | . 4 Arizona | Orange | 7 | . 5 Arizona | Banana | 12 | . 6 Florida | Apple | 0 | . 7 Florida | Orange | 14 | . 8 Florida | Banana | 190 | . df.stack().reset_index().rename(columns={&#39;level_0&#39;:&#39;group1&#39;,&#39;level_1&#39;:&#39;group2&#39;,0:&#39;X&#39;}) #tidy data! 인덱스도 쉽고, 범주로 나뉘어서 query,groupby 도 쉬워짐 . group1 group2 X . 0 Texas | Apple | 12 | . 1 Texas | Orange | 10 | . 2 Texas | Banana | 40 | . 3 Arizona | Apple | 9 | . 4 Arizona | Orange | 7 | . 5 Arizona | Banana | 12 | . 6 Florida | Apple | 0 | . 7 Florida | Orange | 14 | . 8 Florida | Banana | 190 | . &#54400;&#51060;2: melt(id_vars=??) . - index_col=0 옵션을 몰랐다면 어떻게 tidydata로 만들까? . url = &#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/state_fruit.csv&#39; df2=pd.read_csv(url) df2 . Unnamed: 0 Apple Orange Banana . 0 Texas | 12 | 10 | 40 | . 1 Arizona | 9 | 7 | 12 | . 2 Florida | 0 | 14 | 190 | . df2.rename(columns={&#39;Unnamed: 0&#39;:&#39;group1&#39;}) #Unnamed: 0 이 너무 불편해서 그룹1로 바꿈 . group1 Apple Orange Banana . 0 Texas | 12 | 10 | 40 | . 1 Arizona | 9 | 7 | 12 | . 2 Florida | 0 | 14 | 190 | . df2.rename(columns={&#39;Unnamed: 0&#39;:&#39;group1&#39;}).melt() #세로로 바뀌었지만 그룹1이 뭔가 이상함 . variable value . 0 group1 | Texas | . 1 group1 | Arizona | . 2 group1 | Florida | . 3 Apple | 12 | . 4 Apple | 9 | . 5 Apple | 0 | . 6 Orange | 10 | . 7 Orange | 7 | . 8 Orange | 14 | . 9 Banana | 40 | . 10 Banana | 12 | . 11 Banana | 190 | . df2.rename(columns={&#39;Unnamed: 0&#39;:&#39;group1&#39;}).melt(id_vars=&#39;group1&#39;) # 수정된 melt형태, 그룹1은 열로 빼고 싶다. . group1 variable value . 0 Texas | Apple | 12 | . 1 Arizona | Apple | 9 | . 2 Florida | Apple | 0 | . 3 Texas | Orange | 10 | . 4 Arizona | Orange | 7 | . 5 Florida | Orange | 14 | . 6 Texas | Banana | 40 | . 7 Arizona | Banana | 12 | . 8 Florida | Banana | 190 | . df2.rename(columns={&#39;Unnamed: 0&#39;:&#39;group1&#39;}).melt(id_vars=&#39;group1&#39;) .rename(columns={&#39;variable&#39;:&#39;group2&#39;,&#39;value&#39;:&#39;X&#39;}) . group1 group2 X . 0 Texas | Apple | 12 | . 1 Arizona | Apple | 9 | . 2 Florida | Apple | 0 | . 3 Texas | Orange | 10 | . 4 Arizona | Orange | 7 | . 5 Florida | Orange | 14 | . 6 Texas | Banana | 40 | . 7 Arizona | Banana | 12 | . 8 Florida | Banana | 190 | . &#54400;&#51060;3 reset_index + melt . df . Apple Orange Banana . Texas 12 | 10 | 40 | . Arizona 9 | 7 | 12 | . Florida 0 | 14 | 190 | . df.reset_index() . index Apple Orange Banana . 0 Texas | 12 | 10 | 40 | . 1 Arizona | 9 | 7 | 12 | . 2 Florida | 0 | 14 | 190 | . df.reset_index().melt(id_vars=&#39;index&#39;) #melt는 사용하나 index칼럼은 칼럼의 범주로 넣어줌 . index variable value . 0 Texas | Apple | 12 | . 1 Arizona | Apple | 9 | . 2 Florida | Apple | 0 | . 3 Texas | Orange | 10 | . 4 Arizona | Orange | 7 | . 5 Florida | Orange | 14 | . 6 Texas | Banana | 40 | . 7 Arizona | Banana | 12 | . 8 Florida | Banana | 190 | . df.reset_index().melt(id_vars=&#39;index&#39;) .rename(columns={&#39;index&#39;:&#39;group1&#39;,&#39;variable&#39;:&#39;group2&#39;,&#39;value&#39;:&#39;X&#39;}) . group1 group2 X . 0 Texas | Apple | 12 | . 1 Arizona | Apple | 9 | . 2 Florida | Apple | 0 | . 3 Texas | Orange | 10 | . 4 Arizona | Orange | 7 | . 5 Florida | Orange | 14 | . 6 Texas | Banana | 40 | . 7 Arizona | Banana | 12 | . 8 Florida | Banana | 190 | . &#54400;&#51060;4 set_index + stack + reset_index + rename . df2.set_index(&#39;Unnamed: 0&#39;) . Apple Orange Banana . Unnamed: 0 . Texas 12 | 10 | 40 | . Arizona 9 | 7 | 12 | . Florida 0 | 14 | 190 | . df2.set_index(&#39;Unnamed: 0&#39;).stack() . Unnamed: 0 Texas Apple 12 Orange 10 Banana 40 Arizona Apple 9 Orange 7 Banana 12 Florida Apple 0 Orange 14 Banana 190 dtype: int64 . df2.set_index(&#39;Unnamed: 0&#39;).stack().reset_index() . Unnamed: 0 level_1 0 . 0 Texas | Apple | 12 | . 1 Texas | Orange | 10 | . 2 Texas | Banana | 40 | . 3 Arizona | Apple | 9 | . 4 Arizona | Orange | 7 | . 5 Arizona | Banana | 12 | . 6 Florida | Apple | 0 | . 7 Florida | Orange | 14 | . 8 Florida | Banana | 190 | . df2.set_index(&#39;Unnamed: 0&#39;).stack().reset_index() .rename(columns={&#39;Unnamed: 0&#39;:&#39;group1&#39;,&#39;level_1&#39;:&#39;group2&#39;,0:&#39;X&#39;}) . group1 group2 X . 0 Texas | Apple | 12 | . 1 Texas | Orange | 10 | . 2 Texas | Banana | 40 | . 3 Arizona | Apple | 9 | . 4 Arizona | Orange | 7 | . 5 Arizona | Banana | 12 | . 6 Florida | Apple | 0 | . 7 Florida | Orange | 14 | . 8 Florida | Banana | 190 | . . Barplot + &#54644;&#46308;&#47532;&#50948;&#52980;&#51032; &#44536;&#47000;&#54532;&#47112;&#51060;&#50612; . &#44592;&#48376;&#49324;&#50857;&#48277; . g=[&#39;A&#39;]*100+[&#39;B&#39;]*200 y=list(np.random.randn(100)*2+2)+list(np.random.randn(200)+3) df=pd.DataFrame({&#39;g&#39;:g,&#39;y&#39;:y}) df . g y . 0 A | -1.594055 | . 1 A | 1.225490 | . 2 A | 2.223234 | . 3 A | 1.842460 | . 4 A | 1.624541 | . ... ... | ... | . 295 B | 3.011681 | . 296 B | 3.558141 | . 297 B | 4.348230 | . 298 B | 3.966407 | . 299 B | 2.694083 | . 300 rows × 2 columns . ggplot(df)+geom_bar(aes(x=&#39;g&#39;,fill=&#39;g&#39;)) ## 디폴트로 카운트를 수행해줌 #안쪽을 채우려면 fill= 이용, 직관적이지는 않다. . &lt;ggplot: (8726962443840)&gt; . df.groupby(by=&#39;g&#39;).count() # = df.groupby(by=&#39;g&#39;).agg({&#39;y&#39;:len}) .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/11/07/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%8B%9C%ED%97%98%EA%B3%B5%EB%B6%803.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/11/07/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%8B%9C%ED%97%98%EA%B3%B5%EB%B6%803.html",
            "date": " • Nov 7, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "데이터시각화 시험공부2",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 import cv2 as cv from scipy import stats . rpy2 . import rpy2 . R 에서 저장된 데이터를 불러오는 함수,패키지 . . &#45936;&#51060;&#53552; &#44032;&#51648;&#44256; &#45440;&#44592;(&#49328;&#51216;&#46020; &#51025;&#50857;) . &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44256; &#45824;&#47029;&#51201;&#51064; &#54644;&#49437;(plotnine&#51032; ggplot) . mpg = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/mpg.csv&#39;) . ggplot(data = mpg) + geom_point(mapping = aes(x = &quot;displ&quot;, y = &quot;hwy&quot;)) ## plotnine(R의 ggplot 짭퉁) # 빠르게 그리기: `mapping = ` 와 `data=`는 생략가능함 # = ggplot(mpg) + geom_point(aes(x = &quot;displ&quot;, y = &quot;hwy&quot;)) ## plotnine . &lt;ggplot: (8782640468625)&gt; . 산점도: 엔진크기displ와 연료효율hwy은 반비례. (엔진이 큰 차일수록 연비가 좋지 않다) | . ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) . 진짜 ggplot에서 그릴때에는 변수이름에 &quot;&quot; 를 제거함 . &#49328;&#51216;&#46020;&#51025;&#50857; (3&#52264;&#50896;) . - 데이터를 다시관찰 . mpg.head() . manufacturer model displ year cyl trans drv cty hwy fl class . 1 audi | a4 | 1.8 | 1999 | 4 | auto(l5) | f | 18 | 29 | p | compact | . 2 audi | a4 | 1.8 | 1999 | 4 | manual(m5) | f | 21 | 29 | p | compact | . 3 audi | a4 | 2.0 | 2008 | 4 | manual(m6) | f | 20 | 31 | p | compact | . 4 audi | a4 | 2.0 | 2008 | 4 | auto(av) | f | 21 | 30 | p | compact | . 5 audi | a4 | 2.8 | 1999 | 6 | auto(l5) | f | 16 | 26 | p | compact | . - class도 함께 plot에 표시하면 데이터를 탐색할때 좀 더 좋을것 같다. . &#49328;&#51216;&#46020; + &#51216;&#53356;&#44592;&#48320;&#44221; . ggplot(data=mpg)+ geom_point(mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,size= &#39;class&#39;)) # ,size = &#39;class&#39; 추가로 산점도의 점크기 변경을 통해 3차원 해석 가능 #하지만 색이 같아서 알아보기 힘들다. . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. . &lt;ggplot: (8782640390747)&gt; . &#49328;&#51216;&#46020; + &#53804;&#47749;&#46020;&#48320;&#44221; . ggplot(data=mpg)+ geom_point(mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,alpha= &#39;class&#39;)) # alpha=&#39;class&#39; 의 추가로 산점도에 크기가 아닌 투명도로 3차원 그래프 생성 . /home/cgb3/anaconda3/envs/dv2021/lib/python3.8/site-packages/plotnine/scales/scale_alpha.py:68: PlotnineWarning: Using alpha for a discrete variable is not advised. . &lt;ggplot: (8745132609895)&gt; . &#49328;&#51216;&#46020;&#50640; &#51216;&#53356;&#44592; + &#53804;&#47749;&#46020; &#54633;&#54616;&#50668; 3&#52264;&#50896; &#54364;&#44592; . ggplot(data=mpg)+ geom_point(mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,size= &#39;class&#39;,alpha=&#39;class&#39;)) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_alpha.py:68: PlotnineWarning: Using alpha for a discrete variable is not advised. . &lt;ggplot: (8782640337583)&gt; . &#49328;&#51216;&#46020; + &#54805;&#53468;(&#51216;&#51032; &#47784;&#50577;) . ggplot(data=mpg)+ geom_point(mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,shape=&#39;class&#39;)) # shape = &#39;class&#39; 추가로 모양을 통한 3차원 해석 . &lt;ggplot: (8782640285946)&gt; . &#49328;&#51216;&#46020; + &#49353;&#44628; . ggplot(data=mpg)+ geom_point(mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,color=&#39;class&#39;)) # color = &#39;class&#39; 를 통해 색깔을 통한 3차원 해석 . &lt;ggplot: (8782640439414)&gt; . &#50612;&#47157;&#44172; &#44536;&#47532;&#44592; . fig=ggplot(data=mpg) #도화지를 준비한다 a1=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;) #변수와 에스테틱사이의 맵핑을 설정한다. point1=geom_point(mapping=a1) #점들의 집합을 만든다. 즉 포인트지옴을 만든다. fig+point1 #도화지와 지옴을 합친다. . &lt;ggplot: (8782638599304)&gt; . &#44061;&#52404; &#51648;&#54693;&#51201;&#51004;&#47196; &#44536;&#47532;&#44592; . a2=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,color=&#39;class&#39;) . a1,a2 . ({&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;}, {&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;, &#39;color&#39;: &#39;class&#39;}) . point2=geom_point(a2) . fig+point2 . &lt;ggplot: (8782638599244)&gt; . &#44536;&#47000;&#54532;&#50640; &#51201;&#54633;&#49440; &#52628;&#44032; . sline1=geom_smooth(a1) . fig+point1+sline1 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782638568807)&gt; . fig+point2+sline1 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782638484258)&gt; . ggplot(data=mpg)+geom_point(mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,color=&#39;class&#39;))+geom_smooth(mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;)) # 명렁어를 통해 한번에 그리기 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782638434226)&gt; . - 공통적인 맵핑규칙은 ggplot()쪽으로 빼기도 한다. (figure를 선언하는 곳에서 공통으로 선언함) . ggplot(data=mpg,mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;))+geom_point(mapping=aes(color=&#39;class&#39;))+geom_smooth() . /home/cgb3/anaconda3/envs/dv2021/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8745132650084)&gt; . - R에서는 confidence interval도 geom_smooth()를 이용하여 확인할 수 있다. . . &#49328;&#51216;&#46020;&#51025;&#50857;2 (4&#52264;&#50896;) . drv (전륜, 후륜, 4륜 구동)에 따라서 데이터를 시각화 하고 싶다. . ggplot(data=mpg,mapping=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;))+geom_point(mapping=aes(size=&#39;class&#39;,color=&#39;drv&#39;),alpha=0.2) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. . &lt;ggplot: (8782638480496)&gt; . 모든 $x$에 대하여 붉은색 점들이 대부분 초록선과 보라색 점들에 비하여 아래쪽에 위치하여 있음 $ to$ 4륜구동방식이 연비가 좋지 않음 | . - 객체지향적 . a3=a2.copy() #a2를 복사하지만 저장된 id는 다름 . id(a1),id(a2),id(a3) . (140522217560720, 140522217562000, 140522213860432) . a1,a2,a3 . ({&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;}, {&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;, &#39;color&#39;: &#39;class&#39;}, {&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;, &#39;color&#39;: &#39;class&#39;}) . a3[&#39;color&#39;]=&#39;drv&#39; a3[&#39;size&#39;]=&#39;class&#39; . a1,a2,a3 . ({&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;}, {&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;, &#39;color&#39;: &#39;class&#39;}, {&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;, &#39;color&#39;: &#39;drv&#39;, &#39;size&#39;: &#39;class&#39;}) . 아래와 같이 선언해도 괜찮음 a3=aes(x=&#39;displ&#39;,y=&#39;hwy&#39;,color=&#39;drv&#39;,size=&#39;class&#39;) . | . point3=geom_point(a3,alpha=0.2) #alpha 로 투명도 조절 . fig+point3 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. . &lt;ggplot: (8782638340422)&gt; . - 여기에 선을 추가하여 보자. . fig+point3+sline1 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782638289195)&gt; . &#44033; &#44536;&#47353;&#48324;&#47196; &#49440;&#51012; &#46384;&#47196; &#44536;&#47532;&#44592; . a4=a2.copy() . a4[&#39;color&#39;]=&#39;drv&#39; . sline2=geom_smooth(a4) . fig+sline2+point3 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782638289165)&gt; . &#49440;&#51032; &#49353;&#44628;&#51012; &#46041;&#51068;&#54616;&#44172; &#54616;&#44256; &#49440;&#51032; &#53440;&#51077;&#51012; &#48320;&#44221;&#54616;&#50668; &#44536;&#47353;&#51012; &#54364;&#49884; . a5=a1.copy() . a5[&#39;linetype&#39;]=&#39;drv&#39; . a5 . {&#39;x&#39;: &#39;displ&#39;, &#39;y&#39;: &#39;hwy&#39;, &#39;linetype&#39;: &#39;drv&#39;} . sline3=geom_smooth(a5,size=0.5,color=&#39;gray&#39;) . fig+point3+sline3 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782638256240)&gt; . fig+point3+sline3+sline1 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782638252496)&gt; . sline2=geom_smooth(a4,size=0.5,linetype=&#39;dashed&#39;) fig+point3+sline2+sline1 # 그래도 색으로 수분하는 것이 시각적으로 훌륭해보임 . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8782716497014)&gt; . - 고차원의 변수를 표현할 수 있는 무기는 다양하다. . 산점도(포인트지옴): 점의크기, 점의형태, 점의색깔, 점의투명도 | 라인플랏(스무스지옴, 라인지옴): 선의형태, 선의색깔, 선의굵기 | . &#44208;&#47200; . - 잘 훈련한다면 여러가지 형태의 고차원 그래프를 우리도 그릴 수 있다. (마치 미나드처럼) . - 해들리위컴은 이러한 방법을 체계적으로 정리했다고 보여진다. . - 해들리위컴: 그래프는 데이터 + 지옴 + 맵핑(변수와 에스테틱간의 맵핑) + 스탯(통계) + 포지션 + 축 + 패싯그리드 7개의 조합으로 그릴수 있다. . 내생각: 지옴과 맵핑만 잘 이용해도 아주 다양한 그래프를 그릴 수 있음. | . . &#54032;&#45796;&#49828;&#50640;&#49436; column&#51012; &#49440;&#53469;&#54616;&#45716; &#48169;&#48277; . &#50696;&#51228;1 &#50676; &#44256;&#47476;&#44592; . 예제 df 생성 . dic={&#39;X1&#39;:np.random.normal(0,1,5), &#39;X2&#39;:np.random.normal(0,1,5), &#39;X3&#39;:np.random.normal(0,1,5)} df=pd.DataFrame(dic) df . X1 X2 X3 . 0 0.316403 | 1.819641 | -0.535532 | . 1 -0.490887 | 1.222014 | -0.886578 | . 2 0.175277 | 1.124193 | 0.919066 | . 3 -1.996759 | -0.253654 | 0.214364 | . 4 0.359407 | -0.505990 | -0.342295 | . df.X1 #방법 1 . 0 0.316403 1 -0.490887 2 0.175277 3 -1.996759 4 0.359407 Name: X1, dtype: float64 . df[&#39;X1&#39;] #방법 2 series를 리턴 . 0 0.316403 1 -0.490887 2 0.175277 3 -1.996759 4 0.359407 Name: X1, dtype: float64 . df[[&#39;X1&#39;]] #방법 3 dataframe을 리턴 . X1 . 0 0.316403 | . 1 -0.490887 | . 2 0.175277 | . 3 -1.996759 | . 4 0.359407 | . df.loc[:,&#39;X1&#39;] #방법 4 loc 이용 , series 리턴 . 0 0.316403 1 -0.490887 2 0.175277 3 -1.996759 4 0.359407 Name: X1, dtype: float64 . df.loc[:,[&#39;X1&#39;]] #방법 5 loc 이용 , dataframe 리턴 . X1 . 0 0.316403 | . 1 -0.490887 | . 2 0.175277 | . 3 -1.996759 | . 4 0.359407 | . df.loc[:,[True,False,False]] #방법 6 , bull 인덱싱 (첫번째 열만 True로 불러오겠다.) . X1 . 0 0.316403 | . 1 -0.490887 | . 2 0.175277 | . 3 -1.996759 | . 4 0.359407 | . df.iloc[:,0] # 방법 7 , 0번째 ( 파이썬이라 0이 1 ), series 리턴 . 0 0.316403 1 -0.490887 2 0.175277 3 -1.996759 4 0.359407 Name: X1, dtype: float64 . df.iloc[:,[0]] # 방법 8 , 0번째 데이터 프레임 리턴 . X1 . 0 0.316403 | . 1 -0.490887 | . 2 0.175277 | . 3 -1.996759 | . 4 0.359407 | . df.iloc[:,[True,False,False]] #방법9 , bull형 iloc . X1 . 0 0.316403 | . 1 -0.490887 | . 2 0.175277 | . 3 -1.996759 | . 4 0.359407 | . - df.X1로 열을 선택하는게 간단하고 편리함. . 단점1: 변수이름을 알고 있어야 한다는 단점이 있음. | 단점2: 변수이름에 .이 있거나 변수이름에서 공백이 있을경우 사용할 수 없음. | . 단점의 예시 . dic={&#39;X.1&#39;:np.random.normal(0,1,5), &#39;X.2&#39;:np.random.normal(0,1,5), &#39;X.3&#39;:np.random.normal(0,1,5)} _df=pd.DataFrame(dic) _df . X.1 X.2 X.3 . 0 0.135452 | -1.312018 | -0.431092 | . 1 -0.531713 | -0.235149 | 0.523488 | . 2 0.360346 | 0.805032 | -1.374940 | . 3 -1.082895 | -0.941979 | 0.687203 | . 4 0.654108 | 0.098545 | 0.870038 | . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv&#39;) . &#50668;&#47084;&#44060;&#51032; &#50676;&#51012; &#49440;&#53469;&#54616;&#44592; . dic2={&#39;X1&#39;:np.random.normal(0,1,5), &#39;X2&#39;:np.random.normal(0,1,5), &#39;X3&#39;:np.random.normal(0,1,5), &#39;X4&#39;:np.random.normal(0,1,5)} df2=pd.DataFrame(dic2) df2 . X1 X2 X3 X4 . 0 -0.289599 | -1.360181 | -1.383041 | 0.039000 | . 1 2.079617 | 0.387465 | 0.409297 | -0.125807 | . 2 0.239503 | -0.119209 | -0.366100 | -0.463638 | . 3 2.425258 | -0.832763 | -0.770286 | -0.015144 | . 4 1.337020 | 0.066310 | 1.293585 | -1.537895 | . df2[[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]] #방법 1 . X1 X2 X3 . 0 -0.289599 | -1.360181 | -1.383041 | . 1 2.079617 | 0.387465 | 0.409297 | . 2 0.239503 | -0.119209 | -0.366100 | . 3 2.425258 | -0.832763 | -0.770286 | . 4 1.337020 | 0.066310 | 1.293585 | . df2.loc[:,[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]] #방법 2 . X1 X2 X3 . 0 -0.289599 | -1.360181 | -1.383041 | . 1 2.079617 | 0.387465 | 0.409297 | . 2 0.239503 | -0.119209 | -0.366100 | . 3 2.425258 | -0.832763 | -0.770286 | . 4 1.337020 | 0.066310 | 1.293585 | . df2.loc[:,&#39;X1&#39;:&#39;X3&#39;] #방법 3 . X1 X2 X3 . 0 -0.289599 | -1.360181 | -1.383041 | . 1 2.079617 | 0.387465 | 0.409297 | . 2 0.239503 | -0.119209 | -0.366100 | . 3 2.425258 | -0.832763 | -0.770286 | . 4 1.337020 | 0.066310 | 1.293585 | . df2.loc[:,[True,True,True,False]] #방법 4 . X1 X2 X3 . 0 -0.289599 | -1.360181 | -1.383041 | . 1 2.079617 | 0.387465 | 0.409297 | . 2 0.239503 | -0.119209 | -0.366100 | . 3 2.425258 | -0.832763 | -0.770286 | . 4 1.337020 | 0.066310 | 1.293585 | . df2.iloc[:,[0,1,2]] #방법 5 . X1 X2 X3 . 0 -0.289599 | -1.360181 | -1.383041 | . 1 2.079617 | 0.387465 | 0.409297 | . 2 0.239503 | -0.119209 | -0.366100 | . 3 2.425258 | -0.832763 | -0.770286 | . 4 1.337020 | 0.066310 | 1.293585 | . df2.iloc[:,:3] #방법 6 # = df2.iloc[:,0:3] # = df2.iloc[:,range(3)] . X1 X2 X3 . 0 -0.289599 | -1.360181 | -1.383041 | . 1 2.079617 | 0.387465 | 0.409297 | . 2 0.239503 | -0.119209 | -0.366100 | . 3 2.425258 | -0.832763 | -0.770286 | . 4 1.337020 | 0.066310 | 1.293585 | . df2.iloc[:,[True,True,True,False]] #방법 7 , iloc + bull . X1 X2 X3 . 0 -0.289599 | -1.360181 | -1.383041 | . 1 2.079617 | 0.387465 | 0.409297 | . 2 0.239503 | -0.119209 | -0.366100 | . 3 2.425258 | -0.832763 | -0.770286 | . 4 1.337020 | 0.066310 | 1.293585 | . 그래서 column의 이름이 integer일 경우는 종종 매우 헷갈리는 일이 일어남 . Note: 사실 이것은 일부러 헷갈리게 예제를 구성한 것이다. 실제로는 헷갈리는 상황이 그렇게 자주 발생하지 않는다. 왜냐하면 보통 위와 같은 형태의 자료는 ndarray로 처리하고 colname이 있는 경우만 데이터프레임으로 처리하기 때문. . . &#49892;&#51204; &#50696;&#51228; . movie data - 특정조건에 맞는 열을 선택 . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv&#39;) . df.columns #열의 이름 출력 . Index([&#39;color&#39;, &#39;director_name&#39;, &#39;num_critic_for_reviews&#39;, &#39;duration&#39;, &#39;director_facebook_likes&#39;, &#39;actor_3_facebook_likes&#39;, &#39;actor_2_name&#39;, &#39;actor_1_facebook_likes&#39;, &#39;gross&#39;, &#39;genres&#39;, &#39;actor_1_name&#39;, &#39;movie_title&#39;, &#39;num_voted_users&#39;, &#39;cast_total_facebook_likes&#39;, &#39;actor_3_name&#39;, &#39;facenumber_in_poster&#39;, &#39;plot_keywords&#39;, &#39;movie_imdb_link&#39;, &#39;num_user_for_reviews&#39;, &#39;language&#39;, &#39;country&#39;, &#39;content_rating&#39;, &#39;budget&#39;, &#39;title_year&#39;, &#39;actor_2_facebook_likes&#39;, &#39;imdb_score&#39;, &#39;aspect_ratio&#39;, &#39;movie_facebook_likes&#39;], dtype=&#39;object&#39;) . df.loc[:,[&#39;color&#39;:&#39;num_voted_users&#39;,&#39;aspect_ratio&#39;]] #출력이 안됨 . File &#34;&lt;ipython-input-92-5458cab12328&gt;&#34;, line 1 df.loc[:,[&#39;color&#39;:&#39;num_voted_users&#39;,&#39;aspect_ratio&#39;]] #출력이 안됨 ^ SyntaxError: invalid syntax . - (팁) 복잡한 조건은 iloc으로 쓰는게 편할때가 있다. $ to$ 그런데 df.columns 변수들이 몇번인지 알아보기 힘듬 . pd.Series(df.columns) # $ 열의 이름을 인덱스와 함께 출력 . 0 color 1 director_name 2 num_critic_for_reviews 3 duration 4 director_facebook_likes 5 actor_3_facebook_likes 6 actor_2_name 7 actor_1_facebook_likes 8 gross 9 genres 10 actor_1_name 11 movie_title 12 num_voted_users 13 cast_total_facebook_likes 14 actor_3_name 15 facenumber_in_poster 16 plot_keywords 17 movie_imdb_link 18 num_user_for_reviews 19 language 20 country 21 content_rating 22 budget 23 title_year 24 actor_2_facebook_likes 25 imdb_score 26 aspect_ratio 27 movie_facebook_likes dtype: object . list(range(13))+[26] # 0~12까지 + 26 . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 26] . df.iloc[:,list(range(13))+[26]] . color director_name num_critic_for_reviews duration director_facebook_likes actor_3_facebook_likes actor_2_name actor_1_facebook_likes gross genres actor_1_name movie_title num_voted_users aspect_ratio . 0 Color | James Cameron | 723.0 | 178.0 | 0.0 | 855.0 | Joel David Moore | 1000.0 | 760505847.0 | Action|Adventure|Fantasy|Sci-Fi | CCH Pounder | Avatar | 886204 | 1.78 | . 1 Color | Gore Verbinski | 302.0 | 169.0 | 563.0 | 1000.0 | Orlando Bloom | 40000.0 | 309404152.0 | Action|Adventure|Fantasy | Johnny Depp | Pirates of the Caribbean: At World&#39;s End | 471220 | 2.35 | . 2 Color | Sam Mendes | 602.0 | 148.0 | 0.0 | 161.0 | Rory Kinnear | 11000.0 | 200074175.0 | Action|Adventure|Thriller | Christoph Waltz | Spectre | 275868 | 2.35 | . 3 Color | Christopher Nolan | 813.0 | 164.0 | 22000.0 | 23000.0 | Christian Bale | 27000.0 | 448130642.0 | Action|Thriller | Tom Hardy | The Dark Knight Rises | 1144337 | 2.35 | . 4 NaN | Doug Walker | NaN | NaN | 131.0 | NaN | Rob Walker | 131.0 | NaN | Documentary | Doug Walker | Star Wars: Episode VII - The Force Awakens | 8 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4911 Color | Scott Smith | 1.0 | 87.0 | 2.0 | 318.0 | Daphne Zuniga | 637.0 | NaN | Comedy|Drama | Eric Mabius | Signed Sealed Delivered | 629 | NaN | . 4912 Color | NaN | 43.0 | 43.0 | NaN | 319.0 | Valorie Curry | 841.0 | NaN | Crime|Drama|Mystery|Thriller | Natalie Zea | The Following | 73839 | 16.00 | . 4913 Color | Benjamin Roberds | 13.0 | 76.0 | 0.0 | 0.0 | Maxwell Moody | 0.0 | NaN | Drama|Horror|Thriller | Eva Boehnke | A Plague So Pleasant | 38 | NaN | . 4914 Color | Daniel Hsia | 14.0 | 100.0 | 0.0 | 489.0 | Daniel Henney | 946.0 | 10443.0 | Comedy|Drama|Romance | Alan Ruck | Shanghai Calling | 1255 | 2.35 | . 4915 Color | Jon Gunn | 43.0 | 90.0 | 16.0 | 16.0 | Brian Herzlinger | 86.0 | 85222.0 | Documentary | John August | My Date with Drew | 4285 | 1.85 | . 4916 rows × 14 columns . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv&#39;) . actor&#46972;&#45716; &#45800;&#50612;&#44032; &#54252;&#54632;&#46108; &#48320;&#49688;&#46308;&#47564; &#48977;&#44256;&#49910;&#45796;. . df.iloc[:,list(map(lambda x : &#39;actor&#39; in x, df.columns) )] # 방법1 iloc에서 map과 람다를 이용 # = df.iloc[:,map(lambda x : &#39;actor&#39; in x, df.columns)] . actor_3_facebook_likes actor_2_name actor_1_facebook_likes actor_1_name actor_3_name actor_2_facebook_likes . 0 855.0 | Joel David Moore | 1000.0 | CCH Pounder | Wes Studi | 936.0 | . 1 1000.0 | Orlando Bloom | 40000.0 | Johnny Depp | Jack Davenport | 5000.0 | . 2 161.0 | Rory Kinnear | 11000.0 | Christoph Waltz | Stephanie Sigman | 393.0 | . 3 23000.0 | Christian Bale | 27000.0 | Tom Hardy | Joseph Gordon-Levitt | 23000.0 | . 4 NaN | Rob Walker | 131.0 | Doug Walker | NaN | 12.0 | . ... ... | ... | ... | ... | ... | ... | . 4911 318.0 | Daphne Zuniga | 637.0 | Eric Mabius | Crystal Lowe | 470.0 | . 4912 319.0 | Valorie Curry | 841.0 | Natalie Zea | Sam Underwood | 593.0 | . 4913 0.0 | Maxwell Moody | 0.0 | Eva Boehnke | David Chandler | 0.0 | . 4914 489.0 | Daniel Henney | 946.0 | Alan Ruck | Eliza Coupe | 719.0 | . 4915 16.0 | Brian Herzlinger | 86.0 | John August | Jon Gunn | 23.0 | . 4916 rows × 6 columns . df.loc[:,list(map(lambda x : &#39;actor&#39; in x, df.columns) )] # 방법 2 loc에서 map과 람다를 이용 # = df.loc[:,map(lambda x : &#39;actor&#39; in x, df.columns)] . actor_3_facebook_likes actor_2_name actor_1_facebook_likes actor_1_name actor_3_name actor_2_facebook_likes . 0 855.0 | Joel David Moore | 1000.0 | CCH Pounder | Wes Studi | 936.0 | . 1 1000.0 | Orlando Bloom | 40000.0 | Johnny Depp | Jack Davenport | 5000.0 | . 2 161.0 | Rory Kinnear | 11000.0 | Christoph Waltz | Stephanie Sigman | 393.0 | . 3 23000.0 | Christian Bale | 27000.0 | Tom Hardy | Joseph Gordon-Levitt | 23000.0 | . 4 NaN | Rob Walker | 131.0 | Doug Walker | NaN | 12.0 | . ... ... | ... | ... | ... | ... | ... | . 4911 318.0 | Daphne Zuniga | 637.0 | Eric Mabius | Crystal Lowe | 470.0 | . 4912 319.0 | Valorie Curry | 841.0 | Natalie Zea | Sam Underwood | 593.0 | . 4913 0.0 | Maxwell Moody | 0.0 | Eva Boehnke | David Chandler | 0.0 | . 4914 489.0 | Daniel Henney | 946.0 | Alan Ruck | Eliza Coupe | 719.0 | . 4915 16.0 | Brian Herzlinger | 86.0 | John August | Jon Gunn | 23.0 | . 4916 rows × 6 columns . . &#48320;&#49688;&#51060;&#47492;&#51060; s&#47196; &#45149;&#45208;&#45716; &#48320;&#49688;&#46308;&#47564; &#48977;&#44256;&#49910;&#45796;. . df.iloc[:,map(lambda x: &#39;s&#39; == x[-1],df.columns )] . df.loc[:,map(lambda x: &#39;s&#39; == x[-1],df.columns )] . &#48320;&#49688;&#51060;&#47492;&#51060; c &#54841;&#51008; d&#47196; &#49884;&#51089;&#54616;&#45716; &#48320;&#49688;&#46308;&#47564; &#48977;&#44256;&#49910;&#45796;. . df.iloc[:,map(lambda x: &#39;c&#39; == x[0] or &#39;d&#39; == x[0] ,df.columns )] . . df&#51032; &#54665;&#51012; &#49440;&#53469;&#54616;&#45716; &#48169;&#48277; . &#52395;&#48264;&#51704; &#54665; &#49440;&#53469;&#54644;&#48372;&#44592; . np.random.seed(1) dic= {&#39;X1&#39;:np.random.normal(0,1,5), &#39;X2&#39;:np.random.normal(0,1,5), &#39;X3&#39;:np.random.normal(0,1,5), &#39;X4&#39;:np.random.normal(0,1,5), &#39;X5&#39;:np.random.normal(0,1,5), &#39;X6&#39;:np.random.normal(0,1,5)} df1=pd.DataFrame(dic) df1 . X1 X2 X3 X4 X5 X6 . 0 1.624345 | -2.301539 | 1.462108 | -1.099891 | -1.100619 | -0.683728 | . 1 -0.611756 | 1.744812 | -2.060141 | -0.172428 | 1.144724 | -0.122890 | . 2 -0.528172 | -0.761207 | -0.322417 | -0.877858 | 0.901591 | -0.935769 | . 3 -1.072969 | 0.319039 | -0.384054 | 0.042214 | 0.502494 | -0.267888 | . 4 0.865408 | -0.249370 | 1.133769 | 0.582815 | 0.900856 | 0.530355 | . df1.iloc[0] #방법 1 , 뭔가 이상하다. . X1 1.624345 X2 -2.301539 X3 1.462108 X4 -1.099891 X5 -1.100619 X6 -0.683728 Name: 0, dtype: float64 . 같은 출력 . df1.iloc[0,:] df1.loc[0] df1.loc[0,:] . df1.iloc[[0]] #방법 2 , 데이터 프레임형태로 불러와서 괜찮아보인다. . X1 X2 X3 X4 X5 X6 . 0 1.624345 | -2.301539 | 1.462108 | -1.099891 | -1.100619 | -0.683728 | . 같은 출력 . df1.iloc[[0],:] df1.loc[[0]] df1.loc[[0],:] df1.iloc[[True,False,False,False,False]] df1.iloc[[True,False,False,False,False],:] df1.loc[[True,False,False,False,False]] df1.loc[[True,False,False,False,False],:] . 1,3&#54665;&#51012; &#49440;&#53469;&#54616;&#45716; &#48169;&#48277; (&#48520;, &#49836;&#46972;&#51060;&#49905;&#46321; &#45796;&#50577;&#54620; &#48169;&#48277; &#51316;&#51116;) . df1.iloc[[0,2],:] #방법 1 . X1 X2 X3 X4 X5 X6 . 0 1.624345 | -2.301539 | 1.462108 | -1.099891 | -1.100619 | -0.683728 | . 2 -0.528172 | -0.761207 | -0.322417 | -0.877858 | 0.901591 | -0.935769 | . df1.loc[[0,2],:] #방법 2 . X1 X2 X3 X4 X5 X6 . 0 1.624345 | -2.301539 | 1.462108 | -1.099891 | -1.100619 | -0.683728 | . 2 -0.528172 | -0.761207 | -0.322417 | -0.877858 | 0.901591 | -0.935769 | . - 대부분의 경우 observation에 특정한 이름이 있는 경우는 없으므로 loc이 그다지 쓸모 없음 . - 그렇지만 특정경우에는 쓸모가 있음 . 날짜 출력 같은 경우, loc이 더 쓸모가 있다. . - Note: 아래의 사실을 기억하자. . 기본적으로는 iloc, loc은 [2], [2:] 처럼 1차원으로 원소를 인덱싱할수도 있고, [2,3], [:,2] 와 같이 2차원으로 인덱싱할 수도 있다. . | 1차원으로 인덱싱하는 경우는 기본적으로 행을 인덱싱한다 $ to$ iloc, loc은 행과 더 친하고 열과 친하지 않다. . | 따라서 열을 선택하는 방법에 있어서 loc, iloc이 그렇제 좋은 방법은 아니다. . | 그렇지만 열을 선택하는 방법은 iloc이나 loc이 제일 편리하다. (이외의 다른 방법이 마땅하게 없음) 그래서 열을 선택할때도 iloc이나 loc을 선호한다. . | row는 특정간격으로 뽑는 일이 빈번함. (예를들어 일별데이터를 주별데이터로 바꾸고싶을때, 바꾸고 싶을 경우?) . | col을 특정간격으로 뽑아야 하는 일은 없음 . | . lambda + map &#51004;&#47196; &#51064;&#45937;&#49905; . np.random.seed(1) df2= pd.DataFrame(np.random.normal(size=(10,4)),columns=list(&#39;ABCD&#39;)) df2 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 3 -0.322417 | -0.384054 | 1.133769 | -1.099891 | . 4 -0.172428 | -0.877858 | 0.042214 | 0.582815 | . 5 -1.100619 | 1.144724 | 0.901591 | 0.502494 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . 7 -0.267888 | 0.530355 | -0.691661 | -0.396754 | . 8 -0.687173 | -0.845206 | -0.671246 | -0.012665 | . 9 -1.117310 | 0.234416 | 1.659802 | 0.742044 | . &#52860;&#47100;A&#51032; &#44050;&#51060; 0&#48372;&#45796; &#53360; &#44221;&#50864;&#51032; &#54665;&#47564; . df2.loc[map(lambda x: x&gt;0,df2[&#39;A&#39;]),:] # 방법 1 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . df2.loc[lambda df: df[&#39;A&#39;]&gt;0,:] # 방법 2 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 1 0.865408 | -2.301539 | 1.744812 | -0.761207 | . 2 0.319039 | -0.249370 | 1.462108 | -2.060141 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . map의 기능 | . (1) 리스트를 원소별로 분해하여 . (2) 어떠한 함수를 적용하여 아웃풋을 구한뒤 . (3) 각각의 아웃풋을 다시 하나의 리스트로 묶음 . 우리는 이중에서 (1),(3)에만 집중했음 | 하지만 생각해보면 일단 (2) 일단 함수를 적용하는 기능이 있었음 | 그런데 위의 코드는 함수를 적용한 결과가 아니라 함수 오브젝트 자체를 전달하여도 동작함 | . 요약!! . True, False로 이루어진 벡터를 리스트의 형태로 전달하여 인덱싱했음 (원래 우리가 알고 있는 개념) | True, False로 이루어진 벡터를 리턴할 수 있는 함수오브젝트 자체를 전달해도 인덱싱이 가능 | . &#52860;&#47100;A&gt;0 &#51060;&#44256; &#52860;&#47100;C&lt;0 &#51064; &#44221;&#50864;&#51032; &#54665; &#51064;&#45937;&#49905; . df2.loc[map(lambda x,y: x&gt;0 and y&lt;0, df2[&#39;A&#39;],df2[&#39;C&#39;]),:] # 방법 1 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . df2.loc[map(lambda x,y: (x&gt;0) &amp; (y&lt;0), df2[&#39;A&#39;],df2[&#39;C&#39;]),:] # 방법 2 # 0&lt;3.2 &amp; 0&lt;2.2 랑 (0&lt;3.2) &amp; (0&lt;2.2) 를 헷갈리면 안된다. . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | . df2.loc[lambda df: (df[&#39;A&#39;] &gt;0) &amp; (df[&#39;C&#39;]&lt;0)] # 방법 3 . A B C D . 0 1.624345 | -0.611756 | -0.528172 | -1.072969 | . 6 0.900856 | -0.683728 | -0.122890 | -0.935769 | .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/11/07/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%8B%9C%ED%97%98%EA%B3%B5%EB%B6%802.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/11/07/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%8B%9C%ED%97%98%EA%B3%B5%EB%B6%802.html",
            "date": " • Nov 7, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "데이터시각화 시험공부1",
            "content": "&#51452;&#47196;&#50416;&#45716; &#54056;&#53412;&#51648;&#46308; . import numpy as np #넘파이 import pandas as pd #판다스 from plotnine import * #플롯나인 import matplotlib.pyplot as plt #맷플랏립 import plotly.express as px #플랏리 상호작용 그래프 from IPython.display import HTML #블로그에 html로 올리려고 변환하는 패키지 import seaborn as sns # 씨본, 히스토그램 깔끔하게 그리는 패키지 import cv2 as cv from scipy import stats . boxplot . boxplot &#49324;&#50857;&#48277; . plt.boxplot() 괄호안에는 양적으로 나열된 리스트를 넣어주면 된다. 바로 시각화가 진행됨 . 괄호안에 ([y1,y2]) 이렇게 넣으면 나란히 그릴 수 있음 (리스트 처리) . y1=[75,75,76,76,77,77,79,79,79,98] y2=[76,76,77,77,78,78,80,80,80,81] plt.boxplot([y1,y2]) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7fc67e227c70&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e227fd0&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e240490&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e2407f0&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7fc67e235370&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e2356d0&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e240b50&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e240eb0&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7fc67e2278e0&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e240130&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7fc67e235a30&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e24d250&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7fc67e235d90&gt;, &lt;matplotlib.lines.Line2D at 0x7fc67e24d5b0&gt;], &#39;means&#39;: []} . . plotly . 그림(그래프)에 마우스를 올리면 상호작용하는 그림 . plotly.express 와 pandas 필요 . 구글에 검색하면 예시가 잘 나와있다! $ to$ plotly 활용!!! . A=pd.DataFrame({&#39;score&#39;:y1,&#39;class&#39;:[&#39;A&#39;]*len(y1)}) B=pd.DataFrame({&#39;score&#39;:y2,&#39;class&#39;:[&#39;B&#39;]*len(y2)}) . df=pd.concat([A,B],ignore_index=True) # 데이터프레임 붙이기 . fig=px.box(data_frame=df, x=&#39;class&#39;,y=&#39;score&#39;) #반응형 플랏 생성 () . HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) #생성된 플랏을 html로 바꾸어 블로그에서 읽을 수 있게 함, 패키지 필요 . . . . Histogram . X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림 . plt.hist() 함수 이용 . np.random.normal() :정규분포&gt; &gt; loc:평균 , scale:표준편차 , size:표본수 , bins= : 범위지정 bins를 늘리면 더 촘촘하게 그릴 수 있다. (정규분포에 더 가까워짐) . plt.hist(np.random.normal(loc=0, scale=1, size=1000000),bins=10) . (array([4.50000e+01, 1.51800e+03, 2.05060e+04, 1.22067e+05, 3.12406e+05, 3.43377e+05, 1.63298e+05, 3.38380e+04, 2.82500e+03, 1.20000e+02]), array([-4.86837268, -3.91680858, -2.96524449, -2.01368039, -1.06211629, -0.1105522 , 0.8410119 , 1.79257599, 2.74414009, 3.69570418, 4.64726828]), &lt;BarContainer object of 10 artists&gt;) . . seaborn . 깔끔하게 히스토그램을 그리는 패키지 . df를 입력으로 받는다 . np.random.seed(43052) #값이 안변하도록 시드설정 y1=np.random.normal(loc=0,scale=1,size=10000) #전북고 A반의 통계학 성적이라 생각하자. y2=np.random.normal(loc=0.5,scale=1,size=10000) #전북고 B반의 통계학 성적이라 생각하자. . A=pd.DataFrame({&#39;score&#39;:y1,&#39;class&#39;:[&#39;A&#39;]*len(y1)}) B=pd.DataFrame({&#39;score&#39;:y2,&#39;class&#39;:[&#39;B&#39;]*len(y2)}) df=pd.concat([A,B],ignore_index=True) . sns.histplot(df,x=&#39;score&#39;,hue=&#39;class&#39;) . &lt;AxesSubplot:xlabel=&#39;score&#39;, ylabel=&#39;Count&#39;&gt; . . plotnine . 인터랙티브 그래프를 위해서 plotly 홈페이지를 방문하여 적당한 코드를 가져온다. . ggplot(df)+geom_histogram(aes(x=&#39;score&#39;,color=&#39;class&#39;)) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: &#39;stat_bin()&#39; using &#39;bins = 84&#39;. Pick better value with &#39;binwidth&#39;. . &lt;ggplot: (8780644355470)&gt; . $ to$ 별로 알아보기가 힘들다 . color&#47484;fill&#47196; &#48148;&#45012;&#51468; , position&#51012; &#46041;&#46321;&#54616;&#44172; , alpha: &#53804;&#47749;&#46020; . ggplot(df)+geom_histogram(aes(x=&#39;score&#39;,fill=&#39;class&#39;),position=&#39;identity&#39;,alpha=0.5) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: &#39;stat_bin()&#39; using &#39;bins = 84&#39;. Pick better value with &#39;binwidth&#39;. . &lt;ggplot: (8743171077340)&gt; . . &#49328;&#51216;&#46020; (scatter plot) . 산점도:직교 좌표계(도표)를 이용해 좌표상의 점들을 표시함으로써 두 개 변수 간의 관계를 나타내는 그래프 방법이다. 산점도는 보통 $X$와 $Y$의 관계를 알고 싶을 경우 그린다. . 박스플랏, 히스토그램은 그림을 그리기 위해서 하나의 변수만 필요함; 산점도를 위해서는 두개의 변수가 필요함. | 두변수 $ to$ 두변수의 관계 | . &#47800;&#47924;&#44172;&#50752; &#53412; &#50696;&#49884; . x=[44,48,49,58,62,68,69,70,76,79] y=[159,160,162,165,167,162,165,175,165,172] . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc64abcd310&gt;] . 표본상관계수 계산 . x=np.array(x) y=np.array(y) . $r= sum_{i=1}^{n} left( frac{(x_i- bar{x})}{ sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2}} frac{(y_i- bar{y})}{ sqrt{ sum_{i=1}^{n}(y_i- bar{y})^2}} right) = sum_{i=1}^{n} tilde{x}_i tilde{y}_i $ . - 사실 $a,b$는 아래와 같이 계산할 수 있다. . $a= sqrt{n} times{ tt np.std(x)}$ . $b= sqrt{n} times{ tt np.std(y)}$ . a=np.sqrt(np.sum((x-np.mean(x))**2)) #괄호 왼쪽 분모 b=np.sqrt(np.sum((y-np.mean(y))**2)) #괄호 오른쪽 분모 n=len(x) np.sqrt(n)*np.std(x), np.sqrt(n)*np.std(y) . (36.58004920718397, 15.21840990379744) . ${ tt np.std(x)}= sqrt{ frac{1}{n} sum_{i=1}^{n}(x_i- bar{x})^2}$ | ${ tt np.std(y)}= sqrt{ frac{1}{n} sum_{i=1}^{n}(y_i- bar{y})^2}$ | . . Note: ${ tt np.std(x,ddof=1)}= sqrt{ frac{1}{n-1} sum_{i=1}^{n}(x_i- bar{x})^2}$ . - 이제 $( tilde{x}_i, tilde{y}_i)$를 그려보자. . xx= (x-np.mean(x))/a yy= (y-np.mean(y))/b plt.plot(xx,yy,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc64abad1c0&gt;] . plotly &#51060;&#50857; . fig=px.scatter(x=xx, y=yy) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) . . . $ tilde{x}_i$, $ tilde{y}_i$ 를 곱한값이 양수인것과 음수인것을 체크해보자. | 양수인쪽이 많은지 음수인쪽이 많은지 생각해보자. | $r= sum_{i=1}^{n} tilde{x}_i tilde{y}_i$ 의 부호는? $ to$ 양수인 쪽이 훨씬 많다축=0 기준으로 사분면을 그어 양수쪽 기울기일지 그 수를 확인 . 1,3분면 :양수 . | . - 상관계수는 두 변수의 관계를 설명하기에 부적절하다. . 상관계수는 1번그림과 같이 두 변수가 선형관계에 있을때 그 정도를 나타내는 통계량일뿐이다. | 선형관계가 아닌것처럼 보이는 자료에서는 상관계수를 계산할수는 있겠으나 의미가 없다. | . - 교훈2: 기본적인 통계량들은 실제자료를 분석하기에 부적절할수 있다. (=통계량은 적절한 가정이 동반되어야 의미가 있다) . . Note: 통계학자는 (1) 적절한 가정을 수학적인 언어로 정의하고 (2) 그 가정하에서 통계량이 의미있다는 것을 증명해야 한다. (3) 그리고 그 결과를 시각화하여 설득한다. . . Anscombe&#39;s quartet . - 교과서에 나오는 그림임. . - 교훈: 데이터를 분석하기 전에 항상 시각화를 하라. . x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5] y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68] y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74] y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73] x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8] y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89] . _, axs = plt.subplots(2,2) axs[0,0].plot(x,y1,&#39;o&#39;) axs[0,1].plot(x,y2,&#39;o&#39;) axs[1,0].plot(x,y3,&#39;o&#39;) axs[1,1].plot(x4,y4,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7a1370fb80&gt;] . - 상관계수를 잠깐 복습해보자. . 상관계수는 -1 ~ 1 사이의 값을 가진다. (코쉬슈바르츠 부등식을 사용하여 증명가능) | 완전한 직선이라면 상관계수가 1 또는 -1이다. | 상관계수가 1에 가까우면 양의 상관관계에 있다고 말하고 -1에 가까우면 음의 상관관계에 있다고 말한다. | . - 의문: 자료의 모양이 직선모양에 가까우면 상관계수가 큰것이 맞나? . $x,y$ 값이 모두 큰 하나의 관측치가 상관계수값을 키울 수 있지 않나? | . - 상관계수가 좋은것은 맞나? (=상관계수는 두 변수의 관계를 설명하기에 충분히 적절한 통계량인가?) . np.corrcoef([x,y1,y2,y3]) . array([[1. , 0.81642052, 0.81623651, 0.81628674], [0.81642052, 1. , 0.7500054 , 0.46871668], [0.81623651, 0.7500054 , 1. , 0.58791933], [0.81628674, 0.46871668, 0.58791933, 1. ]]) . np.corrcoef([x4,y4]) . array([[1. , 0.81652144], [0.81652144, 1. ]]) . - 위의 4개의 그림에 대한 상관계수는 모두 같다. (0.81652) . - 상관계수는 두 변수의 관계를 설명하기에 부적절하다. . 상관계수는 1번그림과 같이 두 변수가 선형관계에 있을때 그 정도를 나타내는 통계량일뿐이다. | 선형관계가 아닌것처럼 보이는 자료에서는 상관계수를 계산할수는 있겠으나 의미가 없다. | . - 교훈2: 기본적인 통계량들은 실제자료를 분석하기에 부적절할수 있다. (=통계량은 적절한 가정이 동반되어야 의미가 있다) . . Note: 통계학자는 (1) 적절한 가정을 수학적인 언어로 정의하고 (2) 그 가정하에서 통계량이 의미있다는 것을 증명해야 한다. (3) 그리고 그 결과를 시각화하여 설득한다. . . &#48177;&#51648;&#50640;&#49436; &#44536;&#47000;&#54532; &#44536;&#47532;&#44592; . - 전략: 그림을 만들고 (도화지를 준비) $ to$ 액시즈를 만들고 (네모틀을 만든다) $ to$ 액시즈에 그림을 그린다. (.plot()을 이용) . fig = plt.figure() # 도화지를 준비한다. . &lt;Figure size 432x288 with 0 Axes&gt; . fig # 현재 도화지상태를 체크 , 아무것도 없음 . &lt;Figure size 432x288 with 0 Axes&gt; . fig.axes # 현재 네모틀 상태를 체크 , 아무것도 없음 . [] . fig.add_axes([0,0,1,1]) # 도화지안에 (0,0) 위치에 길이가 (1,1) 인 네모틀을 만든다. . &lt;matplotlib.axes._axes.Axes at 0x7feb7c4d5370&gt; . fig.axes # 현재 네모틀 상태를 체크 --&gt; 네모틀이 하나 있음. . [&lt;matplotlib.axes._axes.Axes at 0x7feb7c4d5370&gt;] . fig # 현재도화지 상태 체크 --&gt; 도화지에 (하나의) 네모틀이 잘 들어가 있음 . axs1=fig.axes[0] ## 첫번째 액시즈 지정 . axs1.plot([1,2,3],&#39;or&#39;) # 첫번쨰 액시즈에 접근하여 그림을 그림 . [&lt;matplotlib.lines.Line2D at 0x7feb7bc4caf0&gt;] . fig #현재 도화지 상태 체크 --&gt; 그림이 잘 그려짐 . - 액시즈추가 . fig.add_axes([1,0,1,1]) #도화지 안에 1,0 자리에 1x1 네모 추가로 만들겠다. . &lt;matplotlib.axes._axes.Axes at 0x7feb7bbfaee0&gt; . fig.axes . [&lt;matplotlib.axes._axes.Axes at 0x7feb7c4d5370&gt;, &lt;matplotlib.axes._axes.Axes at 0x7feb7bbfaee0&gt;] . fig #잘 추가 되었음 . axs2=fig.axes[1] ## 두번째 액시즈 지정 . axs2.plot([1,2,3],&#39;ok&#39;) ## 두번째 액시즈에 그림그림 . [&lt;matplotlib.lines.Line2D at 0x7feb7bbfad60&gt;] . fig ## 현재 도화지 체크 . axs1.plot([1,2,3],&#39;--&#39;) ### 액시즈1에 점선추가 . [&lt;matplotlib.lines.Line2D at 0x7feb7bbd8f10&gt;] . fig ## 현재 도화지 체크 . &#44536;&#47536; &#47112;&#51060;&#50500;&#50883;&#51012; &#48320;&#44221; . fig.axes . [&lt;matplotlib.axes._axes.Axes at 0x7feb7c4d5370&gt;, &lt;matplotlib.axes._axes.Axes at 0x7feb7bbfaee0&gt;] . fig = plt.figure() #도화지 초기화 . &lt;Figure size 432x288 with 0 Axes&gt; . fig.axes . [] . fig.subplots(1,2) # 서브플랏을 이용해 나란한 그래프 (1행 2열) 생성 . array([&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;], dtype=object) . fig.axes . [&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;] . ax1,ax2 = fig.axes # ax1이 첫번째 ax2가 두번째로 저장(튜플로) . ax1.plot([1,2,3],&#39;or&#39;) ax2.plot([1,2,3],&#39;ob&#39;) . [&lt;matplotlib.lines.Line2D at 0x7feb7baee8b0&gt;] . fig #뭔가 좁다 . fig.set_figwidth(10) #도화지 늘리기 fig . ax1.plot([1,2,3],&#39;--&#39;) #계속 업데이트 가능 . [&lt;matplotlib.lines.Line2D at 0x7feb7baee040&gt;] . &#55176;&#49828;&#53664;&#44536;&#47016; &#49900;&#54868; . np.random.seed(43052) x2=np.random.uniform(low=-1,high=1,size=100000) y2=np.random.uniform(low=-1,high=1,size=100000) radius = x2**2+y2**2 x3=x2[radius&lt;1] y3=y2[radius&lt;1] . h=0.05 _,axs= plt.subplots(2,2) axs[0,0].hist(y2[(x2&gt; -h )*(x2&lt; h )]) # &#39;*&#39; 는 and axs[0,0].set_xlim(-1.1,1.1) #축 범위 조절 axs[0,1].hist(y2[(x2&gt; 0.9-h )*(x2&lt; 0.9+h )]) axs[0,1].set_xlim(-1.1,1.1) axs[1,0].hist(y3[(x3&gt; -h )*(x3&lt; h )]) axs[1,0].set_xlim(-1.1,1.1) axs[1,1].hist(y3[(x3&gt; 0.9-h )*(x3&lt; 0.9+h )]) axs[1,1].set_xlim(-1.1,1.1) . (-1.1, 1.1) . &#48128;&#46020;&#54364;&#54788; . np.random.seed(43052) x4=np.random.normal(size=10000) y4=np.random.normal(size=10000) . plt.plot(x4,y4,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7feb7b54eac0&gt;] . - 디자인적인 측면에서 보면 올바른 시각화라 볼 수 없다. (이 그림이 밀도를 왜곡시킨다) . - 아래와 같은 그림이 더 우수하다. (밀도를 표현하기 위해 투명도라는 개념을 도입) . plt.scatter(x4,y4,alpha=0.01) . &lt;matplotlib.collections.PathCollection at 0x7ff0f352d610&gt; . np.corrcoef(x4,y4) . array([[ 1. , -0.01007718], [-0.01007718, 1. ]]) . . maplotlib + seaborn &#47484; &#51060;&#50857;&#54620; &#49328;&#51216;&#46020; . x=[44,48,49,58,62,68,69,70,76,79] # 몸무게 y=[159,160,162,165,167,162,165,175,165,172] #키 g=&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;M&#39;,&#39;M&#39;,&#39;M&#39;,&#39;M&#39;,&#39;M&#39; . plt.plot(x,y,&#39;o&#39;) #matplotlib . [&lt;matplotlib.lines.Line2D at 0x7fc64ab3d6d0&gt;] . sns.scatterplot(x=x,y=y,hue=g) # 씨본 . &lt;AxesSubplot:&gt; . fig, (ax1,ax2) = plt.subplots(1,2) ax1.plot(x,y,&#39;o&#39;) sns.scatterplot(x=x,y=y,hue=g,ax=ax2) # 겹쳐그리기 , ax1,ax2 이라고 지정하는 함수를 각각 추가 # 씨본과 맷플랏을 동시에 그릴수 있음 . &lt;AxesSubplot:&gt; . ax1.set_title(&#39;matplotlib&#39;) ax2.set_title(&#39;seaborn&#39;) #그래프각각의 제목 지정 fig.set_figwidth(8) #그래프의 사이즈 늘리기 fig . plt.plot([1,2,3],[3,4,5],&#39;or&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc64a83ba30&gt;] . sns.set_theme() #같은 코드이지만 씨본을 통해 비주얼라이징을 더 예쁘게 변화 . plt.plot([1,2,3],[3,4,5],&#39;or&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc64a700490&gt;] . . &#51221;&#44508;&#48516;&#54252;&#52628;&#51221;, &#48128;&#46020;&#52628;&#51221;&#44257;&#49440; . np.random.seed(43052) x=np.random.normal(size=1000,loc=2,scale=1.5) #정규분포 생성 from scipy import stats #t분포 패키지 y=stats.t.rvs(10,size=1000) #t분포 생성 . - 이 자료가 정규분포를 따르는지 어떻게 체크할 수 있을까? . plt.hist(x) . (array([ 10., 24., 99., 176., 232., 222., 165., 53., 16., 3.]), array([-2.44398446, -1.53832428, -0.6326641 , 0.27299608, 1.17865626, 2.08431645, 2.98997663, 3.89563681, 4.80129699, 5.70695718, 6.61261736]), &lt;BarContainer object of 10 artists&gt;) . - 종모양이므로 정규분포인듯 하다. . - 밀도추정곡선이 있었으면 좋겠다. (KDE로 추정) $ to$ seaborn을 활용하여 그려보자. . fig, (ax1,ax2) = plt.subplots(1,2) sns.histplot(x,kde=True,ax=ax1) sns.histplot(y,kde=True,ax=ax2) #kde 가 곡선 추가 옵션 . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . 둘다 종모양이다!! . scale을 표준으로 바꿔서 비교해본다 . xx= (x-np.mean(x)) / np.std(x,ddof=1) yy= (y-np.mean(y)) / np.std(y,ddof=1) fig, (ax1,ax2) = plt.subplots(1,2) sns.histplot(xx,kde=True,ax=ax1) sns.histplot(yy,kde=True,ax=ax2) . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . 정확한 분포를 위해 박스플랏이랑 같이 비교 . xx= (x-np.mean(x)) / np.std(x,ddof=1) yy= (y-np.mean(y)) / np.std(y,ddof=1) fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) fig.tight_layout() ax1.boxplot(xx) sns.histplot(xx,kde=True,ax=ax2) ax3.boxplot(yy) sns.histplot(yy,kde=True,ax=ax4) . &lt;AxesSubplot:ylabel=&#39;Count&#39;&gt; . t분포는 종모양이나 정규분포는 아니다. $ to$ t분포의 꼬리가 더 두껍다 . . qqplot . 분포를 특정하기에 좋은 시각화 . np.random.seed(43052) x=np.random.normal(size=1000,loc=2,scale=1.5) y=stats.t.rvs(df=10,size=1000)/np.sqrt(10/8)*1.5 + 2 . fig, ax =plt.subplots(2,3) . (ax1,ax2,ax3), (ax4,ax5,ax6) = ax . sns.boxplot(x,ax=ax1) #box sns.histplot(x,kde=True,ax=ax2) #hist _ = stats.probplot(x,plot=ax3) #qqplot sns.boxplot(y,ax=ax4) sns.histplot(y,kde=True,ax=ax5) _ = stats.probplot(y,plot=ax6) #그림 사이즈조정 fig.set_figwidth(10) fig.set_figheight(8) fig.tight_layout() fig . /home/kdj/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. /home/kdj/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. . t분포: 푸른점들이 대체로 붉은선위에 놓여있는듯 하지만 양끝단에서는 그렇지 않다. (중앙부근은 정규분포와 비슷하지만, 꼬리부분은 정규분포와 확실히 다르다) | 왼쪽꼬리: 이론적으로 나와야 할 값보다 더 작은값이 실제로 관측됨 | 오른쪽꼬리: 이론적으로 나와야 할 값보다 더 큰값이 실제로 관측됨 | 해석: 이 분포는 정규분포보다 두꺼운 꼬리를 가진다. | . . &#48516;&#50948;&#49688;&#47484; &#44396;&#54616;&#45716; &#45796;&#50577;&#54620; &#48169;&#48277; . m=[i/1000 for i in np.arange(1000)+1] #리스트 컴프리헨션 # np.arange(1000) : 0~999 . $m= big { frac{i}{1000}: i in {1,2,3, dots,1000 } big }= big { frac{1}{1000}, frac{2}{1000}, dots, frac{1000}{1000} big }$ | . m[:5] . [0.001, 0.002, 0.003, 0.004, 0.005] . range? . Init signature: range(self, /, *args, **kwargs) Docstring: range(stop) -&gt; range object range(start, stop[, step]) -&gt; range object Return an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. range(i, j) produces i, i+1, i+2, ..., j-1. start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement). Type: type Subclasses: . range . 처음부터 정수 시퀀스를 생성하는 개체 반환(포함) 단계별로 정지(정지)합니다. . 범위(i, j)는 i, i+1, i+2, ..., j-1을 생성합니다. . start 기본값은 0이고 stop은 생략됩니다! range(4)는 0, 1, 2, 3을 생성합니다. . 이들은 정확히 4개의 원소 목록에 대한 유효한 지수이다. . 단계가 지정되면 증분(또는 감소)을 지정합니다. . stats.norm.ppf? . Signature: stats.norm.ppf(q, *args, **kwds) Docstring: Percent point function (inverse of `cdf`) at q of the given RV. Parameters - q : array_like lower tail probability arg1, arg2, arg3,... : array_like The shape parameter(s) for the distribution (see docstring of the instance object for more information) loc : array_like, optional location parameter (default=0) scale : array_like, optional scale parameter (default=1) Returns - x : array_like quantile corresponding to the lower tail probability q. File: ~/anaconda3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py Type: method . stats.norm.ppf() . 주어진 RV의 q에서 백분율 포인트 함수(&#39;cdf&#39;의 역) . Parameters . q : array_like 낮은 꼬리 확률 arg1, arg2, arg3,... : array_like 분포에 대한 형상 모수(의 문서 문자열 참조) 자세한 내용을 보려면 인스턴스 개체) loc : array_like, 옵션 위치 매개 변수(기본값=0) scale : array_like, 옵션 척도 모수(기본값=1) . return . x : array_like = 아래쪽 꼬리 확률 q에 해당하는 분위수입니다. . q=[] for i in range(len(m)): q=q+[stats.norm.ppf(m[i])] q[:5] . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . q=[stats.norm.ppf(m[i]) for i in range(len(m))] q[:5] . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . q=list(map(stats.norm.ppf, m)) q[:5] . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . stats.norm.ppf(m)[:5] . array([-3.09023231, -2.87816174, -2.74778139, -2.65206981, -2.5758293 ]) . . &#51339;&#51008; &#49884;&#44033;&#54868; &#54616;&#44592; . - 왜 우수한 그래프일까? . 자료를 파악하는 기법은 최근까지도 산점도, 막대그래프, 라인플랏에 의존 | 이러한 플랏의 단점은 고차원의 자료를 분석하기 어렵다는 것임 | 미나드는 여러그램을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함. | . &#49884;&#44033;&#54868; &#50696;&#51228; . x=[44,48,49,58,62,68,69,70,76,79] ## 몸무게 y=[159,160,162,165,167,162,165,175,165,172] ## 키 g= &#39;f&#39;,&#39;f&#39;,&#39;f&#39;,&#39;f&#39;,&#39;m&#39;,&#39;f&#39;,&#39;m&#39;,&#39;m&#39;,&#39;m&#39;,&#39;m&#39; df=pd.DataFrame({&#39;w&#39;:x,&#39;h&#39;:y,&#39;g&#39;:g}) . df . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 4 62 | 167 | m | . 5 68 | 162 | f | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . - 미나드의 접근방법 . sns.scatterplot(data=df,x=&#39;w&#39;,y=&#39;h&#39;,hue=&#39;g&#39;) . &lt;AxesSubplot:xlabel=&#39;w&#39;, ylabel=&#39;h&#39;&gt; . - 일반적인 사람들 (보통 색깔을 사용할 생각을 못한다.) . figs = sns.FacetGrid(df,col=&#39;g&#39;) figs.map (sns.scatterplot,&#39;w&#39;,&#39;h&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f00b6fa36d0&gt; . - 생각보다 데이터가 정리된 형태에 따라서 시각화에 대한 사고방식이 달라진다. 아래와 같은 자료를 받았다고 하자. . df1=df.query(&quot;g ==&#39;f&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 여성.csv df2=df.query(&quot;g ==&#39;m&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 남성.csv . df1 . w h . 0 44 | 159 | . 1 48 | 160 | . 2 49 | 162 | . 3 58 | 165 | . 5 68 | 162 | . df2 . w h . 4 62 | 167 | . 6 69 | 165 | . 7 70 | 175 | . 8 76 | 165 | . 9 79 | 172 | . - 데이터프레임을 바꿀 생각을 하는게 쉽지 않다. . (방법1) . df1[&#39;g&#39;]= &#39;f&#39; . df1 . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 5 68 | 162 | f | . df2[&#39;g&#39;]= &#39;m&#39; . df2 . w h g . 4 62 | 167 | m | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . pd.concat([df1,df2]) . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 5 68 | 162 | f | . 4 62 | 167 | m | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . (방법2) . df1=df.query(&quot;g ==&#39;f&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 여성.csv df2=df.query(&quot;g ==&#39;m&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 남성.csv . pd.concat([df1,df2],keys=[&#39;f&#39;,&#39;m&#39;]).reset_index().iloc[:,[0,2,3]].rename(columns={&#39;level_0&#39;:&#39;g&#39;}) . g w h . 0 f | 44 | 159 | . 1 f | 48 | 160 | . 2 f | 49 | 162 | . 3 f | 58 | 165 | . 4 f | 68 | 162 | . 5 m | 62 | 167 | . 6 m | 69 | 165 | . 7 m | 70 | 175 | . 8 m | 76 | 165 | . 9 m | 79 | 172 | . - 어려운점: . (1) 센스가 없어서 색깔을 넣어서 그룹을 구분할 생각을 못함 | (2) 변형해야할 데이터를 생각못함 | (3) 데이터를 변형할 생각을 한다고 해도 변형하는 실제적인 코드를 구현할 수 없음 (그래서 엑셀을 킨다..) (1) 기획력부족 -&gt; 훌륭한 시각화를 많이 볼것 | (2) 데이터프레임에 대한 이해도가 부족 -&gt; tidydata에 대한 개념 | (3) 프로그래밍 능력 부족 -&gt; 코딩공부열심히.. | . | . - 목표: . (2) 어떠한 데이터 형태로 변형해야하는가? | (3) 그러한 데이터 형태로 바꾸기 위한 pandas 숙련도 | .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/11/07/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%8B%9C%ED%97%98%EA%B3%B5%EB%B6%801.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/11/07/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94%EC%8B%9C%ED%97%98%EA%B3%B5%EB%B6%801.html",
            "date": " • Nov 7, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "데이터시각화 10/27 강의 정리",
            "content": "import pandas as pd import numpy as np . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv&#39;) . df #항공 정보 . df.columns #열이름 나열 . https://github.com/PacktPublishing/Pandas-Cookbook/blob/master/data/descriptions/flights_description.csv | . 칼럼 설명 . groupby . - 데이터프레임을 여러개의 서브데이터프레임으로 나누는 기능 . - 단독으로 쓸 이유는 별로 없다. $ to$ 그룹을 나누고 어떠한 &quot;연산&quot;을 하기 위함 . df.groupby(by=&#39;AIRLINE&#39;) . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f3368772fa0&gt; . 데이터프레임을 각 항공사 별로 나눔 | . - 확인 . grouped_df = df.groupby(by=&#39;AIRLINE&#39;) . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv&#39;) . grouped_df.groups . 너무 보기 힘듬 | . - 보기좋은 형태로 확인 . list(grouped_df.groups) #항공사안 카테고리의 리스트 출력 . [&#39;AA&#39;, &#39;AS&#39;, &#39;B6&#39;, &#39;DL&#39;, &#39;EV&#39;, &#39;F9&#39;, &#39;HA&#39;, &#39;MQ&#39;, &#39;NK&#39;, &#39;OO&#39;, &#39;UA&#39;, &#39;US&#39;, &#39;VX&#39;, &#39;WN&#39;] . grouped_df.get_group(&#39;AA&#39;) #항공사별로 나눈 것의 특정 카테고리를 df로 보기 . MONTH DAY WEEKDAY AIRLINE ORG_AIR DEST_AIR SCHED_DEP DEP_DELAY AIR_TIME DIST SCHED_ARR ARR_DELAY DIVERTED CANCELLED . 3 1 | 1 | 4 | AA | DFW | DCA | 1555 | 7.0 | 126.0 | 1192 | 1935 | -7.0 | 0 | 0 | . 6 1 | 1 | 4 | AA | DFW | MSY | 1250 | 84.0 | 64.0 | 447 | 1410 | 83.0 | 0 | 0 | . 8 1 | 1 | 4 | AA | ORD | STL | 1845 | -5.0 | 44.0 | 258 | 1950 | -5.0 | 0 | 0 | . 15 1 | 1 | 4 | AA | DEN | DFW | 1445 | -6.0 | 93.0 | 641 | 1745 | 4.0 | 0 | 0 | . 26 1 | 1 | 4 | AA | LAX | AUS | 1430 | 33.0 | 157.0 | 1242 | 1925 | 41.0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 58470 12 | 31 | 4 | AA | DFW | FAT | 1020 | -3.0 | 196.0 | 1313 | 1156 | -2.0 | 0 | 0 | . 58475 12 | 31 | 4 | AA | IAH | CLT | 710 | 1.0 | 113.0 | 912 | 1037 | -12.0 | 0 | 0 | . 58476 12 | 31 | 4 | AA | DFW | TPA | 1020 | -3.0 | 121.0 | 929 | 1340 | -6.0 | 0 | 0 | . 58479 12 | 31 | 4 | AA | DFW | ELP | 1200 | 3.0 | 94.0 | 551 | 1250 | 13.0 | 0 | 0 | . 58487 12 | 31 | 4 | AA | SFO | DFW | 515 | 5.0 | 166.0 | 1464 | 1045 | -19.0 | 0 | 0 | . 8900 rows × 14 columns . for g in grouped_df.groups: . print(g) display(grouped_df.get_group(g)) $ to$ 리스트 카테고리들 전부를 확인 . AIRLINE&#51012; &#44592;&#51456;&#51004;&#47196; &#45936;&#51060;&#53552;&#54532;&#47112;&#51076;&#51012; &#45208;&#45572;&#44256; $ to$ ARR_DELAY&#50640; mean&#54632;&#49688;&#47484; &#51201;&#50857;: (AIRLINE $ to$ {ARR_DELAY: mean}) . - 방법1 (기본, agg와 딕셔너리 이용) . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:&#39;mean&#39;}) . ARR_DELAY . AIRLINE . AA 5.542661 | . AS -0.833333 | . B6 8.692593 | . DL 0.339691 | . EV 7.034580 | . F9 13.630651 | . HA 4.972973 | . MQ 6.860591 | . NK 18.436070 | . OO 7.593463 | . UA 7.765755 | . US 1.681105 | . VX 5.348884 | . WN 6.397353 | . - 방법2 ($ star star star$) . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:np.mean}) . ARR_DELAY . AIRLINE . AA 5.542661 | . AS -0.833333 | . B6 8.692593 | . DL 0.339691 | . EV 7.034580 | . F9 13.630651 | . HA 4.972973 | . MQ 6.860591 | . NK 18.436070 | . OO 7.593463 | . UA 7.765755 | . US 1.681105 | . VX 5.348884 | . WN 6.397353 | . - 방법3 , 나눠진 df에서 ARR_DELAY 뽑고, 평균함수적용 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(&#39;mean&#39;) . AIRLINE AA 5.542661 AS -0.833333 B6 8.692593 DL 0.339691 EV 7.034580 F9 13.630651 HA 4.972973 MQ 6.860591 NK 18.436070 OO 7.593463 UA 7.765755 US 1.681105 VX 5.348884 WN 6.397353 Name: ARR_DELAY, dtype: float64 . - 방법4 ($ star$) . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(np.mean) . AIRLINE AA 5.542661 AS -0.833333 B6 8.692593 DL 0.339691 EV 7.034580 F9 13.630651 HA 4.972973 MQ 6.860591 NK 18.436070 OO 7.593463 UA 7.765755 US 1.681105 VX 5.348884 WN 6.397353 Name: ARR_DELAY, dtype: float64 . - 방법5 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].mean() . AIRLINE AA 5.542661 AS -0.833333 B6 8.692593 DL 0.339691 EV 7.034580 F9 13.630651 HA 4.972973 MQ 6.860591 NK 18.436070 OO 7.593463 UA 7.765755 US 1.681105 VX 5.348884 WN 6.397353 Name: ARR_DELAY, dtype: float64 . - 방법2와 방법4는 사용자정의 함수를 쓸 수 있다는 장점이 있음 . - 방법6 . def f(x): return -np.mean(x) #사용자정의함수 지정 . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:f}) . ARR_DELAY . AIRLINE . AA -5.542661 | . AS 0.833333 | . B6 -8.692593 | . DL -0.339691 | . EV -7.034580 | . F9 -13.630651 | . HA -4.972973 | . MQ -6.860591 | . NK -18.436070 | . OO -7.593463 | . UA -7.765755 | . US -1.681105 | . VX -5.348884 | . WN -6.397353 | . - 방법7 . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;:lambda x: -np.mean(x)}) . ARR_DELAY . AIRLINE . AA -5.542661 | . AS 0.833333 | . B6 -8.692593 | . DL -0.339691 | . EV -7.034580 | . F9 -13.630651 | . HA -4.972973 | . MQ -6.860591 | . NK -18.436070 | . OO -7.593463 | . UA -7.765755 | . US -1.681105 | . VX -5.348884 | . WN -6.397353 | . - 방법8 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(lambda x: -np.mean(x)) . AIRLINE AA -5.542661 AS 0.833333 B6 -8.692593 DL -0.339691 EV -7.034580 F9 -13.630651 HA -4.972973 MQ -6.860591 NK -18.436070 OO -7.593463 UA -7.765755 US -1.681105 VX -5.348884 WN -6.397353 Name: ARR_DELAY, dtype: float64 . &#51077;&#47141;&#51060; &#50668;&#47084;&#44060;&#51064; &#49324;&#50857;&#51088; &#51221;&#51032; &#54632;&#49688;&#51032; &#49324;&#50857; . def f(x,y): return np.mean(x)**y . - 방법1 . df.groupby(by=&#39;AIRLINE&#39;)[&#39;ARR_DELAY&#39;].agg(f,2) . AIRLINE AA 30.721086 AS 0.694444 B6 75.561166 DL 0.115390 EV 49.485310 F9 185.794656 HA 24.730460 MQ 47.067715 NK 339.888677 OO 57.660681 UA 60.306954 US 2.826113 VX 28.610564 WN 40.926120 Name: ARR_DELAY, dtype: float64 . - 방법2 . df.groupby(by=&#39;AIRLINE&#39;).agg({&#39;ARR_DELAY&#39;: lambda x: f(x,2)}) . ARR_DELAY . AIRLINE . AA 30.721086 | . AS 0.694444 | . B6 75.561166 | . DL 0.115390 | . EV 49.485310 | . F9 185.794656 | . HA 24.730460 | . MQ 47.067715 | . NK 339.888677 | . OO 57.660681 | . UA 60.306954 | . US 2.826113 | . VX 28.610564 | . WN 40.926120 | . &#54876;&#50857; . AIRLINE,WEEKDAY $ to$ {CANCELLED: sum} . - 방법1~5 . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:&#39;sum&#39;}) . CANCELLED . AIRLINE WEEKDAY . AA 1 41 | . 2 9 | . 3 16 | . 4 20 | . 5 18 | . ... ... ... | . WN 3 18 | . 4 10 | . 5 7 | . 6 10 | . 7 7 | . 98 rows × 1 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:np.sum}) . CANCELLED . AIRLINE WEEKDAY . AA 1 41 | . 2 9 | . 3 16 | . 4 20 | . 5 18 | . ... ... ... | . WN 3 18 | . 4 10 | . 5 7 | . 6 10 | . 7 7 | . 98 rows × 1 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[&#39;CANCELLED&#39;].agg(&#39;sum&#39;) . AIRLINE WEEKDAY AA 1 41 2 9 3 16 4 20 5 18 .. WN 3 18 4 10 5 7 6 10 7 7 Name: CANCELLED, Length: 98, dtype: int64 . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[&#39;CANCELLED&#39;].agg(np.sum) . AIRLINE WEEKDAY AA 1 41 2 9 3 16 4 20 5 18 .. WN 3 18 4 10 5 7 6 10 7 7 Name: CANCELLED, Length: 98, dtype: int64 . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[&#39;CANCELLED&#39;].sum() . AIRLINE WEEKDAY AA 1 41 2 9 3 16 4 20 5 18 .. WN 3 18 4 10 5 7 6 10 7 7 Name: CANCELLED, Length: 98, dtype: int64 . AIRLINE,WEEKDAY $ to$ {CANCELLED: sum, mean} , {DIVERTED: sum, mean} . - 방법 1~4 (5번은 쓸 수 없다) . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:[&#39;sum&#39;,&#39;mean&#39;],&#39;DIVERTED&#39;:[&#39;sum&#39;,&#39;mean&#39;]}) . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:[np.sum,np.mean],&#39;DIVERTED&#39;:[np.sum,np.mean]}) . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[[&#39;CANCELLED&#39;,&#39;DIVERTED&#39;]].agg([&#39;sum&#39;,&#39;mean&#39;]) #컬럼을 리스트로 전달 . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;])[[&#39;CANCELLED&#39;,&#39;DIVERTED&#39;]].agg([np.sum,np.mean]) . CANCELLED DIVERTED . sum mean sum mean . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 6 | 0.004699 | . 2 9 | 0.007341 | 2 | 0.001631 | . 3 16 | 0.011949 | 2 | 0.001494 | . 4 20 | 0.015004 | 5 | 0.003751 | . 5 18 | 0.014151 | 1 | 0.000786 | . ... ... ... | ... | ... | ... | . WN 3 18 | 0.014118 | 2 | 0.001569 | . 4 10 | 0.007911 | 4 | 0.003165 | . 5 7 | 0.005828 | 0 | 0.000000 | . 6 10 | 0.010132 | 3 | 0.003040 | . 7 7 | 0.006066 | 3 | 0.002600 | . 98 rows × 4 columns . . AIRLINE,WEEKDAY $ to$ {CANCELLED: sum, mean, size} , {AIR_TIME: mean,var} . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]).agg({&#39;CANCELLED&#39;:[&#39;sum&#39;,&#39;mean&#39;,&#39;size&#39;],&#39;AIR_TIME&#39;:[&#39;mean&#39;,&#39;var&#39;]}) #딕셔너리 활용 . CANCELLED AIR_TIME . sum mean size mean var . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 1277 | 147.610569 | 5393.806723 | . 2 9 | 0.007341 | 1226 | 143.851852 | 5359.890719 | . 3 16 | 0.011949 | 1339 | 144.514005 | 5378.854539 | . 4 20 | 0.015004 | 1333 | 141.124618 | 4791.524627 | . 5 18 | 0.014151 | 1272 | 145.430966 | 5884.592076 | . ... ... ... | ... | ... | ... | ... | . WN 3 18 | 0.014118 | 1275 | 104.219920 | 2901.873447 | . 4 10 | 0.007911 | 1264 | 107.200800 | 2966.568935 | . 5 7 | 0.005828 | 1201 | 107.893635 | 3268.717093 | . 6 10 | 0.010132 | 987 | 109.247433 | 3152.753719 | . 7 7 | 0.006066 | 1154 | 107.602273 | 3183.126889 | . 98 rows × 5 columns . df.groupby(by=[&#39;AIRLINE&#39;,&#39;WEEKDAY&#39;]) .agg({&#39;CANCELLED&#39;:[np.sum,np.mean,len],&#39;AIR_TIME&#39;:[np.mean,lambda x: np.std(x,ddof=1)**2]}) . CANCELLED AIR_TIME . sum mean len mean &lt;lambda_0&gt; . AIRLINE WEEKDAY . AA 1 41 | 0.032106 | 1277 | 147.610569 | 5393.806723 | . 2 9 | 0.007341 | 1226 | 143.851852 | 5359.890719 | . 3 16 | 0.011949 | 1339 | 144.514005 | 5378.854539 | . 4 20 | 0.015004 | 1333 | 141.124618 | 4791.524627 | . 5 18 | 0.014151 | 1272 | 145.430966 | 5884.592076 | . ... ... ... | ... | ... | ... | ... | . WN 3 18 | 0.014118 | 1275 | 104.219920 | 2901.873447 | . 4 10 | 0.007911 | 1264 | 107.200800 | 2966.568935 | . 5 7 | 0.005828 | 1201 | 107.893635 | 3268.717093 | . 6 10 | 0.010132 | 987 | 109.247433 | 3152.753719 | . 7 7 | 0.006066 | 1154 | 107.602273 | 3183.126889 | . 98 rows × 5 columns . grouping by continuous variable &#50672;&#49549;&#54805;&#48320;&#49688; &#44536;&#47353;&#54868; . df . MONTH DAY WEEKDAY AIRLINE ORG_AIR DEST_AIR SCHED_DEP DEP_DELAY AIR_TIME DIST SCHED_ARR ARR_DELAY DIVERTED CANCELLED . 0 1 | 1 | 4 | WN | LAX | SLC | 1625 | 58.0 | 94.0 | 590 | 1905 | 65.0 | 0 | 0 | . 1 1 | 1 | 4 | UA | DEN | IAD | 823 | 7.0 | 154.0 | 1452 | 1333 | -13.0 | 0 | 0 | . 2 1 | 1 | 4 | MQ | DFW | VPS | 1305 | 36.0 | 85.0 | 641 | 1453 | 35.0 | 0 | 0 | . 3 1 | 1 | 4 | AA | DFW | DCA | 1555 | 7.0 | 126.0 | 1192 | 1935 | -7.0 | 0 | 0 | . 4 1 | 1 | 4 | WN | LAX | MCI | 1720 | 48.0 | 166.0 | 1363 | 2225 | 39.0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 58487 12 | 31 | 4 | AA | SFO | DFW | 515 | 5.0 | 166.0 | 1464 | 1045 | -19.0 | 0 | 0 | . 58488 12 | 31 | 4 | F9 | LAS | SFO | 1910 | 13.0 | 71.0 | 414 | 2050 | 4.0 | 0 | 0 | . 58489 12 | 31 | 4 | OO | SFO | SBA | 1846 | -6.0 | 46.0 | 262 | 1956 | -5.0 | 0 | 0 | . 58490 12 | 31 | 4 | WN | MSP | ATL | 525 | 39.0 | 124.0 | 907 | 855 | 34.0 | 0 | 0 | . 58491 12 | 31 | 4 | OO | SFO | BOI | 859 | 5.0 | 73.0 | 522 | 1146 | -1.0 | 0 | 0 | . 58492 rows × 14 columns . - 목표: DIST를 적당한 구간으로 나누어 카테고리화 하고 그것을 바탕으로 groupby를 수행하자. . df.DIST.hist() #앞쪽에 몰려있음 . &lt;AxesSubplot:&gt; . df.DIST.describe() #대략적 통계값 산출 . count 58492.000000 mean 872.900072 std 624.996805 min 67.000000 25% 391.000000 50% 690.000000 75% 1199.000000 max 4502.000000 Name: DIST, dtype: float64 . - 구간을 아래와 같이 설정한다. . bins=[-np.inf, 400, 700, 1200, np.inf] # -무한대 ~400 ~700 ~1200 ~무한대 . - pd.cut()을 이용하여 각 구간의 observation을 카테고리화(mapping) 하자. . cuts=pd.cut(df.DIST,bins=bins) #(적용할 칼럼, bins=구간) cuts . 0 (400.0, 700.0] 1 (1200.0, inf] 2 (400.0, 700.0] 3 (700.0, 1200.0] 4 (1200.0, inf] ... 58487 (1200.0, inf] 58488 (400.0, 700.0] 58489 (-inf, 400.0] 58490 (700.0, 1200.0] 58491 (400.0, 700.0] Name: DIST, Length: 58492, dtype: category Categories (4, interval[float64]): [(-inf, 400.0] &lt; (400.0, 700.0] &lt; (700.0, 1200.0] &lt; (1200.0, inf]] . - cuts, AIRLINE $ to$ {DIVERTED: sum} . df.groupby([cuts,&#39;AIRLINE&#39;]).agg({&#39;DIVERTED&#39;:sum}) #구분별 항공사 diverted합 산출 . DIVERTED . DIST AIRLINE . (-inf, 400.0] AA 0 | . AS 0 | . B6 0 | . DL 1 | . EV 3 | . F9 0 | . HA 0 | . MQ 0 | . NK 0 | . OO 5 | . UA 2 | . US 0 | . VX 0 | . WN 1 | . (400.0, 700.0] AA 3 | . AS 0 | . B6 0 | . DL 12 | . EV 8 | . F9 1 | . HA 0 | . MQ 4 | . NK 1 | . OO 7 | . UA 1 | . US 0 | . VX 0 | . WN 2 | . (700.0, 1200.0] AA 10 | . AS 0 | . B6 1 | . DL 6 | . EV 4 | . F9 0 | . HA 0 | . MQ 1 | . NK 1 | . OO 5 | . UA 4 | . US 0 | . VX 0 | . WN 4 | . (1200.0, inf] AA 13 | . AS 0 | . B6 1 | . DL 5 | . EV 0 | . F9 1 | . HA 1 | . MQ 0 | . NK 3 | . OO 4 | . UA 12 | . US 1 | . VX 1 | . WN 8 | . - 아래와 비교해보자. . df.groupby([&#39;AIRLINE&#39;]).agg({&#39;DIVERTED&#39;:sum}) . DIVERTED . AIRLINE . AA 26 | . AS 0 | . B6 2 | . DL 24 | . EV 15 | . F9 2 | . HA 1 | . MQ 5 | . NK 5 | . OO 21 | . UA 19 | . US 1 | . VX 1 | . WN 15 | . - cuts을 이용하여 추가그룹핑을 하면 조금 다른 특징들을 데이터에서 발견할 수 있다. . AA항공사와 DL항공사는 모두 비슷한 우회횟수를 가지고 있음. | AA항공사는 700회이상의 구간에서 우회를 많이하고 DL항공사는 400~700사이에서 우회를 많이 한다. (패턴이 다름) 상세한 패턴 확인 가능 | . - 구간이름에 label을 붙이는 방법 labels=[] 이용 . bins . [-inf, 400, 700, 1200, inf] . cuts2=pd.cut(df.DIST,bins=bins,labels=[&#39;Q1&#39;,&#39;Q2&#39;,&#39;Q3&#39;,&#39;Q4&#39;]) cuts2 . 0 Q2 1 Q4 2 Q2 3 Q3 4 Q4 .. 58487 Q4 58488 Q2 58489 Q1 58490 Q3 58491 Q2 Name: DIST, Length: 58492, dtype: category Categories (4, object): [&#39;Q1&#39; &lt; &#39;Q2&#39; &lt; &#39;Q3&#39; &lt; &#39;Q4&#39;] . df.groupby(by=[cuts2,&#39;AIRLINE&#39;]).agg({&#39;DIVERTED&#39;:sum}) . DIVERTED . DIST AIRLINE . Q1 AA 0 | . AS 0 | . B6 0 | . DL 1 | . EV 3 | . F9 0 | . HA 0 | . MQ 0 | . NK 0 | . OO 5 | . UA 2 | . US 0 | . VX 0 | . WN 1 | . Q2 AA 3 | . AS 0 | . B6 0 | . DL 12 | . EV 8 | . F9 1 | . HA 0 | . MQ 4 | . NK 1 | . OO 7 | . UA 1 | . US 0 | . VX 0 | . WN 2 | . Q3 AA 10 | . AS 0 | . B6 1 | . DL 6 | . EV 4 | . F9 0 | . HA 0 | . MQ 1 | . NK 1 | . OO 5 | . UA 4 | . US 0 | . VX 0 | . WN 4 | . Q4 AA 13 | . AS 0 | . B6 1 | . DL 5 | . EV 0 | . F9 1 | . HA 1 | . MQ 0 | . NK 3 | . OO 4 | . UA 12 | . US 1 | . VX 1 | . WN 8 | . df.groupby(cuts2).agg({&#39;DIVERTED&#39;:len}) . DIVERTED . DIST . Q1 15027 | . Q2 14697 | . Q3 14417 | . Q4 14351 | . &#49689;&#51228; . 구간을 . bins=[-np.inf, 400, 700, 1200, np.inf] . 이 아니라 . bins=[-np.inf, 400, 600, 800, 1000, 1200, np.inf] . 와 같이 나누고 적당한 각구간별로 해당하는 관측치의 수를 구하라. . . &#45936;&#51060;&#53552;&#49884;&#44033;&#54868; 10/27&#49688;&#50629; &#44284;&#51228; 201514142 &#44608;&#46041;&#51456; . bins=[-np.inf, 400, 600, 800, 1000, 1200, np.inf] . cuts3=pd.cut(df.DIST,bins=bins,labels=[&#39;Q1&#39;,&#39;Q2&#39;,&#39;Q3&#39;,&#39;Q4&#39;,&#39;Q5&#39;,&#39;Q6&#39;]) cuts3 . 0 Q2 1 Q6 2 Q3 3 Q5 4 Q6 .. 58487 Q6 58488 Q2 58489 Q1 58490 Q4 58491 Q2 Name: DIST, Length: 58492, dtype: category Categories (6, object): [&#39;Q1&#39; &lt; &#39;Q2&#39; &lt; &#39;Q3&#39; &lt; &#39;Q4&#39; &lt; &#39;Q5&#39; &lt; &#39;Q6&#39;] . df.groupby(cuts3).agg({&#39;DIVERTED&#39;:len}) . DIVERTED . DIST . Q1 15027 | . Q2 9130 | . Q3 8553 | . Q4 7542 | . Q5 3889 | . Q6 14351 | .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/28/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-1027-%EA%B0%95%EC%9D%98-%EC%A0%95%EB%A6%AC.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/28/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-1027-%EA%B0%95%EC%9D%98-%EC%A0%95%EB%A6%AC.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "회귀분석 7장 연습문제R",
            "content": "3.7&#51088;&#47308; &#54644;&#49437; . x &lt;- c(4.2,3.8,4.8,3.4,4.5,4.6,4.3,3.7,3.9) y &lt;- c(2.8,2.5,3.1,2.1,2.9,2.6,2.4,2.4,2.5) lm37 &lt;- lm(y~x) . coef(lm37) . &lt;dl class=dl-inline&gt;(Intercept)0.345994832041343x0.542635658914729&lt;/dl&gt; $ to X = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 vdots &amp; vdots 1 &amp; x_n end{bmatrix} , beta = begin{bmatrix} 0.346 0.543 end{bmatrix}$ . 7.1 . 연습문제 3.7의 자료로부터 다음을 구하여라. . 1) $X&#39;X$ . $X&#39;X$ = $ begin{bmatrix} n &amp; sum_{}^{}x_i sum_{}^{}x_i &amp; sum_{}^{}{x_i}^2 end{bmatrix}$ . n , $ sum_{}^{}x_i$ , $ sum_{}^{}{x_i}^2$ . n &lt;- length(x) sx &lt;- sum(x) sxs &lt;- sum(x^2) c(n,sx,sxs) . &lt;ol class=list-inline&gt;9 | 37.2 | 155.48 | &lt;/ol&gt; $ to X&#39;X = begin{bmatrix} 9 &amp; 37.2 37.2 &amp; 155.48 end{bmatrix}$ . 2) $(X&#39;X)^{-1}$ . $(X&#39;X)^{-1}$ = $ frac{1}{n sum_{}^{}{x_i}^2 - ( sum_{}^{}x_i)^2} $ $ begin{bmatrix} sum_{}^{}{x_i}^2 &amp; - sum_{}^{}x_i - sum_{}^{}x_i &amp; n end{bmatrix}$ . - $ frac{1}{n sum_{}^{}{x_i}^2 - ( sum_{}^{}x_i)^2} $ 계산 . M &lt;- 1/(n*sxs-(sx)^2) M . 0.0645994832041333 $ sum_{}^{}{x_i}^2 , - sum_{}^{}x_i , n$ . c(sxs*M,-1*sx*M,n*M) . &lt;ol class=list-inline&gt;10.0439276485787 | -2.40310077519376 | 0.5813953488372 | &lt;/ol&gt; $ to (X&#39;X)^{-1} = begin{bmatrix} 10.04 &amp; -2.403 -2.403 &amp; 0.581 end{bmatrix} $ . 3) $X&#39;Y$ . $X&#39;Y$ = $ begin{bmatrix} sum_{}^{}y_i sum_{}^{}x_iy_i end{bmatrix} $ . $ sum_{}^{}y_i , sum_{}^{}x_iy_i $ . sy &lt;- sum(y) sxy &lt;- sum(x*y) c(sy,sxy) . &lt;ol class=list-inline&gt;23.3 | 97.24 | &lt;/ol&gt; $ to X&#39;Y$ = $ begin{bmatrix} 23.3 97.24 end{bmatrix} $ . 4) $(X&#39;X)^{-1}X&#39;Y$ . $(X&#39;X)^{-1}X&#39;Y$ = $ begin{bmatrix} frac{(sxs*sy -sx*sxy)}{n*sxs-sx^2} frac{n*sxy - sx*sy}{n*sxs - sx^2} end{bmatrix} $ . 10.04*23.3-2.403*97.24 . 0.264279999999985 (-2.403*23.3)+(0.581*97.24) . 0.506539999999987 A = matrix(c(10.04,-2.403,-2.403,0.581),2,2) #(X&#39;X)^-1 B = matrix(c(23.3,97.24),2,1) #X&#39;Y print(A) print(B) . [,1] [,2] [1,] 10.040 -2.403 [2,] -2.403 0.581 [,1] [1,] 23.30 [2,] 97.24 . A %*% B . A matrix: 2 × 1 of type dbl 0.26428 | . 0.50654 | . $ to (X&#39;X)^{-1}X&#39;Y$ = $ begin{bmatrix} 0.26428 0.50654 end{bmatrix} $ . . 7.7 . 어린이들의 체중($y$ :파운드)이 신장($x_1$:인치)가 연령($x_2$:살)에 따라 어떻게 영향을 받는지 파악하기 위해 12명의 어린이로부터 다음의 자료를 얻었다. . 변수 값 . $y$ | 64 | 71 | 53 | 67 | 55 | 58 | 77 | 57 | 56 | 51 | 76 | 68 | . $x_1$ | 57 | 59 | 49 | 62 | 51 | 50 | 55 | 48 | 42 | 42 | 61 | 57 | . $x_2$ | 8 | 10 | 6 | 11 | 8 | 7 | 10 | 9 | 10 | 6 | 12 | 9 | . 1) $x_1$&#50640; &#45824;&#54620; $y$ / $x_2$&#50640; &#45824;&#54620; $y$ / $x_1$&#50640; &#45824;&#54620; $x_2$ &#51032; &#49328;&#51216;&#46020;&#47484; &#44536;&#47532;&#44256;, &#44033;&#44033;&#51032; &#49345;&#44288;&#44228;&#49688; $r_{x_1y},r_{x_2y},r_{x_1x_2}$&#47484; &#44396;&#54616;&#50668;&#46972;. . y &lt;- c(64,71,53,67,55,58,77,57,56,51,76,68) x1 &lt;- c(57,59,49,62,51,50,55,48,42,42,61,57) x2 &lt;- c(8,10,6,11,8,7,10,9,10,6,12,9) . $x_1$에 대한 $y$ / $x_2$에 대한 $y$ / $x_1$에 대한 $x_2$ . par(mfrow=c(1,3)) plot(x1,y,main=&quot;x1:y&quot;) plot(x2,y,main=&quot;x2:y&quot;) plot(x1,x2,main=&quot;x1:x2&quot;) . 상관계수 구하기 . cor(x1,y) cor(x2,y) cor(x1,x2) . 0.814256949718907 0.769816801847935 0.613838630337317 $x_1$에 대한 $y$ 의 상관계수,$r_{x_1y}: 0.8143$ . $x_2$에 대한 $y$ 의 상관계수,$r_{x_2y}: 0.7698$ . $x_1$에 대한 $x_2$ 의 상관계수,$r_{x_1x_2}: 0.6138$ . 2) $y= beta_0 + beta_1x_1+ epsilon$&#51012; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#44228;&#49688;&#50752; &#51092;&#52264;&#51228;&#44273;&#54633;&#51012; &#44396;&#54616;&#50668;&#46972;. . 회귀계수 . coef(lmx1y) . &lt;dl class=dl-inline&gt;(Intercept)6.18984870668621x11.07223035627135&lt;/dl&gt; $ to beta_0 = 6.19 , beta_1 = 1.072 $ . 잔차제곱합 . sum((y-(6.19 + 1.072*x1))^2) . 299.329232 3) $y= beta_0 + beta_2x_2+ epsilon$&#51012; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#44228;&#49688;&#50752; &#51092;&#52264;&#51228;&#44273;&#54633;&#51012; &#44396;&#54616;&#50668;&#46972;. . 회귀계수 . coef(lmx2y) . &lt;dl class=dl-inline&gt;(Intercept)30.5714285714286x23.64285714285714&lt;/dl&gt; $ to beta_0 = 30.57 , beta_2 = 3.643 $ . 잔차제곱합 . sum((y-(30.57 + 3.643*x2))^2) . 361.857144 4) $y= beta_0 + beta_1x_1+ beta_2x_2+ epsilon$&#51012; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#44228;&#49688;&#50752; &#51092;&#52264;&#51228;&#44273;&#54633;&#51012; &#44396;&#54616;&#50668;&#46972;. . lmyx1x2 &lt;- lm(y~x1+x2) . 회귀계수 . coef(lmyx1x2) . &lt;dl class=dl-inline&gt;(Intercept)6.55304825080945x10.722037958356366x22.05012635236516&lt;/dl&gt; $ to beta_0 = 6.553 , beta_1 = 0.722, beta_2 = 2.05 $ . 잔차제곱합 . sum((y-(6.553 + 0.722*x1 + 2.05*x2 ))^2) . 195.427516 5) 3&#44060; &#47784;&#54805;&#51032; &#51092;&#52264;&#51228;&#44273;&#54633;&#51012; &#48708;&#44368;&#54616;&#44256; &#44536; &#44208;&#44284;&#50640; &#45824;&#54644; &#45436;&#51032;&#54616;&#50668;&#46972; . 중회귀모형의 잔차제곱합이 $195.43$으로 가장 작다. 이것은 $x_1,x_2$이 서로 연관이 있고, 이때의 $y$가 제일 잘 적합됐다는 뜻으로 해석 할 수 있다. . . 7.10 . 다음 주어진 자료에 대하여 . 범주 값 . $y$ | 6 | 8 | 1 | 0 | 5 | 3 | 2 | -4 | 10 | -3 | 5 | . $x_1$ | 1 | 4 | 9 | 11 | 3 | 8 | 5 | 10 | 2 | 7 | 6 | . $x_2$ | 8 | 2 | -8 | -10 | 6 | -6 | 0 | -12 | 4 | -2 | -4 | . 1) &#51473;&#49440;&#54805;&#54924;&#44480;&#47784;&#54805; $y = beta_0 + beta_1x_1+ beta_2x_2+ epsilon$&#51012; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#49440;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. . y &lt;- c(6,8,1,0,5,3,2,-4,10,-3,5) x1 &lt;- c(1,4,9,11,3,8,5,10,2,7,6) x2 &lt;- c(8,2,-8,-10,6,-6,0,-12,4,-2,-4) . lm710 &lt;- lm(y ~ x1+x2) . coef(lm710) . &lt;dl class=dl-inline&gt;(Intercept)14x1-2x2-0.500000000000001&lt;/dl&gt; $ beta_0 = 14 , beta_1 = -2 , beta_2 = -0.5 $ . 2) $ sigma^2$&#51032; &#48520;&#54200;&#52628;&#51221;&#47049; $MSE$&#47484; &#44396;&#54616;&#44256; &#52628;&#51221;&#46108; $MSE$&#44050;&#51012; &#51060;&#50857;&#54616;&#50668; . anova(lm710) . A anova: 3 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x11 | 116.081818 | 116.081818 | 13.6566845 | 0.006081901 | . x21 | 5.918182 | 5.918182 | 0.6962567 | 0.428255902 | . Residuals8 | 68.000000 | 8.500000 | NA | NA | . $MSE = 8.5$ . (1) $Var(b_1), Var(b_2)$&#47484; &#44033;&#44033; &#52628;&#51221;&#54616;&#50668;&#46972;. . X &lt;- matrix(c(1,1,1,1,1,1,1,1,1,1,1,array(x1),array(x2)),11,3) B &lt;- matrix(c(14,-2,-0.5),3,1) Y &lt;- matrix(y,11,1) . XtX &lt;- t(X) %*% X #[X&#39;X] XtX . A matrix: 3 × 3 of type dbl 11 | 66 | -22 | . 66 | 506 | -346 | . -22 | -346 | 484 | . XtXr &lt;- solve(XtX) # [X&#39;X]의 역함수 XtXr . A matrix: 3 × 3 of type dbl 4.3704790 | -0.84946237 | -0.40860215 | . -0.8494624 | 0.16897081 | 0.08218126 | . -0.4086022 | 0.08218126 | 0.04224270 | . (XtXr)* (8.5^2) # [(X&#39;X)^-1]*(MSE^2) = Var(b) . A matrix: 3 × 3 of type dbl 315.76711 | -61.373656 | -29.521505 | . -61.37366 | 12.208141 | 5.937596 | . -29.52151 | 5.937596 | 3.052035 | . $Var(b_1) : [(X&#39;X)^{-1}]*(MSE^2)$ 의 대각 2번째 원소 . 12.21 . $Var(b_2) :[(X&#39;X)^{-1}]*(MSE^2)$ 의 대각 3번째 원소&gt; 3.05 . (2) $x_1 =3, x_2=5$&#51068; &#46412; $y$&#51032; &#50696;&#52769;&#44050;&#51032; &#48516;&#49328;&#51012; &#44396;&#54616;&#50668;&#46972;. . 14-2*3-0.5*5 #y의 예측값 . 5.5 sum((5.5-mean(14-2*x1-0.5*x2))^2) . 6.25 $𝑦$ 의 예측값의 분산 : 6.25 .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/10/25/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D7%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/10/25/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D7%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "기계학습 3장 예제 파이썬",
            "content": "Advertising data &#48520;&#47084;&#50724;&#44256; df&#47196; &#47564;&#46308;&#44592; . import matplotlib.pyplot as plt import numpy as np import pandas as pd ad = pd.read_csv(&quot;Advertising.csv&quot;, header=0) # csv 파일 불러오기 . 1. Logistic regression model . statmodels에 의한 로지스틱 모형 적합 . ad[&#39;sales&#39;] = (ad[&#39;sales&#39;] &gt; 10)*1 #sales 변수가 10을 넘으면 1(판매량 높음), 그렇지 않으면 0으로 할당하여 분류문제로 세팅 X = ad[[&#39;TV&#39;,&#39;radio&#39;,&#39;newspaper&#39;]] Y = ad[&#39;sales&#39;] . linear model with statsmodels . import statsmodels.api as sm #statsmodels 패키지 불러오기 model = sm.GLM.from_formula(&quot;sales ~ TV + radio + newspaper&quot;, family = sm.families.Binomial(), data=ad) result = model.fit() result.summary() . Generalized Linear Model Regression Results Dep. Variable: sales | No. Observations: 200 | . Model: GLM | Df Residuals: 196 | . Model Family: Binomial | Df Model: 3 | . Link Function: logit | Scale: 1.0000 | . Method: IRLS | Log-Likelihood: -13.360 | . Date: Sat, 23 Oct 2021 | Deviance: 26.719 | . Time: 20:03:15 | Pearson chi2: 30.1 | . No. Iterations: 11 | | . Covariance Type: nonrobust | | . | coef std err z P&gt;|z| [0.025 0.975] . Intercept -21.5305 | 6.269 | -3.435 | 0.001 | -33.817 | -9.244 | . TV 0.2089 | 0.060 | 3.454 | 0.001 | 0.090 | 0.328 | . radio 0.4054 | 0.124 | 3.277 | 0.001 | 0.163 | 0.648 | . newspaper -0.0086 | 0.026 | -0.332 | 0.740 | -0.059 | 0.042 | . sklearn&#51012; &#51060;&#50857;&#54620; &#47196;&#51648;&#49828;&#54001; &#51201;&#54633; . (머신러닝에 특화된 패키지 , 서머리같은건 제공 잘 안해줌) . from sklearn.linear_model import LogisticRegression . model1 = LogisticRegression() # model instance setup model2 = model1.fit(X,Y) # model fitting to data print(model2.intercept_) print(model2.coef_) print(model2.score(X,Y)) . [-21.16756665] [[ 0.2054616 0.39803989 -0.00823107]] 0.97 . from sklearn import metrics Y_pred = model2.predict(X) . cm = metrics.confusion_matrix(Y, Y_pred) # confusion matrix cm . array([[ 42, 3], [ 3, 152]]) . model2.score(X,Y) # 정분류율 . 0.97 . &#50696;&#52769;&#49457;&#45733;&#51032; &#54217;&#44032;&#47484; &#50948;&#54644; &#54984;&#47144;&#51088;&#47308;&#50752; &#54217;&#44032;&#51088;&#47308;&#47484; &#51201;&#45817;&#54620; &#48708;&#50984;&#47196; &#45208;&#45572;&#50612; &#44228;&#49328;&#54644; &#48372;&#51088;. . from sklearn.model_selection import train_test_split X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=0) . model1 = LogisticRegression() # model instance setup model2 = model1.fit(X_train,Y_train) # model fitting to data . print(model2.intercept_) print(model2.coef_) print(model2.score(X,Y)) . [-21.87095867] [[0.20169776 0.38998892 0.02502465]] 0.96 . model2.predict_proba(X_test) # predicted probability for 0 vs 1 . array([[3.68362025e-01, 6.31637975e-01], [9.98889402e-01, 1.11059773e-03], [9.49784560e-01, 5.02154405e-02], [0.00000000e+00, 1.00000000e+00], [7.68983531e-08, 9.99999923e-01], [9.99484252e-01, 5.15748381e-04], [3.03408631e-01, 6.96591369e-01], [1.36557432e-13, 1.00000000e+00], [8.49772353e-01, 1.50227647e-01], [1.22679644e-12, 1.00000000e+00], [0.00000000e+00, 1.00000000e+00], [2.75460182e-01, 7.24539818e-01], [1.77948022e-05, 9.99982205e-01], [2.34914310e-11, 1.00000000e+00], [1.08841492e-02, 9.89115851e-01], [3.33913329e-05, 9.99966609e-01], [0.00000000e+00, 1.00000000e+00], [9.97690690e-01, 2.30930953e-03], [1.23437704e-06, 9.99998766e-01], [3.13082893e-14, 1.00000000e+00], [0.00000000e+00, 1.00000000e+00], [6.14898732e-04, 9.99385101e-01], [1.01664233e-10, 1.00000000e+00], [2.94828121e-06, 9.99997052e-01], [9.99942555e-01, 5.74451124e-05], [5.25259866e-08, 9.99999947e-01], [1.28092454e-07, 9.99999872e-01], [0.00000000e+00, 1.00000000e+00], [1.99917860e-11, 1.00000000e+00], [9.96672041e-01, 3.32795930e-03], [1.32267261e-04, 9.99867733e-01], [0.00000000e+00, 1.00000000e+00], [0.00000000e+00, 1.00000000e+00], [0.00000000e+00, 1.00000000e+00], [9.99992240e-01, 7.76014393e-06], [9.99997225e-01, 2.77524737e-06], [9.83558085e-01, 1.64419151e-02], [1.57451874e-09, 9.99999998e-01], [3.82797066e-05, 9.99961720e-01], [9.99992461e-01, 7.53906740e-06], [9.39035328e-02, 9.06096467e-01], [9.97181415e-01, 2.81858519e-03], [7.61125996e-09, 9.99999992e-01], [3.41060513e-13, 1.00000000e+00], [1.02140518e-14, 1.00000000e+00], [2.49832425e-05, 9.99975017e-01], [9.99999996e-01, 4.18250051e-09], [3.06304657e-01, 6.93695343e-01], [2.38279196e-09, 9.99999998e-01], [9.02643249e-02, 9.09735675e-01], [1.29806943e-03, 9.98701931e-01], [0.00000000e+00, 1.00000000e+00], [0.00000000e+00, 1.00000000e+00], [3.20645732e-11, 1.00000000e+00], [9.07025643e-01, 9.29743571e-02], [9.80798465e-01, 1.92015346e-02], [0.00000000e+00, 1.00000000e+00], [9.74632153e-10, 9.99999999e-01], [4.36317649e-13, 1.00000000e+00], [9.99776048e-01, 2.23951730e-04]]) .",
            "url": "https://cjfal.github.io/dj/python/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2021/10/25/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-3%EC%9E%A5-%EC%98%88%EC%A0%9C%ED%8C%8C%EC%9D%B4%EC%8D%AC.html",
            "relUrl": "/python/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2021/10/25/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-3%EC%9E%A5-%EC%98%88%EC%A0%9C%ED%8C%8C%EC%9D%B4%EC%8D%AC.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "기계학습 3장 예제 R",
            "content": "&#50696;&#51228;1 : Advertising data . ad &lt;- read.csv(&quot;Advertising.csv&quot;) options(digits=4) # 자료수 표현 . fit &lt;- lm(sales ~TV + radio +newspaper , data = ad) fit1 &lt;- summary(fit) . (pvlaue=1-pf(fit1$fstatistic[1],fit1$fstatistic[2], fit1$fstatistic[3])) # pvalue for F test . value: 0 F검정결과 p값이 매우작아 연관성이 있다고 확신할 수 있다. (계수중 하나는 0이 아니다.) . RSE = sqrt(sum(fit$residuals^2)/(fit$df.residual)) #연관성의 정도 RSE . 1.68551037341474 R2 = fit1$r.squared R2 . 0.897210638178952 fit1$coefficients[,4] # 어떤 media가 판매량에 영향을 미치는가? . &lt;dl class=dl-inline&gt;(Intercept)1.26729450513134e-17TV1.50995995481439e-81radio1.50533892057562e-54newspaper0.859915050080572&lt;/dl&gt; 신문은 연관성이 관측되지 않음, 귀무가설을 기각한다. 0.8599 : 굉장히 큰값 . cbind(fit1$coefficients[,1]-2*fit1$coefficients[,2] ,fit1$coefficients[,1]+2*fit1$coefficients[,2]) #신뢰구간, 추정치 +- 표준편차*2 . A matrix: 4 × 2 of type dbl (Intercept) 2.31507 | 3.56271 | . TV 0.04297 | 0.04855 | . radio 0.17131 | 0.20575 | . newspaper-0.01278 | 0.01070 | . 신문에 대해서는 신뢰구간이 0을 포함하는 비교적 넓은 구간 . 선형성이 만족되는가? . 앞선 Figure 4 에서 선형성을 의심할 수 있는 정황이 발견 (3D plot) . fit2 = lm(sales ~ TV + radio, data=ad) # main fit3 = lm(sales ~ TV * radio, data=ad) # interaction c(summary(fit2)$r.squared,summary(fit3)$r.squared) #상호작용이 존재하는가? . &lt;ol class=list-inline&gt;0.897194261082896 | 0.967790549848252 | &lt;/ol&gt; summary(fit3)$coefficients[4,] # interaction term . &lt;dl class=dl-inline&gt;Estimate0.00108649469798996Std. Error5.24203957977123e-05t value20.7265641828171Pr(&gt;|t|)2.75768099928009e-51&lt;/dl&gt; . &#50696;&#51228;2 : &#8216;mtcars&#8217; dataset : &#49440;&#54805;&#47784;&#54805; - gradient descent algorithm &#51201;&#54633; . . . lm( mtcars$mpg~ mtcars$disp) . Call: lm(formula = mtcars$mpg ~ mtcars$disp) Coefficients: (Intercept) mtcars$disp 29.5999 -0.0412 . gradientDesc 랑 lm이랑 거의 비슷 .",
            "url": "https://cjfal.github.io/dj/r/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2021/10/25/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-3%EC%9E%A5-%EC%98%88%EC%A0%9CR.html",
            "relUrl": "/r/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2021/10/25/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-3%EC%9E%A5-%EC%98%88%EC%A0%9CR.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "데이터시각화 10/18 강의 과제",
            "content": "import numpy as np import matplotlib.pyplot as plt import pandas as pd from plotnine import * . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/extremum.csv&#39;) . temp=np.array(df.iloc[:,3]) . np.random.seed(1) ϵ1=np.random.normal(size=656, scale=10) icecream=temp*2 + 30 + ϵ1 . np.random.seed(2) ϵ2=np.random.normal(size=656,scale=1) disease=temp*0.5 + 40 +ϵ2 . df1=pd.DataFrame({&#39;temp&#39;:temp, &#39;icecream&#39;:icecream, &#39;disease&#39;:disease}) . def f(x): if x&lt;0: y=&#39;group0&#39; elif x&lt;10: y=&#39;group10&#39; elif x&lt;20: y=&#39;group20&#39; else: y=&#39;group30&#39; return y . df1[&#39;temp2&#39;]=list(map(f,df1.temp)) . ggplot(data=df1)+geom_point(aes(x=&#39;icecream&#39;,y=&#39;disease&#39;,colour=&#39;temp2&#39;),alpha=0.5) . &lt;ggplot: (8774727276960)&gt; . ggplot(data=df1)+geom_point(aes(x=&#39;icecream&#39;,y=&#39;disease&#39;,colour=&#39;temp2&#39;),alpha=0.2)+geom_smooth(aes(x=&#39;icecream&#39;,y=&#39;disease&#39;,colour=&#39;temp2&#39;),size=2,linetype=&#39;dashed&#39;) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8774723030923)&gt; . np.random.seed(1) ϵ1=np.random.normal(size=656, scale=10) icecream=temp*2 + 30 + ϵ1 . np.random.seed(2) ϵ2=np.random.normal(size=656,scale=1) disease= 30+ temp*0.0 + icecream*0.15 +ϵ2*2 . df2=pd.DataFrame({&#39;temp&#39;:temp,&#39;icecream&#39;:icecream,&#39;disease&#39;:disease}) df2[&#39;temp2&#39;]=list(map(f,df2.temp)) . ggplot(data=df2)+geom_point(aes(x=&#39;icecream&#39;,y=&#39;disease&#39;,colour=&#39;temp2&#39;),alpha=0.2)+geom_smooth(aes(x=&#39;icecream&#39;,y=&#39;disease&#39;,colour=&#39;temp2&#39;),size=2,linetype=&#39;dashed&#39;) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/smoothers.py:310: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings. . &lt;ggplot: (8774722931447)&gt; .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/18/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%941018%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/18/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%941018%EA%B3%BC%EC%A0%9C.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "회귀분석 6장",
            "content": "6.2 . 다음 자료는 A 제품을 생산하는 공장의 총자산(x)과 총수입(y)과의 관계를 조사한 표이다. . x 25 6 8 5 1 24 17 2 13 14 . y | 10.1 | 2.9 | 3.0 | 1.8 | 0.1 | 9.4 | 6.9 | 0.3 | 5.1 | 6.0 | . 1) &#49328;&#51216;&#46020;&#47484; &#44536;&#47532;&#44256;, &#51201;&#54633;&#54620; &#47784;&#54805;&#51012; &#49444;&#51221;&#54616;&#50668;&#46972;. . x &lt;- c(25,6,8,5,1,24,17,2,13,14) y &lt;- c(10.1,2.9,3.0,1.8,0.1,9.4,6.9,0.3,5.1,6.0) plot(x,y) lm62&lt;-lm(y~x) abline(lm62,col=&#39;red&#39;) . 적합한 모형은 1차선형회귀모형이다. . 2) &#50896;&#51216;&#51012; &#51648;&#45208;&#45716; &#54924;&#44480;&#47784;&#54805; $y= beta_1x + epsilon$ &#51012; &#51201;&#54633;&#49884;&#53412;&#44256; &#48516;&#49328;&#48516;&#49437;&#54364;&#47484; &#51089;&#49457;&#54616;&#50668; &#44480;&#47924;&#44032;&#49444; $H_0 : beta_1 = 0 $ &#51012; &#44160;&#51221;&#54616;&#50668;&#46972;. $ alpha = 0.05 $ . b1 구하기 . sum(x*y)/sum(x^2) #기울기 구하기 . 0.401410579345088 b1(기울기) 값은 0.401410579345088 이다. . 불편 추정량: yhat = 0.401410579345088*x . yhat = 0.401410579345088*x . y-0.401410579345088*x #잔차구하기 . &lt;ol class=list-inline&gt;0.0647355163727994 | 0.491536523929472 | -0.211284634760704 | -0.20705289672544 | -0.301410579345088 | -0.233853904282112 | 0.0760201511335046 | -0.502821158690176 | -0.118337531486144 | 0.380251889168768 | &lt;/ol&gt; sum(y-0.401410579345088*x) #잔차의 합이 0이 아니다. . -0.56221662468512 영가설 $H_0 : beta_1 = 0$ 검정 . 회귀모형의 변동 (F 통계량 구하기) . SST = sum(y^2) SSR = sum(yhat^2) SSE = SST - SSR MSE = SSE/(length(x)-1) F0 = (SSR)/(SSE/(length(x)-1)) F0 . 3212.53761820648 F 통계량은 3212.53761820648 이고 유의수준 0.05일때 자유도 1,9인 F값은 3.26 이므로 영가설 $H_0$를 기각한다. . 즉, x는 y에 영향을 끼친다. . 3) &#45800;&#49692;&#54924;&#44480;&#47784;&#54805; $y= beta_0 + beta_1x+ epsilon$&#51012; &#51201;&#54633;&#49884;&#53412;&#44256; &#44208;&#51221;&#44228;&#49688; $R^2$ &#51012; &#44228;&#49328;&#54616;&#50668;&#46972;. . 단순회귀모형 적합 . coef(lm62) . &lt;dl class=dl-inline&gt;(Intercept)-0.168452830188679x0.411169811320755&lt;/dl&gt; $ beta_0 = -0.168452830188679 , beta_1 = 0.411169811320755 $ . 단순회귀모형 :$y = -0.168452830188679 + 0.411169811320755x$ . summary(lm62)$r.squared #결정계수 . 0.99289614378722 결정계수가 1에 굉장히 가까운 것으로 보아 관계를 설명하기 굉장히 좋은 모형이다. . . 6.5 . 다음 표는 남녀 어린이 각각 16명에게 비타민B를 복용시켰을 때 4주간의 성장률에 관한 자료이다. . 남 여 . 성장률(y) | 복용량(x) | 성장률(y) | 복용량(x) | . 17.1 | 0.301 | 18.5 | 0.301 | . 14.3 | 0.301 | 22.1 | 0.301 | . 21.6 | 0.301 | 15.3 | 0.301 | . 24.5 | 0.602 | 23.6 | 0.602 | . 20.6 | 0.602 | 26.9 | 0.602 | . 23.8 | 0.602 | 20.2 | 0.602 | . 27.7 | 0.903 | 24.3 | 0.903 | . 31.0 | 0.903 | 27.1 | 0.903 | . 29.4 | 0.903 | 30.1 | 0.903 | . 30.1 | 1.204 | 28.1 | 0.903 | . 28.6 | 1.204 | 30.3 | 1.204 | . 34.2 | 1.204 | 33.0 | 1.204 | . 37.3 | 1.204 | 35.8 | 1.204 | . 33.3 | 1.505 | 32.6 | 1.505 | . 31.8 | 1.505 | 36.1 | 1.505 | . 40.2 | 1.505 | 30.5 | 1.505 | . 1) &#51452;&#50612;&#51652; &#51088;&#47308;&#50640; $y= beta_0 + beta_1x + epsilon$ &#51060; &#49457;&#47549;&#54620;&#45796;&#44256; &#44032;&#51221;&#54616;&#44256; &#45224;&#45376; &#50612;&#47536;&#51060; &#44033;&#44033;&#51032; &#44221;&#50864;&#50640; &#54924;&#44480;&#49440;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. . 남자 어린이의 경우 . eatboy &lt;- c(0.301,0.301,0.301,0.602,0.602,0.602,0.903,0.903,0.903,1.204,1.204,1.204,1.204,1.505,1.505,1.505) growboy &lt;- c(17.1,14.3,21.6,24.5,20.6,23.8,27.7,31.0,29.4,30.1,28.6,34.2,37.3,33.3,31.8,40.2) lmboy &lt;- lm(growboy~eatboy) coef(lmboy) . &lt;dl class=dl-inline&gt;(Intercept)14.1775757575758eatboy14.8253297090506&lt;/dl&gt; 남자어린이의 추정된 회귀선은 $y = 14.1775757575758 + 14.8253297090506x $ 이다. . 여자 어린이의 경우 . eatgirl &lt;- c(0.301,0.301,0.301,0.602,0.602,0.602,0.903,0.903,0.903,0.903,1.204,1.204,1.204,1.505,1.505,1.505) growgirl &lt;- c(18.5, 22.1,15.3 ,23.6 ,26.9 ,20.2 ,24.3 ,27.1 ,30.1 ,28.1 ,30.3 , 33.0,35.8 ,32.6 ,36.1 ,30.5) lmgirl &lt;- lm(growgirl~eatgirl) coef(lmgirl) . &lt;dl class=dl-inline&gt;(Intercept)15.65625eatgirl12.7353266888151&lt;/dl&gt; 여자어린이의 추정된 회귀선은 $y = 15.65625 +12.7353266888151x $ 이다. . 2) &#46160; &#44060;&#51032; &#52628;&#51221;&#46108; &#54924;&#44480;&#49440;&#51012; &#44057;&#51008; &#54217;&#47732;&#49345;&#50640; &#45208;&#53440;&#45236;&#50612; &#48708;&#44368;&#54644; &#48372;&#46972;. . plot(growboy~eatboy,col=&quot;red&quot;) abline(lmboy,col=&#39;red&#39;) par(new = T) plot(growgirl~eatgirl,col=&#39;blue&#39;) abline(lmgirl,col=&#39;blue&#39;) . 3) &#46160; &#54924;&#44480;&#51649;&#49440;&#51032; &#44592;&#50872;&#44592;&#44032; &#44057;&#51008;&#51648; &#44160;&#51221;&#54616;&#50668;&#46972;. $ alpha = 0.05 $ . boycoef &lt;- summary(lmboy)$coefficients girlcoef &lt;- summary(lmgirl)$coefficients db &lt;- (girlcoef[2,1]-boycoef[2,1]) #두 기울기의 차이 sd &lt;- sqrt(girlcoef[2,2]^2+boycoef[2,2]^2) #두 기울기의 분산 합 df &lt;- (lmboy$df.residual+lmgirl$df.residual) #자유도 td &lt;- (abs(db/sd)) #t 통계량 2*pt(-td,df) #유의확률 . 0.432356448008467 기울기의 검정 $H_0 : beta_{1boy} = beta_{1girl}$ 에 대한 유의확률이 0.43 이므로 영가설을 기각할 수 없다. 동일한 기울기로 볼 수있다. . 4) &#46160; &#54924;&#44480;&#51649;&#49440;&#51032; &#46041;&#51068;&#49457; &#50668;&#48512;&#47484; &#44160;&#51221;&#54616;&#50668;&#46972;. $ alpha = 0.05 $ . anova(lmboy) anova(lmgirl) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . eatboy 1 | 616.0656 | 616.06556 | 58.36567 | 2.331699e-06 | . Residuals14 | 147.7738 | 10.55527 | NA | NA | . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . eatgirl 1 | 440.8333 | 440.83333 | 52.03501 | 4.469824e-06 | . Residuals14 | 118.6060 | 8.47186 | NA | NA | . 분산분석표에서 완전모형의 잔차제곱합 . $SSE(F) = SSE_1 + SSE_2 = 147.7738 + 118.6060 = 266.3798$ . 자유도 : $df_F = 14 + 14 =28 $ . eatbar &lt;- (1/(length(eatboy)*2))*(sum(eatboy)+sum(eatgirl)) growbar &lt;- (1/(length(eatboy)*2))*(sum(growboy)+sum(growgirl)) sum((eatboy-eatbar)*(growboy-growbar))+sum((eatgirl-eatbar)*(growgirl-growbar))/(sum((eatboy-eatbar)^2)+(sum((eatgirl-eatbar)^2))) #b1(기울기) . 47.8825170071177 growbar-47.8825170071177*eatbar #b0 (절편) . -16.1883077830255 . . 6.7 . 단순 회귀 모형에서 회귀제곱합 . $SSR = y&#39;[X(X&#39;X)^{-1} - frac{J}{n}]y = sum_{}^{} ( hat{y_i}- bar{y})^2$ . 의 기댓값을 [정리 6.4]를 이용하여 구하고 잔차제곱합 . $ SSE = sum_{i=1}^{n} (y_i - hat{y_1})^2 $ = $ y&#39;[I-X(X&#39;X)^{-1} X&#39;] y $ . 의 기댓값도 구하여 보아라 . - [정리 6.4] : 만약 벡터 $y sim N( mu,V)$ 이면 ($V$는 분산공분산행렬) . 1) $E(y&#39;Ay) = tr(AV) + mu&#39;A mu $ (이것은 y가 정규분포가 아닐 때도 성립) . 2) $Cov(y,y&#39;Ay) = 2VA mu$ . 1) . $E(SSR) = E(y&#39;Ay) to$ A = $ [X(X&#39;X)^{-1} - frac{J}{n}] $ . $ V = I sigma^2 $ 이므로 . $E(SSR) = sigma^2 tr(A) + mu&#39;A mu $ . = $ sigma^2 + { beta_1}^2 sum{}^{} (x_i - bar{x})^2 $ 이다. . 2) . $E(SSE) = E(y&#39;By) to$ B = $ [I - X(X&#39;X)^{-1}$ . $E(SSE) = sigma^2 tr(B) + mu&#39;B mu $ . = $(n-2) sigma^2$ 이다. . . 6.10 . 단순회귀모형 . $ y = beta_0 + beta_1 x + epsilon $ , $ epsilon sim N(0, sigma^2)$ . 에서 $H_0 : beta_1 = 0$ 이 성립하면 검정통계량 . $F_0 = frac{MSR}{MSE}$ . 은 비중심모수 $ lambda = 0$ 인 중심 $F(1,n-2)$ 분포가 됨을 보여라([정리 6.8]이용) . - [정리 6.8] . $y sim N( mu,I)$ 이면 $ y&#39;Ay sim $ $ chi^2 {&#39;} $ $ (k, frac{1}{2} mu&#39;A mu)$ 이 되기 위한 필요충분조건은 $A$가 계수 $k$인 멱등행렬이다. . 풀이 . $SSR = y&#39;[X(X&#39;X)^{-1} - frac{J}{n}]y $ . $ SSE = y&#39;[I-X(X&#39;X)^{-1} X&#39;] y $ 에서 . $[X(X&#39;X)^{-1} - frac{J}{n}]$ = A . $[I-X(X&#39;X)^{-1} X&#39;]$ = B 로 두고 $SSR,SSE$ 를 $ sigma^2$ 으로 나누면 . $ frac{SSR}{ sigma^2} = y&#39; frac{A}{ sigma^2}y $ 이고, . $ frac{SSE}{ sigma^2} = y&#39; frac{B}{ sigma^2}y $ 이다. . AVB = 0 이므로 바로 위 두식은 독립이다. . F 통계량을 구해보면 . $ F = frac{MSR}{MSE} = frac{ frac{SSR}{ sigma^2}/1}{ frac{SSE}{ sigma^2}/(n-2)} = F&#39; $ 이다. (이때 $F&#39; : (1,n-2; lambda)$) . 여기서 $ lambda = { beta_1}^2 sum{}^{} (x_i- bar{x})^2 / 2 sigma^2 $ 이다. . 이때 $ beta_1 = 0$ 이면, $ lambda = 0 $이므로 . $F_0 = frac{MSR}{MSE}$ 는 비중심모수 $ lambda = 0$ 인 중심 $F(1,n-2)$ 분포가 된다. .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/10/15/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D6%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/10/15/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D6%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "date": " • Oct 15, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "회귀분석 5장",
            "content": "5.2 . - 어떤 화학반응에서 촉매의 양(x)이 합성물의 소득량(y)에 어떻게 영향을 끼치는지 알아보기 위해 12번 실험하여 다음의 자료를 얻었다. . x(g) 1 1 1 2 2 2 4 4 4 8 8 8 . y(g) | 13.5 | 15.4 | 16.1 | 18.2 | 19.6 | 20.2 | 21.8 | 22.2 | 23.1 | 23.6 | 24.7 | 24.9 | . x &lt;- c(1,1,1,2,2,2,4,4,4,8,8,8) y &lt;- c(13.5,15.4,16.1,18.2,19.6,20.2,21.8,22.2,23.1,23.6,24.7,24.9) . 1) x&#50640;&#45824;&#54620; y&#51032; &#49328;&#51216;&#46020;&#47484; &#44536;&#47140;&#46972;. . plot(x,y,col=&quot;red&quot;) . 2) $ log_{10} x$&#47484; &#44228;&#49328;&#54616;&#44256; $ log_{10} x$&#50640; &#45824;&#54620; y&#44050;&#51032; &#49328;&#51216;&#46020;&#47484; &#44536;&#47140;&#46972;. . logx &lt;- log(x,base=10) logx . &lt;ol class=list-inline&gt;0 | 0 | 0 | 0.301029995663981 | 0.301029995663981 | 0.301029995663981 | 0.602059991327962 | 0.602059991327962 | 0.602059991327962 | 0.903089986991944 | 0.903089986991944 | 0.903089986991944 | &lt;/ol&gt; plot(logx,y,col=&quot;blue&quot;) . par(mfrow=c(1,2)) plot(x,y,col=&quot;red&quot;) plot(logx,y,col=&quot;blue&quot;,xlim=c(0,8)) # xlim 으로 x범위 조정 . 3) &#50948; 1)&#44284; 2) &#51473; &#50612;&#45712; &#44163;&#51060; &#45908; &#49440;&#54805;&#50640; &#44032;&#44620;&#50868;&#44032;? . 2)의 로그 그래프가 같은 범위에서 더 선형에 가깝다. . . 5.3 . - 연습문제 5.2의 자료에 대해 다음 질문에 답하여라. . 1) $ y = beta_0 + beta_1 x + epsilon $ &#47484; &#51201;&#54633;&#49884;&#53412;&#44256; $MSE$ &#47484; &#44396;&#54616;&#50668;&#46972;. . lm53 &lt;- lm(y~x) coefficients(lm53) . &lt;dl class=dl-inline&gt;(Intercept)15.8130434782609x1.18985507246377&lt;/dl&gt; $y= 15.51 + 1.19x$ . anova(lm53) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x 1 | 122.10888 | 122.108877 | 34.1147 | 0.0001636405 | . Residuals10 | 35.79362 | 3.579362 | NA | NA | . $MSE$ :Mean Square Residuals = 3.579362 . 2) $ y = beta_0 + beta_1 log_{10} x + epsilon $ &#47484; &#44032;&#51221;&#54616;&#44256; $MSE$ &#47484; &#44228;&#49328;&#54616;&#50668;&#46972;. . lm53log &lt;- lm(y~logx) anova(lm53log) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . logx 1 | 146.32817 | 146.328167 | 126.4247 | 5.374543e-07 | . Residuals10 | 11.57433 | 1.157433 | NA | NA | . $MSE$ :Mean Square Residuals = 1.157433 . 3) &#50948; 1)&#44284; 2)&#51032; &#47784;&#54805; &#51473; &#50612;&#45712; &#44163;&#51060; &#45908; &#51201;&#54633;&#54620;&#51648; $MSE$ &#47484; &#51060;&#50857;&#54616;&#50668; &#48708;&#44368;&#54616;&#50668;&#46972;. . $ log_{10} x$ 에서의 $MSE$ 가 더 작다. 즉, 2)의 모형이 더 적합하다. . . 5.12 . - 다음 과 같은 자료에 대하여 . x 2 6 10 . y | 4 | 7 | 4 | . 1) &#45800;&#49692;&#54924;&#44480;&#47784;&#54805; $ y = beta_0 + beta_1 x + epsilon $ , $ epsilon sim N(0, sigma^2)$&#51012; &#44032;&#51221;&#54616;&#44256; &#54924;&#44480;&#51649;&#49440;&#51012; &#52628;&#51221;&#54616;&#50668;&#46972;. &#46608; $ beta_1$ &#51032; 90% &#49888;&#47280;&#44396;&#44036;&#51012; &#44396;&#54616;&#50668;&#46972;. . x &lt;- c(2, 6, 10) y &lt;- c(4, 7, 4) lm512 &lt;- lm(y~x) coefficients(lm512) confint(lm512,level = 0.90) # 기울기와 절편의 90% 신뢰구간 . &lt;dl class=dl-inline&gt;(Intercept)5x-4.81432375190295e-16&lt;/dl&gt; A matrix: 2 × 2 of type dbl 5 %95 % . (Intercept)-13.676329 | 23.676329 | . x -2.733935 | 2.733935 | . 기울기 $ beta_1$의 90% 신뢰구간:$(-2.733935, 2.733935)$ . 2) $ y = beta_0 + beta_1 x + epsilon $ , $ epsilon sim N(0,k^2 {x_i}^2)$ &#51012; &#44032;&#51221;&#54616;&#50668; &#44032;&#51473;&#54924;&#44480;&#51649;&#49440;&#51012; &#52628;&#51221;&#54616;&#44256; $ beta_1$ &#51032; 90% &#49888;&#47280;&#44396;&#44036;&#51012; &#44396;&#54616;&#50668;&#46972;. . 오차항의 분산 : 2 . w &lt;- sqrt(1/2) xw &lt;- x/w yw &lt;- y/w b0w &lt;- 5/w b1w &lt;- 0 xbar &lt;- mean(x) ybar &lt;- mean(y) . 가중 정규방정식의 해 . sum(w*(x-xbar)*(y-ybar))/sum(w*(x-xbar)^2) #b1 . 0 ybar #b0 . 5 3) &#50948; 1), 2)&#51032; &#44208;&#44284;&#47484; &#48708;&#44368;&#54616;&#50668;&#46972; . 1)과 2) 는 y=5 로 같다. . . 5.14 . - 다음은 어떤 컴퓨터 부품의 과거 14개월(x) 동안의 판매액(y)에 관한 자료이다. . x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 . y | 6.0 | 6.3 | 6.1 | 6.8 | 7.5 | 8.0 | 8.1 | 8.5 | 9.0 | 8.7 | 7.9 | 8.2 | 8.4 | 9.0 | . 1) &#45936;&#51060;&#53552;&#51032; &#49328;&#51216;&#46020;&#47484; &#44536;&#47140;&#46972;. . x &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14) y &lt;- c(6.0,6.3,6.1,6.8,7.5,8.0,8.1,8.5,9.0,8.7,7.9,8.2,8.4,9.0) plot(x,y) . 2) &#45800;&#49692;&#54924;&#44480;&#47784;&#54805;&#51012; &#44032;&#51221;&#54616;&#44256; &#51092;&#52264;&#51032; &#49328;&#51216;&#46020;&#47484; &#45208;&#53440;&#45236;&#44256; &#51088;&#44592;&#49345;&#44288;&#51060; &#51316;&#51116;&#54616;&#45716;&#51648;&#47484; &#44160;&#53664;&#54616;&#50668;&#46972;. . lm514 &lt;- lm(y~x) coefficients(lm514) . &lt;dl class=dl-inline&gt;(Intercept)6.13296703296704x0.215604395604395&lt;/dl&gt; err &lt;- function(A){ print(0.215604395604395 * A + 6.13296703296704) } . yerr = err(x)-y . [1] 6.348571 6.564176 6.779780 6.995385 7.210989 7.426593 7.642198 7.857802 [9] 8.073407 8.289011 8.504615 8.720220 8.935824 9.151429 . plot(x,yerr) . 2차곡선식의 회귀선이 적절해보이는 산점도이다. . 3) Durbin-Watson d &#53685;&#44228;&#47049;&#51032; &#44050;&#51012; &#44396;&#54616;&#50668; $H_0 : rho = 0 , H_1 : rho neq 0 $ &#51012; $ alpha = 0.05 $ &#47196; &#44160;&#51221;&#54616;&#50668;&#46972;. . d통계량 구하기 . derr &lt;- err(x)-y . [1] 6.348571 6.564176 6.779780 6.995385 7.210989 7.426593 7.642198 7.857802 [9] 8.073407 8.289011 8.504615 8.720220 8.935824 9.151429 . derr . &lt;ol class=list-inline&gt;0.348571428571435 | 0.26417582417583 | 0.679780219780225 | 0.19538461538462 | -0.289010989010985 | -0.57340659340659 | -0.457802197802195 | -0.6421978021978 | -0.926593406593405 | -0.41098901098901 | 0.604615384615384 | 0.520219780219779 | 0.535824175824175 | 0.151428571428569 | &lt;/ol&gt; length(derr) . 14 temp &lt;- 0 for (i in 2:14) temp &lt;- temp + sum((derr[i]-derr[i-1])^2) return(temp) . 2.3106819466248 temp/(sum(derr^2)) #d 통계량 . 0.624575413892953 d 통계량은 0.6246 으로 0이상 2이하이다. 양의 자기상관을 가진다고 볼 수 있다. . 4) &#50948; 3)&#51032; &#44160;&#51221;&#44208;&#44284;&#45716; 2)&#51032; &#49328;&#51216;&#46020;&#50640;&#49436; &#50619;&#51008; &#45712;&#45196;&#44284; &#51068;&#52824;&#54616;&#45716;&#51648; &#45436;&#51032;&#54616;&#50668;&#46972;. . $2)$ 의 산점도는 2차곡선식의 회귀선이 타당해보이며, $3)$의 검정결과는 양의 자기상관을 이야기하고있다. 느낌이 일치하지 않는다. .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/10/15/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D5%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/10/15/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D5%EC%9E%A5%EA%B3%BC%EC%A0%9C.html",
            "date": " • Oct 15, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "데이터시각화 10/13 강의 과제",
            "content": "import pandas as pd . df=pd.read_csv(&#39;https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv&#39;) . df.columns . Index([&#39;color&#39;, &#39;director_name&#39;, &#39;num_critic_for_reviews&#39;, &#39;duration&#39;, &#39;director_facebook_likes&#39;, &#39;actor_3_facebook_likes&#39;, &#39;actor_2_name&#39;, &#39;actor_1_facebook_likes&#39;, &#39;gross&#39;, &#39;genres&#39;, &#39;actor_1_name&#39;, &#39;movie_title&#39;, &#39;num_voted_users&#39;, &#39;cast_total_facebook_likes&#39;, &#39;actor_3_name&#39;, &#39;facenumber_in_poster&#39;, &#39;plot_keywords&#39;, &#39;movie_imdb_link&#39;, &#39;num_user_for_reviews&#39;, &#39;language&#39;, &#39;country&#39;, &#39;content_rating&#39;, &#39;budget&#39;, &#39;title_year&#39;, &#39;actor_2_facebook_likes&#39;, &#39;imdb_score&#39;, &#39;aspect_ratio&#39;, &#39;movie_facebook_likes&#39;], dtype=&#39;object&#39;) . df.iloc[:,list(map(lambda x : &#39;face&#39; in x, df.columns) )] . director_facebook_likes actor_3_facebook_likes actor_1_facebook_likes cast_total_facebook_likes facenumber_in_poster actor_2_facebook_likes movie_facebook_likes . 0 0.0 | 855.0 | 1000.0 | 4834 | 0.0 | 936.0 | 33000 | . 1 563.0 | 1000.0 | 40000.0 | 48350 | 0.0 | 5000.0 | 0 | . 2 0.0 | 161.0 | 11000.0 | 11700 | 1.0 | 393.0 | 85000 | . 3 22000.0 | 23000.0 | 27000.0 | 106759 | 0.0 | 23000.0 | 164000 | . 4 131.0 | NaN | 131.0 | 143 | 0.0 | 12.0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 4911 2.0 | 318.0 | 637.0 | 2283 | 2.0 | 470.0 | 84 | . 4912 NaN | 319.0 | 841.0 | 1753 | 1.0 | 593.0 | 32000 | . 4913 0.0 | 0.0 | 0.0 | 0 | 0.0 | 0.0 | 16 | . 4914 0.0 | 489.0 | 946.0 | 2386 | 5.0 | 719.0 | 660 | . 4915 16.0 | 16.0 | 86.0 | 163 | 0.0 | 23.0 | 456 | . 4916 rows × 7 columns .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/13/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%941013%EA%B0%95%EC%9D%98%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/13/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%941013%EA%B0%95%EC%9D%98%EA%B3%BC%EC%A0%9C.html",
            "date": " • Oct 13, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "데이터시각화 10/6 강의 정리",
            "content": "import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats . qqplot . - 히스토그램이나 박스플랏보다 분포를 특정하기에 좋은 시각화는 없을까? . 정규분포 크기 1000 평균 2 표준편차 1.5 . t분포 자유도 10 크기 1000 표준편차 루트(자유도/(자유도-2)) . t분포는 평균이 0 | . np.random.seed(43052) x=np.random.normal(size=1000,loc=2,scale=1.5) y=stats.t.rvs(df=10,size=1000)/np.sqrt(10/8)*1.5 + 2 . plt.hist(y) . (array([ 1., 2., 5., 47., 164., 342., 311., 95., 26., 7.]), array([-5.32281821, -4.04507802, -2.76733783, -1.48959763, -0.21185744, 1.06588276, 2.34362295, 3.62136314, 4.89910334, 6.17684353, 7.45458373]), &lt;BarContainer object of 10 artists&gt;) . 가끔식 튀는 큰값이 나온다 . - 우리가 관측한 $x_1, dots,x_{1000}$이 $N(2,1.5^2)$에서 나온 샘플인지 궁금하다. . (1) 관측한 값을 순서대로 나열하여 $x_{(1)},x_{(2)}, dots, x_{(1000)}$을 만든다. . x[:2] #2번째 까지만 보이기 . array([2.57513073, 3.62626175]) . $x_1=2.57513073, quad x_2=3.62626175$ | . x.sort() # 작은 순 정렬 . x[:2] . array([-2.44398446, -2.14071467]) . $x_{(1)}= -2.44398446, quad x_{(2)}=-2.14071467$ | . (2) 파이썬이나 R로 $N(2,1.5^2)$에서 1000개의 정규분포를 생성. 그리고 순서대로 나열하여 $ tilde{x}_{(1)}, tilde{x}_{(2)}, dots, tilde{x}_{(1000)}$를 만든다. . (3) $x_{(1)} approx tilde{x}_{(1)}, dots , x_{(1000)} approx tilde{x}_{(1000)}$ 이면 x는 정규분포일것 . 그런데 $ tilde{x}_{(1)}, tilde{x}_{(2)}, dots, tilde{x}_{(1000)}$은 시뮬레이션을 할때마다 다른값이 나올테니까 불안정한 느낌이 든다. $ to$ 이론적인 값을 계산하자. . xx = (x-np.mean(x)) / np.std(x,ddof=1) # x를 표준화 xx[:2] . array([-3.05569305, -2.84275629]) . print(stats.norm.ppf(0.001)) #실제우리가 관측한 값 print(stats.norm.ppf(0.002)) . -3.090232306167813 -2.878161739095483 . m=[i/1000 for i in np.arange(1000)+1] # 이론적인 값 - 분위수 . np.arange(1000)[:10] . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . q=[] for i in range(len(m)): q=q+[stats.norm.ppf(m[i])] . q[:2] . [-3.090232306167813, -2.878161739095483] . $xx approx q$ 을 확인하기 위해서 $(q,q)$그래프와 $(q,xx)$의 그래프를 그려서 겹쳐보자. . plt.plot(q,xx,&#39;o&#39;) #표준화 : 동그라미점 선 plt.plot(q,q,&#39;-&#39;) #이론적인값 : 직선 . [&lt;matplotlib.lines.Line2D at 0x7f00b75caca0&gt;] . 해석: 점들이 주황색선 근처에 모여있을수록 정규분포에 가깝다. 굉장히 좋게 나온 예시 . | . - 아래와 같이 쉽게 그릴수도 있다. (_ = stats.probplot(x,plot=plt)우리가 그린그림과 조금 다르게 보인다) y축이 다름 . 자세히보면 조금 다르게 그려지긴 하는데 이는 $m=( frac{1}{1000}, dots, frac{999}{1000}, frac{1000}{1000})$와 같이 계산하지 않고 약간 보정한값을 계산하기 때문임 . | stats.probplot? 을 통하여 확인한 결과 아래와 같은 코드로 구현됨 . ### 보정하는방법1 n=len(xx) m=[((i+1)-0.3175)/(n+0.365) for i in range(n)] m[-n]=0.5**(1/n) m[0]=1-m[-n] . | 프로그램에 따라서 아래와 같이 보정하는 경우도 있음### 보정하는방법2 m=[(i-3/8)/(n+1/4) for i in np.arange(1000)+1] . | 또 자세히보면 stats.probplot은 y축에 표준화전의 x값이 있음을 알 수 있음. | . _ = stats.probplot(x,plot=plt) . _ = stats.probplot(x,plot=plt) # 정규분포 . _ = stats.probplot(y,plot=plt) # t분포 . t분포: 푸른점들이 대체로 붉은선위에 놓여있는듯 하지만 양끝단에서는 그렇지 않다. (중앙부근은 정규분포와 비슷하지만, 꼬리부분은 정규분포와 확실히 다르다) | 왼쪽꼬리: 이론적으로 나와야 할 값보다 더 작은값이 실제로 관측됨 | 오른쪽꼬리: 이론적으로 나와야 할 값보다 더 큰값이 실제로 관측됨 | 해석: 이 분포는 정규분포보다 두꺼운 꼬리를 가진다. | . &#49436;&#47196; &#45796;&#47480; &#54056;&#53412;&#51648;&#47484; &#49324;&#50857;&#54664;&#45716;&#45936; &#48537;&#50668;&#49436; &#44536;&#47532;&#44256; &#49910;&#51020; . matplot 기반이라 같이 쓸 수 있는 패키지가 있음 . fig , (ax1,ax2) = plt.subplots(1,2) fig.set_figwidth(8) # 그래프 크기 늘리기 . _ = stats.probplot(x,plot=ax1) _ = stats.probplot(y,plot=ax2) ax1.set_title(&#39;normal dist&#39;) ax2.set_title(&#39;t dist&#39;) . Text(0.5, 1.0, &#39;t dist&#39;) . fig . &#50696;&#51228;4 (boxplot, histrogram, qqplot) . - 박스플랏, 히스토그램, qqplot을 같이 그려보자. . fig, ax =plt.subplots(2,3) . (ax1,ax2,ax3), (ax4,ax5,ax6) = ax #각각의 축에 접근 . sns.boxplot(x,ax=ax1) #ax1에 박스플랏을 넣겠다 sns.histplot(x,kde=True,ax=ax2) #ax2에 히스토그램 _ = stats.probplot(x,plot=ax3) #ax3에 qq플랏 sns.boxplot(y,ax=ax4) sns.histplot(y,kde=True,ax=ax5) _ = stats.probplot(y,plot=ax6) #넒게 조정 fig.set_figwidth(10) fig.set_figheight(8) fig.tight_layout() . /home/kdj/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /home/kdj/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . fig . 지금까지는 가운데를 타겟으로 봤는데, 아래쪽그래프들은 양극단 데이터도 무시할 수가 없다. -&gt; ex)아마존 : 정규분포의 가운데가 마냥 맞는 것은 아니다. 전략 수립에서 정규성을 벗어나는지 봐야한다. . &#48516;&#50948;&#49688;&#47484; &#44396;&#54616;&#45716; &#45796;&#50577;&#54620; &#48169;&#48277; . m=[i/1000 for i in np.arange(1000)+1] #리스트 컴프리헨션 # np.arange(1000) : 0~999 . $m= big { frac{i}{1000}: i in {1,2,3, dots,1000 } big }= big { frac{1}{1000}, frac{2}{1000}, dots, frac{1000}{1000} big }$ | . m[:5] . [0.001, 0.002, 0.003, 0.004, 0.005] . range? . Init signature: range(self, /, *args, **kwargs) Docstring: range(stop) -&gt; range object range(start, stop[, step]) -&gt; range object Return an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. range(i, j) produces i, i+1, i+2, ..., j-1. start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement). Type: type Subclasses: . range . 처음부터 정수 시퀀스를 생성하는 개체 반환(포함) 단계별로 정지(정지)합니다. . 범위(i, j)는 i, i+1, i+2, ..., j-1을 생성합니다. . start 기본값은 0이고 stop은 생략됩니다! range(4)는 0, 1, 2, 3을 생성합니다. . 이들은 정확히 4개의 원소 목록에 대한 유효한 지수이다. . 단계가 지정되면 증분(또는 감소)을 지정합니다. . stats.norm.ppf? . Signature: stats.norm.ppf(q, *args, **kwds) Docstring: Percent point function (inverse of `cdf`) at q of the given RV. Parameters - q : array_like lower tail probability arg1, arg2, arg3,... : array_like The shape parameter(s) for the distribution (see docstring of the instance object for more information) loc : array_like, optional location parameter (default=0) scale : array_like, optional scale parameter (default=1) Returns - x : array_like quantile corresponding to the lower tail probability q. File: ~/anaconda3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py Type: method . stats.norm.ppf() . 주어진 RV의 q에서 백분율 포인트 함수(&#39;cdf&#39;의 역) . Parameters . q : array_like 낮은 꼬리 확률 arg1, arg2, arg3,... : array_like 분포에 대한 형상 모수(의 문서 문자열 참조) 자세한 내용을 보려면 인스턴스 개체) loc : array_like, 옵션 위치 매개 변수(기본값=0) scale : array_like, 옵션 척도 모수(기본값=1) . return . x : array_like = 아래쪽 꼬리 확률 q에 해당하는 분위수입니다. . q=[] for i in range(len(m)): q=q+[stats.norm.ppf(m[i])] q[:5] . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . q=[stats.norm.ppf(m[i]) for i in range(len(m))] q[:5] . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . q=list(map(stats.norm.ppf, m)) q[:5] . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . stats.norm.ppf(m)[:5] . array([-3.09023231, -2.87816174, -2.74778139, -2.65206981, -2.5758293 ]) . lambda . lambda &#49324;&#50857;&#48277; . f = lambda x,y,z : x+y+z ## lambda 입력:출력 #이자체가 오브젝트로 취급된다. . f(2,3,4) . 9 . lambda &#46356;&#54260;&#53944;&#51077;&#47141;&#44050; . x= (lambda a=&#39;fee&#39;,b=&#39;fie&#39;,c=&#39;foe&#39;: a+b+c) . x(&#39;wee&#39;) . &#39;weefiefoe&#39; . lambda&#51032; &#47532;&#49828;&#53944;&#54868; . l = [lambda x: x**2, lambda x: x**3, lambda x: x**4] #리스트안에 람다 3개 . for f in l: print(f(2)) . 4 8 16 . lambda&#51032; &#46357;&#49492;&#45320;&#47532;&#54868; . dct={&#39;f1&#39;: (lambda x: x+1), &#39;f2&#39;: (lambda x: x+22), &#39;f3&#39;: (lambda x: x+333)} . dct[&#39;f1&#39;](1), dct[&#39;f2&#39;](1), dct[&#39;f3&#39;](1) . (2, 23, 334) . lambda&#51032; &#51312;&#44148;&#48512; &#52636;&#47141; . lower = lambda x,y : x if x&lt;y else y . lower(&#39;a&#39;,&#39;b&#39;) . &#39;a&#39; . lower(&#39;c&#39;,&#39;b&#39;) . &#39;b&#39; . lambda expression &#51012; return &#51077;&#47141;&#44032;&#45733; . def action(x): return (lambda y: x+y) #리턴을 람다로 . act = action(99) ## act를 99+y를 수행하는 함수로 저장 act2 = action(98) ## act2를 98+y를 수행하는 함수로 저장 . action은 마치 함수를 만드는 함수같다.. | . print(act(2)) # act안에 y에 2를 넣겠다. 99+2 print(act2(2)) # act2안에 y에 2를 넣겠다. 98+2 . 101 100 . &#50696;&#51228;6&#51032; &#48156;&#51204; . action = lambda x: (lambda y: x+y) # 람다 출력에 또 람다 . act= action(99) #x에 99를 넣은 함수 act 선언 act2=action(98) #x에 98를 넣은 함수 act2 선언 . print(act(2)) print(act2(2)) . 101 100 . 괄호를 생략하여 선언하면 . action = lambda x: lambda y: x+y act= action(99) act2=action(98) print(act(2)) print(act2(2)) # 똑같다. . 101 100 . map . map? #map(func, *iterables) --&gt; map 개체 #다음 인수를 사용하여 함수를 계산하는 반복자를 만듭니다. #각 반복 사항 최단 시간이 소진되면 중지됩니다. . Init signature: map(self, /, *args, **kwargs) Docstring: map(func, *iterables) --&gt; map object Make an iterator that computes the function using arguments from each of the iterables. Stops when the shortest iterable is exhausted. Type: type Subclasses: . map &#49324;&#50857;&#48169;&#48277; . def inc(x): return x+1 #임시로 쓸건데 공간도 차지하고 좀 그럼 $ to$ 람다로 처리 . list(map(inc,[1,2,3,4])) . [2, 3, 4, 5] . &#50696;&#51228;1&#51032; &#48320;&#54805;(&#46988;&#45796;&#49324;&#50857;) . list(map(lambda x: x+1,[1,2,3,4])) . [2, 3, 4, 5] . list(map(def inc(x): return x+1,[1,2,3,4])) #안됨 . File &#34;&lt;ipython-input-69-e70d258d54b1&gt;&#34;, line 1 list(map(def inc(x): return x+1,[1,2,3,4])) #안됨 ^ SyntaxError: invalid syntax . 함수명을 쓰는 자리에 lambda로 표현한 오브젝트 자체를 전달할 수 있다. $ to$ 코드가 간단하다. | . map&#44284; &#47532;&#49828;&#53944;&#52980;&#54532;&#47532;&#54760;&#49496; &#48708;&#44368; . (함수선언) . f = lambda x: &#39;X&#39; in x . f(&#39;X1&#39;),f(&#39;X2&#39;),f(&#39;Y1&#39;),f(&#39;Y2&#39;) . (True, True, False, False) . (map) . list(map(f,[&#39;X1&#39;,&#39;X2&#39;,&#39;Y3&#39;,&#39;Y4&#39;])) . [True, True, False, False] . (리스트컴프리헨션과 비교) . [f(x) for x in [&#39;X1&#39;,&#39;X2&#39;,&#39;Y3&#39;,&#39;Y4&#39;]] . [True, True, False, False] . &#46160;&#44060;&#51032; &#51077;&#47141;&#51012; &#48155;&#45716; &#54632;&#49688;(pow) map, &#47532;&#49828;&#53944;&#52980;&#54532;&#47532;&#54760;&#49496; &#48708;&#44368; . (함수소개) . pow(2,4) #2를 4제곱 . 16 . (map) . list(map(pow,[2,2,2,3,3,3],[0,1,2,0,1,2])) . [1, 2, 4, 1, 3, 9] . (리스트컴프리헨션과 비교) . [pow(x,y) for x,y in zip([2,2,2,3,3,3],[0,1,2,0,1,2])] #zip이라는 새로운 오브젝트 생성 . [1, 2, 4, 1, 3, 9] . map은 (하나의 함수,다양한 입력)인 경우 사용가능 . l=[lambda x: x+1, lambda x: x+2, lambda x: x+3 ] . list(map(l,[100,200,300])) . TypeError Traceback (most recent call last) &lt;ipython-input-78-dcc049c06067&gt; in &lt;module&gt; -&gt; 1 list(map(l,[100,200,300])) TypeError: &#39;list&#39; object is not callable . 리스트컴프리헨션은 (다양한함수,다양한입력)이 가능함 . [l[i](x) for i,x in zip([0,1,2],[100,200,300])] . [101, 202, 303] . 리스트컴프리헨션은 (다양한함수,다양한입력)이 가능함 . [l[i](x) for i,x in zip([0,1,2],[100,200,300])] . [101, 202, 303] . 종합:map을 리스트컴프리헨션과 비교 . (1) 반복인덱스를 쓰지 않는 장점 . (2) 좀 더 제약적으로 사용할 수 밖에 없다는 단점 . &#51339;&#51008; &#49884;&#44033;&#54868; &#54616;&#44592; . - 왜 우수한 그래프일까? . 자료를 파악하는 기법은 최근까지도 산점도, 막대그래프, 라인플랏에 의존 | 이러한 플랏의 단점은 고차원의 자료를 분석하기 어렵다는 것임 | 미나드는 여러그램을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함. | . &#49884;&#44033;&#54868; &#50696;&#51228; . x=[44,48,49,58,62,68,69,70,76,79] ## 몸무게 y=[159,160,162,165,167,162,165,175,165,172] ## 키 g= &#39;f&#39;,&#39;f&#39;,&#39;f&#39;,&#39;f&#39;,&#39;m&#39;,&#39;f&#39;,&#39;m&#39;,&#39;m&#39;,&#39;m&#39;,&#39;m&#39; df=pd.DataFrame({&#39;w&#39;:x,&#39;h&#39;:y,&#39;g&#39;:g}) . df . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 4 62 | 167 | m | . 5 68 | 162 | f | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . - 미나드의 접근방법 . sns.scatterplot(data=df,x=&#39;w&#39;,y=&#39;h&#39;,hue=&#39;g&#39;) . &lt;AxesSubplot:xlabel=&#39;w&#39;, ylabel=&#39;h&#39;&gt; . - 일반적인 사람들 (보통 색깔을 사용할 생각을 못한다.) . figs = sns.FacetGrid(df,col=&#39;g&#39;) figs.map (sns.scatterplot,&#39;w&#39;,&#39;h&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f00b6fa36d0&gt; . - 생각보다 데이터가 정리된 형태에 따라서 시각화에 대한 사고방식이 달라진다. 아래와 같은 자료를 받았다고 하자. . df1=df.query(&quot;g ==&#39;f&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 여성.csv df2=df.query(&quot;g ==&#39;m&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 남성.csv . df1 . w h . 0 44 | 159 | . 1 48 | 160 | . 2 49 | 162 | . 3 58 | 165 | . 5 68 | 162 | . df2 . w h . 4 62 | 167 | . 6 69 | 165 | . 7 70 | 175 | . 8 76 | 165 | . 9 79 | 172 | . - 데이터프레임을 바꿀 생각을 하는게 쉽지 않다. . (방법1) . df1[&#39;g&#39;]= &#39;f&#39; . df1 . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 5 68 | 162 | f | . df2[&#39;g&#39;]= &#39;m&#39; . df2 . w h g . 4 62 | 167 | m | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . pd.concat([df1,df2]) . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 5 68 | 162 | f | . 4 62 | 167 | m | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . (방법2) . df1=df.query(&quot;g ==&#39;f&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 여성.csv df2=df.query(&quot;g ==&#39;m&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 남성.csv . pd.concat([df1,df2],keys=[&#39;f&#39;,&#39;m&#39;]).reset_index().iloc[:,[0,2,3]].rename(columns={&#39;level_0&#39;:&#39;g&#39;}) . g w h . 0 f | 44 | 159 | . 1 f | 48 | 160 | . 2 f | 49 | 162 | . 3 f | 58 | 165 | . 4 f | 68 | 162 | . 5 m | 62 | 167 | . 6 m | 69 | 165 | . 7 m | 70 | 175 | . 8 m | 76 | 165 | . 9 m | 79 | 172 | . - 어려운점: . (1) 센스가 없어서 색깔을 넣어서 그룹을 구분할 생각을 못함 | (2) 변형해야할 데이터를 생각못함 | (3) 데이터를 변형할 생각을 한다고 해도 변형하는 실제적인 코드를 구현할 수 없음 (그래서 엑셀을 킨다..) (1) 기획력부족 -&gt; 훌륭한 시각화를 많이 볼것 | (2) 데이터프레임에 대한 이해도가 부족 -&gt; tidydata에 대한 개념 | (3) 프로그래밍 능력 부족 -&gt; 코딩공부열심히.. | . | . - 목표: . (2) 어떠한 데이터 형태로 변형해야하는가? | (3) 그러한 데이터 형태로 바꾸기 위한 pandas 숙련도 | .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/10/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EA%B3%B5%EB%B6%80-%EC%95%84%EB%A7%885.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/10/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EA%B3%B5%EB%B6%80-%EC%95%84%EB%A7%885.html",
            "date": " • Oct 10, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "데이터시각화 10/6 강의 과제",
            "content": "import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats . &#45936;&#51060;&#53552;&#49884;&#44033;&#54868; 10/6&#51068;&#51088; &#44284;&#51228; 201514142 &#44608;&#46041;&#51456; . np.random.seed(201514142) x=np.random.chisquare(df=5, size=100) . fig, ax =plt.subplots(1,3) (ax1,ax2,ax3) =ax . sns.boxplot(x,ax=ax1) #ax1에 박스플랏을 넣겠다 sns.histplot(x,kde=True,ax=ax2) #ax2에 히스토그램 _ = stats.probplot(x,plot=ax3) #ax3에 qq플랏 . fig.set_figwidth(10) fig.set_figheight(5) fig.tight_layout() fig .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/08/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/08/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-%EA%B3%BC%EC%A0%9C.html",
            "date": " • Oct 8, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "데이터시각화 중간고사 공부2",
            "content": "import matplotlib.pyplot as plt import numpy as np import cv2 as cv import pandas as pd . Histogram Equalization, HE, &#55176;&#49828;&#53664;&#44536;&#47016; &#54217;&#54876;&#54868; . 이미지의 명암대비 개선 . . 이미지 불러오기 . img = cv.imread(&#39;사진이름.확장자&#39;,흑백-&gt;0) . img = cv.imread(&#39;450px-Unequalized_Hawkes_Bay_NZ.jpg&#39;,0) . img 입력시 array 숫자들이 출력됨 . $ to$ 그림이 픽셀의 조합이기때문 . $ to$ 이미지자료는 사실 0~255 사이의 어떠한 숫자들이 포함된 매트릭스일 뿐 . $ to$ 매트릭스에 있는 숫자들을 색깔로 표현하여 값이 클수록 하얗게, 값이 작을수록 검게 그린다. 극단적으로 0은 검은색, 255는 흰색이다. . plt.imshow(img,cmap=&#39;gray&#39;,vmin=0,vmax=255) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fd5b2e211f0&gt; . - 이미지가 넘파이 매트릭스일 뿐이라는 것을 판다스를 활용하면 더 잘 시각화하여 이해할 수 있다. . plt.imshow(img[200:300,400:500],cmap=&#39;gray&#39;,vmin=0,vmax=255) . &lt;matplotlib.image.AxesImage at 0x7fd5b266f100&gt; . df=pd.DataFrame(img) df.iloc[200:300,400:500].style.set_properties(**{&#39;font-size&#39;:&#39;10pt&#39;}).background_gradient(&#39;gray&#39;,vmin=0,vmax=255) . 데이터 프레임인데 각 셀의 색이 다르다 -&gt; like 그림을 크게 확대한것 . img.flatten().shape . (135000,) . fig1=plt.hist(img.flatten(),256,[0,256]) . - 히스토그램을 그려보니 120~200 사이에 너무 값들이 모여있음 . - 원래 0~255까지의 색을 표현할 수 있는데 컴퓨터가 표현가능한 색상보다 적은 조합만을 사용하고 있음. . $ to$ 더 많은 색상 표현 가능 = 히스토그램을 더 평평하게 = 선명 . img2=cv.equalizeHist(img) . fig2_1=plt.hist(img2.flatten(),256,[0,256]) . fig2_2=plt.hist(img2.flatten(),10,[0,256]) . plt.imshow(img2,cmap=&#39;gray&#39;,vmin=0,vmax=255) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fd5b05eb0d0&gt; . _img=np.hstack((img,img2)) . plt.imshow(_img,cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7fd5b051af40&gt; . $ to$ 수정된 사진 비교 (더 진해 졌다) . . &#51060;&#48120;&#51648; &#51088;&#47308;&#50640; &#45824;&#54620; &#51060;&#54644; . - 흑백이미지 . 차원: 세로픽셀수 $ times$ 가로픽셀수 | 값: 0~255 (값이 클수록 흰색) | . - 칼라이미지 . 차원: 세로픽셀수 $ times$ 가로픽셀수 $ times$ 3 | 값: 0~255 (값이 클수록 진한빨강, 진한파랑, 진한녹색) | . hani=cv.imread(&#39;450px-Unequalized_Hawkes_Bay_NZ.jpg&#39;) . import matplotlib.pyplot as plt plt.imshow(hani) . &lt;matplotlib.image.AxesImage at 0x7fd5b0938b50&gt; . hani.shape . (300, 450, 3) . import numpy as np hani_red=np.zeros_like(hani) hani_green=np.zeros_like(hani) hani_blue=np.zeros_like(hani) hani_red[:,:,0]=hani[:,:,0] hani_green[:,:,1]=hani[:,:,1] hani_blue[:,:,2]=hani[:,:,2] . _img2=np.hstack((hani_red,hani_green,hani_blue)) plt.imshow(_img2) . &lt;matplotlib.image.AxesImage at 0x7fd5b03e8580&gt; . plt.imshow(hani_red+hani_green+hani_blue) . &lt;matplotlib.image.AxesImage at 0x7fd5b033aac0&gt; . &#49328;&#51216;&#46020; (scatter plot) . 산점도:직교 좌표계(도표)를 이용해 좌표상의 점들을 표시함으로써 두 개 변수 간의 관계를 나타내는 그래프 방법이다. 산점도는 보통 $X$와 $Y$의 관계를 알고 싶을 경우 그린다. . 박스플랏, 히스토그램은 그림을 그리기 위해서 하나의 변수만 필요함; 산점도를 위해서는 두개의 변수가 필요함. | 두변수 $ to$ 두변수의 관계 | . &#47800;&#47924;&#44172;&#50752; &#53412; &#50696;&#49884; . x=[44,48,49,58,62,68,69,70,76,79] y=[159,160,162,165,167,162,165,175,165,172] . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5b027e4f0&gt;] . 키가 큰 사람일수록 몸무게도 많이 나간다. (반대도 성립) | 키와 몸무게는 관계가 있어보인다. (정비례) | . - 얼만큼 정비례인지? . 이 질문에 대답하기 위해서는 상관계수의 개념을 알아야 한다. | 상관계수에 대한 개념은 산점도를 이해함에 있어서 핵심개념이다. | . - (표본)상관계수 . $$r= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y}) }{ sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2 sum_{i=1}^{n}(y_i- bar{y})^2 }} $$ . 분모를 계산했다고 치자. 계산한 값을 어떤 상수 $c$라고 생각하자. 이 값을 분자안에 넣을수도 있다. | . $$r= sum_{i=1}^{n} frac{1}{c}(x_i- bar{x})(y_i- bar{y}) $$ . 위의 식은 아래와 같이 다시 쓸 수 있다. | . $$r= sum_{i=1}^{n} left( frac{(x_i- bar{x})}{ sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2}} frac{(y_i- bar{y})}{ sqrt{ sum_{i=1}^{n}(y_i- bar{y})^2}} right)$$ . 편의상 아래와 같이 정의하자. | . $$ tilde{x}_i= frac{(x_i- bar{x})}{ sqrt{ sum_{i=1}^n(x_i- bar{x})^2}}$$ . $$ tilde{y}_i= frac{(y_i- bar{y})}{ sqrt{ sum_{i=1}^n(y_i- bar{y})^2}}$$ . 결국 $r$은 아래와 같은 모양이다. | . $$r= sum_{i=1}^{n} tilde{x}_i tilde{y}_i $$ . x=np.array(x) y=np.array(y) . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5b02662e0&gt;] . plt.plot(x-np.mean(x), y-np.mean(y),&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5b01c7190&gt;] . $$r= sum_{i=1}^{n} left( frac{(x_i- bar{x})}{ sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2}} frac{(y_i- bar{y})}{ sqrt{ sum_{i=1}^{n}(y_i- bar{y})^2}} right)$$ . a=np.sqrt(np.sum((x-np.mean(x))**2)) #괄호 왼쪽 분모 b=np.sqrt(np.sum((y-np.mean(y))**2)) #괄호 오른쪽 분모 a,b . (36.58004920718396, 15.218409903797438) . $a&gt;b$ 이므로 $ {x_i }$들이 $ {y_i }$들 보다 좀 더 퍼져있다. (=평균근처에 몰려있지 않다) | . - 사실 $a,b$는 아래와 같이 계산할 수 있다. . $a= sqrt{n} times{ tt np.std(x)}$ . $b= sqrt{n} times{ tt np.std(y)}$ . n=len(x) np.sqrt(n)*np.std(x), np.sqrt(n)*np.std(y) . (36.58004920718397, 15.21840990379744) . ${ tt np.std(x)}= sqrt{ frac{1}{n} sum_{i=1}^{n}(x_i- bar{x})^2}$ | ${ tt np.std(y)}= sqrt{ frac{1}{n} sum_{i=1}^{n}(y_i- bar{y})^2}$ | . . Note: ${ tt np.std(x,ddof=1)}= sqrt{ frac{1}{n-1} sum_{i=1}^{n}(x_i- bar{x})^2}$ . - 이제 $( tilde{x}_i, tilde{y}_i)$를 그려보자. . xx= (x-np.mean(x))/a yy= (y-np.mean(y))/b plt.plot(xx,yy,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5b01a9580&gt;] . 평균도 비슷하고 퍼진정도도 비슷하다. | . - 질문1: $r$의 값이 양수인가? 음수인가? . plotly 사용하여 그려보자. . import plotly.express as px from IPython.display import HTML fig=px.scatter(x=xx, y=yy) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) . . . $ tilde{x}_i$, $ tilde{y}_i$ 를 곱한값이 양수인것과 음수인것을 체크해보자. | 양수인쪽이 많은지 음수인쪽이 많은지 생각해보자. | $r= sum_{i=1}^{n} tilde{x}_i tilde{y}_i$ 의 부호는? $ to$ 양수인 쪽이 훨씬 많다축=0 기준으로 사분면을 그어 양수쪽 기울기일지 그 수를 확인 . 1,3분면 :양수 . | . - 질문2: 아래와 같은 두개의 데이터set이 있다고 하자. . x1=np.arange(0,10,0.1) y1=x1+np.random.normal(loc=0,scale=1.0,size=len(x1)) . plt.plot(x1,y1,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5ab5801c0&gt;] . x2=np.arange(0,10,0.1) y2=x2+np.random.normal(loc=0,scale=7.0,size=len(x2)) plt.plot(x2,y2,&#39;x&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5ab54cdf0&gt;] . 같이 그리기 . plt.plot(x1,y1,&#39;o&#39;) plt.plot(x2,y2,&#39;x&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5ab523f40&gt;] . 각 데이터셋의 표준상관계수를 각각 $r_1$(파란색), $r_2$(주황색)라고 하자. . (1) $r_1$, $r_2$의 부호는 양수인가? 음수인가? . $r_1$ :양수&gt; $r_2$ :양수 . n=len(x1) xx1= (x1-np.mean(x1)) / (np.std(x1) * np.sqrt(n)) yy1= (y1-np.mean(y1)) / (np.std(y1) * np.sqrt(n)) xx2= (x2-np.mean(x2)) / (np.std(x2) * np.sqrt(n)) yy2= (y2-np.mean(y2)) / (np.std(y2) * np.sqrt(n)) . plt.plot(xx1,yy1,&#39;o&#39;) plt.plot(xx2,yy2,&#39;x&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd5ab4885b0&gt;] . (2) $r_1,r_2$의 값중 어떠한 값이 더 절대값이 큰가? . &#49345;&#44288;&#44228;&#49688; &#44228;&#49328; . sum(xx1*yy1), sum(xx2*yy2) . (0.9361646680973845, 0.4067814338019102) . 상관계수 r1이 더 크다 (절댓값이 더 크다) .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/03/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EA%B3%B5%EB%B6%802.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/03/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94-%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EA%B3%B5%EB%B6%802.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "데이터시각화 중간고사 공부1",
            "content": "import matplotlib.pyplot as plt import numpy as np . boxplot . 단순한 평균비교보다 분포를 비교해보는 것이 중요하다. 분포를 살펴보는 방법 중 유용한 방법이 박스플랏이다. . 함수 :&gt;&gt;import matplotlib.pyplot as plt&gt;&gt;plt.boxplot() . y1=[75,75,76,76,77,77,79,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들 y2=[76,76,77,77,78,78,80,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 . np.mean(y1), np.mean(y2) . (79.1, 78.3) . plt.boxplot(y1) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae2999040&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae29993a0&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae2999700&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2999a60&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae298ec70&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae2999dc0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae29a5190&gt;], &#39;means&#39;: []} . plt.boxplot(y2) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae51dec10&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae51def70&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae51ea310&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae51ea670&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae51de8b0&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae51ea9d0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae51ead30&gt;], &#39;means&#39;: []} . plt.boxplot([y1,y2]) # 나란히 그리기 . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae5149670&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae51499d0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae5153e80&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae515f220&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae5149d30&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae51530d0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae515f580&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae515f8e0&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae5149310&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae5153b20&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae5153430&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae515fc40&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae5153790&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae515ffa0&gt;], &#39;means&#39;: []} . 박스플랏 설명 그림 . np.random.seed(916170) # connection path is here: https://stackoverflow.com/questions/6146290/plotting-a-line-over-several-graphs mu, sigma = 0, 1 # mean and standard deviation s = np.random.normal(mu, sigma, 1000) fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize=(10, 5)) # rectangular box plot bplot = axes.boxplot(s, vert=False, patch_artist=True, showfliers=True, # This would show outliers (the remaining .7% of the data) positions = [0], boxprops = dict(linestyle=&#39;--&#39;, linewidth=2, color=&#39;Black&#39;, facecolor = &#39;red&#39;, alpha = .4), medianprops = dict(linestyle=&#39;-&#39;, linewidth=2, color=&#39;Yellow&#39;), whiskerprops = dict(linestyle=&#39;-&#39;, linewidth=2, color=&#39;Blue&#39;, alpha = .4), capprops = dict(linestyle=&#39;-&#39;, linewidth=2, color=&#39;Black&#39;), flierprops = dict(marker=&#39;o&#39;, markerfacecolor=&#39;green&#39;, markersize=10, linestyle=&#39;none&#39;, alpha = .4), widths = .3, zorder = 1) axes.set_xlim(-4, 4) plt.xticks(fontsize = 14) axes.set_yticks([]) axes.annotate(r&#39;&#39;, xy=(-.73, .205), xycoords=&#39;data&#39;, xytext=(.66, .205), textcoords=&#39;data&#39;, arrowprops=dict(arrowstyle=&quot;|-|&quot;, connectionstyle=&quot;arc3&quot;) ); axes.text(0, .25, &quot;Interquartile Range n(IQR)&quot;, horizontalalignment=&#39;center&#39;, fontsize=18) axes.text(0, -.21, r&quot;Median&quot;, horizontalalignment=&#39;center&#39;, fontsize=16); axes.text(2.65, -.15, &quot; &quot;Maximum &quot;&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(-2.65, -.15, &quot; &quot;Minimum &quot;&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(-.68, -.24, r&quot;Q1&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(-2.65, -.21, r&quot;(Q1 - 1.5*IQR)&quot;, horizontalalignment=&#39;center&#39;, fontsize=16); axes.text(.6745, -.24, r&quot;Q3&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(.6745, -.30, r&quot;(75th Percentile)&quot;, horizontalalignment=&#39;center&#39;, fontsize=12); axes.text(-.68, -.30, r&quot;(25th Percentile)&quot;, horizontalalignment=&#39;center&#39;, fontsize=12); axes.text(2.65, -.21, r&quot;(Q3 + 1.5*IQR)&quot;, horizontalalignment=&#39;center&#39;, fontsize=16); axes.annotate(&#39;Outliers&#39;, xy=(2.93,0.015), xytext=(2.52,0.20), fontsize = 18, arrowprops={&#39;arrowstyle&#39;: &#39;-&gt;&#39;, &#39;color&#39;: &#39;black&#39;, &#39;lw&#39;: 2}, va=&#39;center&#39;); axes.annotate(&#39;Outliers&#39;, xy=(-3.01,0.015), xytext=(-3.41,0.20), fontsize = 18, arrowprops={&#39;arrowstyle&#39;: &#39;-&gt;&#39;, &#39;color&#39;: &#39;black&#39;, &#39;lw&#39;: 2}, va=&#39;center&#39;); fig.tight_layout() . . plotly . 그림(그래프)에 마우스를 올리면 상호작용하는 그림 . plotly.express 와 pandas 필요 . import plotly.express as px import pandas as pd from IPython.display import HTML . 박스플랏에서 구한 성적을 df로 구현. . 열에 A라는것을 y1 수(길이)만큼 배열 . pd.concat([df1,df2]) :데이터프레임 합치기 ignore_index=True -&gt; 기존에 있던 인덱스를 무시해라 . 0~9 / 0~9 (기존) -&gt; 0~19 (무시) . A=pd.DataFrame({&#39;score&#39;:y1,&#39;class&#39;:[&#39;A&#39;]*len(y1)}) B=pd.DataFrame({&#39;score&#39;:y2,&#39;class&#39;:[&#39;B&#39;]*len(y2)}) . df=pd.concat([A,B],ignore_index=True) . html 같은 경우 블로그에 올릴 때 유용하다. (포맷을 변환) . 반응형 플랏을 볼 수 있다. (마우스 올리면 값나옴,HTML형태로 그려진 그림) . fig=px.box(data_frame=df, x=&#39;class&#39;,y=&#39;score&#39;) . HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) . . . Histogram . X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림 . plt.hist() 함수 이용 . np.random.normal() :정규분포&gt; &gt; loc:평균 , scale:표준편차 , size:표본수 . plt.hist(np.random.normal(loc=0, scale=1, size=1000000)) . (array([2.40000e+01, 1.21000e+03, 2.13380e+04, 1.39948e+05, 3.51614e+05, 3.39662e+05, 1.27147e+05, 1.80510e+04, 9.87000e+02, 1.90000e+01]), array([-5.05590169, -4.03792348, -3.01994528, -2.00196708, -0.98398887, 0.03398933, 1.05196753, 2.06994573, 3.08792394, 4.10590214, 5.12388034]), &lt;BarContainer object of 10 artists&gt;) . bins를 이용해 더 촘촘하게 그릴 수 있다. (정규분포에 더 가까워짐) . plt.hist(np.random.normal(loc=0, scale=1, size=1000000),bins=50) . (array([2.0000e+00, 6.0000e+00, 1.6000e+01, 3.9000e+01, 6.1000e+01, 1.3800e+02, 2.9000e+02, 5.0700e+02, 9.0400e+02, 1.6030e+03, 2.6310e+03, 4.1220e+03, 6.4460e+03, 9.5500e+03, 1.3819e+04, 1.9254e+04, 2.5638e+04, 3.3537e+04, 4.1336e+04, 4.9881e+04, 5.8100e+04, 6.5370e+04, 7.0605e+04, 7.3583e+04, 7.4674e+04, 7.2582e+04, 6.8887e+04, 6.2318e+04, 5.4823e+04, 4.6453e+04, 3.8169e+04, 3.0141e+04, 2.3006e+04, 1.7010e+04, 1.2102e+04, 8.3090e+03, 5.5450e+03, 3.4440e+03, 2.2440e+03, 1.2700e+03, 7.4300e+02, 4.3200e+02, 2.1800e+02, 9.4000e+01, 5.4000e+01, 1.9000e+01, 1.3000e+01, 5.0000e+00, 4.0000e+00, 3.0000e+00]), array([-4.56309489, -4.37544858, -4.18780228, -4.00015598, -3.81250967, -3.62486337, -3.43721707, -3.24957076, -3.06192446, -2.87427816, -2.68663185, -2.49898555, -2.31133925, -2.12369294, -1.93604664, -1.74840034, -1.56075403, -1.37310773, -1.18546143, -0.99781512, -0.81016882, -0.62252252, -0.43487621, -0.24722991, -0.05958361, 0.1280627 , 0.315709 , 0.5033553 , 0.69100161, 0.87864791, 1.06629421, 1.25394051, 1.44158682, 1.62923312, 1.81687942, 2.00452573, 2.19217203, 2.37981833, 2.56746464, 2.75511094, 2.94275724, 3.13040355, 3.31804985, 3.50569615, 3.69334246, 3.88098876, 4.06863506, 4.25628137, 4.44392767, 4.63157397, 4.81922028]), &lt;BarContainer object of 50 artists&gt;) . . 평균이 항상 좋은 중심경향값은 아니지만, 특수한 상황을 가정하면 평균이 좋은 중심경향값임 . np.random.seed(43052) #값이 안변하도록 시드설정 y1=np.random.normal(loc=0,scale=1,size=10000) #전북고 A반의 통계학 성적이라 생각하자. y2=np.random.normal(loc=0.5,scale=1,size=10000) #전북고 B반의 통계학 성적이라 생각하자. . np.mean(y1), np.mean(y2) #np.mean:평균, 튜플로 나옴 . (-0.011790879905079434, 0.4979147460611458) . (np.mean(y2)-np.mean(y1)).round(3) #소수 n째자리에서 반올림 . 0.51 . plt.boxplot([y1,y2]) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae27ac7c0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae27acb20&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2737fa0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2743340&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae27ace80&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2737220&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae27436a0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2743a00&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae27ac460&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2737c40&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae2737580&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2743d60&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f3ae27378e0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3ae2751100&gt;], &#39;means&#39;: []} . 분포의 모양이 거의 일치하다고 할만큼 비슷하다. . $B반의 성적 approx A반의 성적 + 0.51$ 라고 주장해도 큰 무리가 없음 . 정규분포인지는 어떻게 알까? $ to$ 히스토그램 그려보기 . plt.hist(y1,bins=50) . plt.hist(y2,bins=50) . plt.hist([y1,y2],bins=200) #같이 그리기 . $ to$ 아웃풋이 너무 지저분하다. . &#49352;&#47196;&#50868; &#54056;&#53412;&#51648; &#51060;&#50857; . R:ggplot2 가 대세&gt; Python:&gt;&gt;matplotlib (매트랩 모방) . seaborn . plotnine (ggplot모방) . plotly 등등 많음 . seaborn . 깔끔하게 히스토그램을 그리는 패키지 . df를 입력으로 받는다 . import seaborn as sns . A=pd.DataFrame({&#39;score&#39;:y1,&#39;class&#39;:[&#39;A&#39;]*len(y1)}) B=pd.DataFrame({&#39;score&#39;:y2,&#39;class&#39;:[&#39;B&#39;]*len(y2)}) df=pd.concat([A,B],ignore_index=True) . sns.histplot(df,x=&#39;score&#39;,hue=&#39;class&#39;) . &lt;AxesSubplot:xlabel=&#39;score&#39;, ylabel=&#39;Count&#39;&gt; . plotnine . 인터랙티브 그래프를 위해서 plotly 홈페이지를 방문하여 적당한 코드를 가져온다. . from plotnine import * . ggplot(df)+geom_histogram(aes(x=&#39;score&#39;,color=&#39;class&#39;)) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: &#39;stat_bin()&#39; using &#39;bins = 84&#39;. Pick better value with &#39;binwidth&#39;. . &lt;ggplot: (8743170859795)&gt; . $ to$ 별로 알아보기가 힘들다 . color를fill로 바꿔줌 , position을 동등하게 , alpha: 투명도 . ggplot(df)+geom_histogram(aes(x=&#39;score&#39;,fill=&#39;class&#39;),position=&#39;identity&#39;,alpha=0.5) . /home/kdj/anaconda3/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: &#39;stat_bin()&#39; using &#39;bins = 84&#39;. Pick better value with &#39;binwidth&#39;. . &lt;ggplot: (8743171077340)&gt; . plotly &#54876;&#50857; . 구글에 검색하면 예시가 잘 나와있다. . import plotly.figure_factory as ff import numpy as np hist_data=[y1,y2] group_labels=[&#39;A&#39;,&#39;B&#39;] fig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_rug=False) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) . . . .",
            "url": "https://cjfal.github.io/dj/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/02/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%941.html",
            "relUrl": "/python/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/2021/10/02/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%941.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "4주차 회귀분석 연습문제 4.8 R 풀이",
            "content": "base . A &lt;- c(5,6,7,8,9,10,11,12,13,14) B &lt;- c(89,87,98,110,103,114,116,110,126,130) . 1)2) &#51208;&#54200;&#44284; &#44592;&#50872;&#44592;&#51032; &#49888;&#47280;&#44396;&#44036; &#48143; &#44160;&#51221; . lm48 &lt;- lm(B~A) summary(lm48) . Call: lm(formula = B ~ A) Residuals: Min 1Q Median 3Q Max -9.3758 -2.1545 0.9152 2.0864 8.3455 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 66.212 5.767 11.482 3.00e-06 *** A 4.430 0.581 7.625 6.16e-05 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 5.278 on 8 degrees of freedom Multiple R-squared: 0.879, Adjusted R-squared: 0.8639 F-statistic: 58.14 on 1 and 8 DF, p-value: 6.161e-05 . coeffs &lt;- coefficients(lm48) confint(lm48,level = 0.95) # 기울기와 절편의 95% 신뢰구간 . A matrix: 2 × 2 of type dbl 2.5 %97.5 % . (Intercept)52.914087 | 79.510156 | . A 3.090412 | 5.770194 | . R을 통해 y=β_0+β_1 x+ϵ 를 가정해 본 결과 절편β_0은 66.212, 기울기β_1은 4.430이 나왔다. 절편의 신뢰구간을 추정해본 결과 절편 β_0의 신뢰구간은 (52.914087, 79.510156) 으로 나왔다. 이어서 자유도가 8인 t분포에서 가운데 면적이 95% 인 경우의 t*값은 2.306이다. 그런데 summary 에서 확인할 수 있는 절편의 t-value은 11.482로 2.306보다 크다. 즉 영가설을 기각한다. . 기울기의 신뢰구간은 (3.090412, 5.770194)이다. summary에서 절편에 대한 t-value 값은 7.625인데 이 값은 자유도 8인 t분포에서 가운데 면적이 95%일때의 t*값인 2.306 보다 크므로 영가설을 기각한다. . 3) &#48516;&#49328;&#48516;&#49437;&#54364;&#47196; &#44592;&#50872;&#44592; &#44160;&#51221; . anova(lm48) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . A1 | 1619.2758 | 1619.27576 | 58.13643 | 6.160731e-05 | . Residuals8 | 222.8242 | 27.85303 | NA | NA | . F-value는 58.136 이다. 분자의 자유도 1 과 분모의 자유도가 8인 F-분포에서 p-value 는 6.161^(-10)이다. 이 p값은 굉장히 작기 때문에 선형모형(기울기와 편차)의 가정이 좋은 가정이라고 말할 수 있다. . 4) &#44208;&#51221;&#44228;&#49688;&#50752; &#51032;&#48120; . (cor(A,B))^2 . 0.879037922792333 결정계수 R^2는 0.8790379이다. 1에 가까운 값으로 추정된 방정식이 판매액과 광고비를 잘 나타내 준다고 말할 수 있다 . 5) x=10 &#51068;&#46412; yhat &#49888;&#47280;&#44396;&#44036; . q10 &lt;- data.frame(A=10) predict(lm48,q10,level=0.95,interval=&quot;confidence&quot;) . A matrix: 1 × 3 of type dbl fitlwrupr . 1110.5152 | 106.6087 | 114.4216 | . 적합된 광고비 값은 110.5152이고, 그 신뢰구간은 (106.6087,114.4216) 이다. . 6) x=10 &#51068;&#46412; y &#50696;&#52769;&#44396;&#44036; . predict(lm48,q10,level=0.95,interval=&quot;predict&quot;) . A matrix: 1 × 3 of type dbl fitlwrupr . 1110.5152 | 97.73341 | 123.2969 | . 적합된 광고비 값은 110.5152이고, 그 예측구간은 (97.73341,123.2969) 이다. . q8 &lt;- data.frame(A=8) q9 &lt;- data.frame(A=9) q10 &lt;- data.frame(A=10) q11 &lt;- data.frame(A=11) q12 &lt;- data.frame(A=12) predict(lm48,q8,level=0.95,interval=&quot;confidence&quot;) predict(lm48,q9,level=0.95,interval=&quot;confidence&quot;) predict(lm48,q10,level=0.95,interval=&quot;confidence&quot;) predict(lm48,q11,level=0.95,interval=&quot;confidence&quot;) predict(lm48,q12,level=0.95,interval=&quot;confidence&quot;) predict(lm48,q8,level=0.95,interval=&quot;predict&quot;) predict(lm48,q9,level=0.95,interval=&quot;predict&quot;) predict(lm48,q10,level=0.95,interval=&quot;predict&quot;) predict(lm48,q11,level=0.95,interval=&quot;predict&quot;) predict(lm48,q12,level=0.95,interval=&quot;predict&quot;) . A matrix: 1 × 3 of type dbl fitlwrupr . 1101.6545 | 97.3128 | 105.9963 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1106.0848 | 102.1784 | 109.9913 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1110.5152 | 106.6087 | 114.4216 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1114.9455 | 110.6037 | 119.2872 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1119.3758 | 114.2736 | 124.4779 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1101.6545 | 88.73311 | 114.576 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1106.0848 | 93.30311 | 118.8666 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1110.5152 | 97.73341 | 123.2969 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1114.9455 | 102.024 | 127.8669 | . A matrix: 1 × 3 of type dbl fitlwrupr . 1119.3758 | 106.1794 | 132.5721 | . &#49888;&#47280;&#45824; &#50696;&#52769;&#45824; &#46020;&#49884; . m_conf &lt;- predict(lm48,level=0.95,interval=&quot;confidence&quot;) plot(B~A) lwr &lt;- m_conf[,2] upr &lt;- m_conf[,3] sx &lt;- sort(A, index.return=TRUE) abline(coef(lm48),lwd=2) lines(sx$x, lwr[sx$ix], col=&quot;blue&quot;, lty=2) lines(sx$x, upr[sx$ix], col=&quot;blue&quot;, lty=2) m_pred &lt;- predict(lm48,level=0.95,interval=&quot;predict&quot;) p_lwr &lt;- m_pred[,2] p_upr &lt;- m_pred[,3] lines(A, p_lwr, col=&quot;red&quot;, lty=2) lines(A, p_upr, col=&quot;red&quot;, lty=2) . Warning message in predict.lm(lm48, level = 0.95, interval = &#34;predict&#34;): “predictions on current data refer to _future_ responses ” . (오류는 무슨뜻일까.. 문제는 없어보인다) . 붉은선은 예측대, 파란선은 신뢰대이다. 예측대가 신뢰대보다 더 넓은 범위를 차지하며 . 신뢰대와 예측대 모두 거의 중앙에 회귀선을 품고있는 모습을 하고 있다. . 값에따른 신뢰구간과 예측구간 . x_0 신뢰구간(confidence) 예측구간(predict) . | lwr | upr | lwr | Upr | . 8 | 97.31280 | 105.9963 | 88.73311 | 114.5760 | . 9 | 102.1784 | 109.9913 | 93.30311 | 118.8666 | . 10 | 106.6087 | 114.4216 | 97.73341 | 123.2969 | . 11 | 110.6037 | 119.2872 | 102.0240 | 127.8669 | . 12 | 114.2736 | 124.4779 | 106.1794 | 132.5721 | .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/09/30/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C_4_8.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/09/30/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C_4_8.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "4주차 회귀분석 연습문제 4.1 R 풀이",
            "content": "Base 코드 . x &lt;- c(28.0,28.0,32.5,39.0,45.9,57.8,58.1,62.5) #시간당평균온도 y &lt;- c(12.4,11.7,12.4,10.8,9.4,9.5,8.0,7.5) #석탄소비량 lm31 &lt;- lm(y~x) . 1) &#44592;&#50872;&#44592;&#50752; &#51208;&#54200;&#51032; &#49888;&#47280;&#44396;&#44036; &#44396;&#54616;&#44592; . summary(lm31) . Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -0.5663 -0.4432 -0.1958 0.2879 1.0560 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 15.83786 0.80177 19.754 1.09e-06 *** x -0.12792 0.01746 -7.328 0.00033 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.6542 on 6 degrees of freedom Multiple R-squared: 0.8995, Adjusted R-squared: 0.8827 F-statistic: 53.69 on 1 and 6 DF, p-value: 0.0003301 . coeffs &lt;- coefficients(lm31) confint(lm31,coeffs[2],level=0.95) #기울기 b1 의 95% 신뢰구간 . A matrix: 1 × 2 of type dbl 2.5 %97.5 % . x-0.1706383 | -0.08520517 | . confint(lm31,level = 0.95) #절편 b0 의 95% 신뢰구간 . A matrix: 2 × 2 of type dbl 2.5 %97.5 % . (Intercept)13.8759886 | 17.79972621 | . x-0.1706383 | -0.08520517 | . 2) &#49888;&#47280;&#44396;&#44036;&#51032; &#51032;&#48120; . 시간당 평균온도를 한 단위(℉) 늘리면 석탄소비량(ton)은 -0.171과 -0.0852 사이의 어느 한 값의 배수만큼 평균적으로 증가함을 신뢰계수 0.95로 예측할 수 있다. . 3) &#44032;&#49444; &#44160;&#51221; &#50689;&#44032;&#49444;=0 &#50976;&#51032;&#49688;&#51456; 0.05 . summary(lm31) . Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -0.5663 -0.4432 -0.1958 0.2879 1.0560 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 15.83786 0.80177 19.754 1.09e-06 *** x -0.12792 0.01746 -7.328 0.00033 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.6542 on 6 degrees of freedom Multiple R-squared: 0.8995, Adjusted R-squared: 0.8827 F-statistic: 53.69 on 1 and 6 DF, p-value: 0.0003301 . 15.838 + 2.447* 0.01746 15.838 - 2.447* 0.01746 . 15.88072462 15.79527538 기울기의 추정값 b1 = 15.838 이고 표준오차 SE는 0.01746 이다. 자유도(degrees of freedom)=6 인 t분포에서 가운데 면적이 95%인 t*=2.447 이며 95% 신뢰구간은 다음과 같이 계산한다. b_1±t^*×SE=15.838 ±2.447 ×0.01746 . 95% 신뢰구간 : (15.79528 , 15.88072) . 이 결과로부터 기울기(평균온도가 1℉ 오를 때 석탄소비량 상승은 15.79528 ton 에서 15.88072 ton 사이에 있을 것이라고 95% 확신한다. . 4) &#44032;&#49444;&#44160;&#51221;2 . Pr(&gt;|t|) 값(p-value)의 값이 극히 작으므로 상당히 믿을 만한 정보이고, 영가설 H0를 기각한다. 즉, 기울기 β_1 은 의미있는 가정값이다. . 5) &#48516;&#49328;&#48516;&#49437;&#51012; &#53685;&#54620; &#44032;&#49444;&#44160;&#51221; . anova(lm31) . A anova: 2 × 5 DfSum SqMean SqF valuePr(&gt;F) . &lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . x1 | 22.980816 | 22.980816 | 53.69488 | 0.0003300523 | . Residuals6 | 2.567934 | 0.427989 | NA | NA | . F-value는 53.695 이다. 분자의 자유도 1 과 6인 F-분포에서 p-value 는 0.00033301이다. 이 p값은 굉장히 작기 때문에 선형모형(기울기와 편차)의 가정이 좋은 가정이라고 말할 수 있다. . 6)&#49884;&#44036;&#45817; &#54217;&#44512; &#50728;&#46020;&#50640; &#46384;&#47480; &#49548;&#48708;&#47049; &#52628;&#51221;, &#49888;&#47280;&#44396;&#44036; &#44396;&#54616;&#44592; . q6 &lt;- data.frame(x=40) predict(lm31,q6, level=0.95,interval=&quot;confidence&quot;) #신뢰구간 . A matrix: 1 × 3 of type dbl fitlwrupr . 110.72099 | 10.1301 | 11.31188 | . 추정된 석탄소비량은 10.72099 ton 이다. 그 신뢰구간은 (10.1301, 11.31188) 이고 y값이 이 범위에 있을 확률이 95%라는 의미이다. . 7) &#50696;&#52769;&#44396;&#44036; &#52628;&#51221; . predict(lm31,q6, level=0.95,interval=&quot;predict&quot;)예측구간 . A matrix: 1 × 3 of type dbl fitlwrupr . 110.72099 | 9.014624 | 12.42735 | . 추정된 석탄소비량은 10.72099 ton 이다. 그 예측구간은 (9.014624, 12.42735) 이고 y값이 이 범위에 있을 확률이 95%라는 의미이다. . 8) 6&#48264; 7&#48264;&#47928;&#51228; &#48708;&#44368; . 신뢰구간:y의 평균이 존재하는 구간을 추정  모집단을 예측 변수의 특정 값의 반응변수의 값으로 제한한다. 예측구간:하나의 y가 존재하는 구간을 추정  모집단을 대부분 포함해야 하므로 구간이 더 넓다. . 9) &#44208;&#51221;&#44228;&#49688; &#48143; &#51032;&#48120; . cor(x,y) # 상관계수 . -0.948413871025354 상관 계수 r :-0.9484139  -1에 가까우므로 강한 음의 상관관계를 가진다고 볼 수 있다. . (cor(x,y))^2 #결정계수 . 0.899488870753297 결정 계수 R^2: 0.8994889 . 결정 계수는 온도변화로 석탄소비량의 89.95%를 설명한다는 뜻이다. . 결정 계수가 1에 거의 근접하므로 가정된 방정식이 온도변화와 석탄소비량의 관계를 잘 설명한다고 볼 수 있다. .",
            "url": "https://cjfal.github.io/dj/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/09/30/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C_4_1.html",
            "relUrl": "/r/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/2021/09/30/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C_4_1.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://cjfal.github.io/dj/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://cjfal.github.io/dj/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "김동준 소개 .",
          "url": "https://cjfal.github.io/dj/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cjfal.github.io/dj/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}